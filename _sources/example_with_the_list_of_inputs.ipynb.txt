{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example : k times repetition with the list of k input files \n",
    "\n",
    "Deepbiome packages takes microbiome abundance data as input and uses the phylogenetic taxonomy to guide the decision of the optimal number of layers and neurons in the deep learning architecture.\n",
    "\n",
    "To use deepbiome, you can experiment (1) __k times repeatition__ or (2) __k fold cross-validation__.\n",
    "For each experiment, we asuume that the dataset is given by\n",
    "- __A list of k input files for k times repeatition.__\n",
    "- __One input file for k times cross validation.__\n",
    "\n",
    "This notebook contains an example of (1) __k times repeatition__ for the deep neural netowrk using deepbiome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load library\n",
    "\n",
    "First, we have to load deepbiome package. The deepbiome package is build on the tensorflow and keras library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "from pkg_resources import resource_filename\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from deepbiome import deepbiome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare the dataset\n",
    "\n",
    "In this example, we assume that we have __a list of k input files for k times repeatition.__\n",
    "\n",
    "Deepbiome needs 4 data as follow:\n",
    "1. __the tree information__\n",
    "1. __the lists of the input files__ (each file has all sample's information for one repeatition)\n",
    "1. __the list of the name of input files__ \n",
    "1. __y__\n",
    "\n",
    "In addition, we can set __the training index for each repeatition__. If we set the index file, deepbiome build the training set for each repeatition based on each fold index in the index file. If not, deepbiome will generate the index file locally.\n",
    "\n",
    "\n",
    "Eath data should have the csv format. Below is the example of each file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of the tree information\n",
    "\n",
    "First we need a file about the phylogenetic tree information. This tree information file should have the format below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genus</th>\n",
       "      <th>Family</th>\n",
       "      <th>Order</th>\n",
       "      <th>Class</th>\n",
       "      <th>Phylum</th>\n",
       "      <th>Domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Streptococcus</td>\n",
       "      <td>Streptococcaceae</td>\n",
       "      <td>Lactobacillales</td>\n",
       "      <td>Bacilli</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tropheryma</td>\n",
       "      <td>Cellulomonadaceae</td>\n",
       "      <td>Actinomycetales</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Veillonella</td>\n",
       "      <td>Veillonellaceae</td>\n",
       "      <td>Selenomonadales</td>\n",
       "      <td>Negativicutes</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Actinomyces</td>\n",
       "      <td>Actinomycetaceae</td>\n",
       "      <td>Actinomycetales</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Flavobacterium</td>\n",
       "      <td>Flavobacteriaceae</td>\n",
       "      <td>Flavobacteriales</td>\n",
       "      <td>Flavobacteria</td>\n",
       "      <td>Bacteroidetes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Prevotella</td>\n",
       "      <td>Prevotellaceae</td>\n",
       "      <td>Bacteroidales</td>\n",
       "      <td>Bacteroidia</td>\n",
       "      <td>Bacteroidetes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Porphyromonas</td>\n",
       "      <td>Porphyromonadaceae</td>\n",
       "      <td>Bacteroidales</td>\n",
       "      <td>Bacteroidia</td>\n",
       "      <td>Bacteroidetes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Parvimonas</td>\n",
       "      <td>Clostridiales_Incertae_Sedis_XI</td>\n",
       "      <td>Clostridiales</td>\n",
       "      <td>Clostridia</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Fusobacterium</td>\n",
       "      <td>Fusobacteriaceae</td>\n",
       "      <td>Fusobacteriales</td>\n",
       "      <td>Fusobacteria</td>\n",
       "      <td>Fusobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Propionibacterium</td>\n",
       "      <td>Propionibacteriaceae</td>\n",
       "      <td>Actinomycetales</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gemella</td>\n",
       "      <td>Bacillales_Incertae_Sedis_XI</td>\n",
       "      <td>Bacillales</td>\n",
       "      <td>Bacilli</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Rothia</td>\n",
       "      <td>Micrococcaceae</td>\n",
       "      <td>Actinomycetales</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Granulicatella</td>\n",
       "      <td>Carnobacteriaceae</td>\n",
       "      <td>Lactobacillales</td>\n",
       "      <td>Bacilli</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Neisseria</td>\n",
       "      <td>Neisseriaceae</td>\n",
       "      <td>Neisseriales</td>\n",
       "      <td>Betaproteobacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Lactobacillus</td>\n",
       "      <td>Lactobacillaceae</td>\n",
       "      <td>Lactobacillales</td>\n",
       "      <td>Bacilli</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Megasphaera</td>\n",
       "      <td>Veillonellaceae</td>\n",
       "      <td>Selenomonadales</td>\n",
       "      <td>Negativicutes</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Catonella</td>\n",
       "      <td>Lachnospiraceae</td>\n",
       "      <td>Clostridiales</td>\n",
       "      <td>Clostridia</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Atopobium</td>\n",
       "      <td>Coriobacteriaceae</td>\n",
       "      <td>Coriobacteriales</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Campylobacter</td>\n",
       "      <td>Campylobacteraceae</td>\n",
       "      <td>Campylobacterales</td>\n",
       "      <td>Epsilonproteobacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Capnocytophaga</td>\n",
       "      <td>Flavobacteriaceae</td>\n",
       "      <td>Flavobacteriales</td>\n",
       "      <td>Flavobacteria</td>\n",
       "      <td>Bacteroidetes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Solobacterium</td>\n",
       "      <td>Erysipelotrichaceae</td>\n",
       "      <td>Erysipelotrichales</td>\n",
       "      <td>Erysipelotrichia</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Moryella</td>\n",
       "      <td>Lachnospiraceae</td>\n",
       "      <td>Clostridiales</td>\n",
       "      <td>Clostridia</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>TM7_genera_incertae_sedis</td>\n",
       "      <td>TM7_genera_incertae_sedis</td>\n",
       "      <td>TM7_genera_incertae_sedis</td>\n",
       "      <td>TM7_genera_incertae_sedis</td>\n",
       "      <td>TM7</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Staphylococcus</td>\n",
       "      <td>Staphylococcaceae</td>\n",
       "      <td>Bacillales</td>\n",
       "      <td>Bacilli</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Filifactor</td>\n",
       "      <td>Peptostreptococcaceae</td>\n",
       "      <td>Clostridiales</td>\n",
       "      <td>Clostridia</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Oribacterium</td>\n",
       "      <td>Lachnospiraceae</td>\n",
       "      <td>Clostridiales</td>\n",
       "      <td>Clostridia</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Burkholderia</td>\n",
       "      <td>Burkholderiaceae</td>\n",
       "      <td>Burkholderiales</td>\n",
       "      <td>Betaproteobacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Sneathia</td>\n",
       "      <td>Leptotrichiaceae</td>\n",
       "      <td>Fusobacteriales</td>\n",
       "      <td>Fusobacteria</td>\n",
       "      <td>Fusobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Treponema</td>\n",
       "      <td>Spirochaetaceae</td>\n",
       "      <td>Spirochaetales</td>\n",
       "      <td>Spirochaetes</td>\n",
       "      <td>Spirochaetes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Moraxella</td>\n",
       "      <td>Moraxellaceae</td>\n",
       "      <td>Pseudomonadales</td>\n",
       "      <td>Gammaproteobacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Haemophilus</td>\n",
       "      <td>Pasteurellaceae</td>\n",
       "      <td>Pasteurellales</td>\n",
       "      <td>Gammaproteobacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Selenomonas</td>\n",
       "      <td>Veillonellaceae</td>\n",
       "      <td>Selenomonadales</td>\n",
       "      <td>Negativicutes</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Corynebacterium</td>\n",
       "      <td>Corynebacteriaceae</td>\n",
       "      <td>Actinomycetales</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Rhizobium</td>\n",
       "      <td>Rhizobiaceae</td>\n",
       "      <td>Rhizobiales</td>\n",
       "      <td>Alphaproteobacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Bradyrhizobium</td>\n",
       "      <td>Bradyrhizobiaceae</td>\n",
       "      <td>Rhizobiales</td>\n",
       "      <td>Alphaproteobacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Methylobacterium</td>\n",
       "      <td>Methylobacteriaceae</td>\n",
       "      <td>Rhizobiales</td>\n",
       "      <td>Alphaproteobacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>OD1_genera_incertae_sedis</td>\n",
       "      <td>OD1_genera_incertae_sedis</td>\n",
       "      <td>OD1_genera_incertae_sedis</td>\n",
       "      <td>OD1_genera_incertae_sedis</td>\n",
       "      <td>OD1</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Finegoldia</td>\n",
       "      <td>Clostridiales_Incertae_Sedis_XI</td>\n",
       "      <td>Clostridiales</td>\n",
       "      <td>Clostridia</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Microbacterium</td>\n",
       "      <td>Microbacteriaceae</td>\n",
       "      <td>Actinomycetales</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Sphingomonas</td>\n",
       "      <td>Sphingomonadaceae</td>\n",
       "      <td>Sphingomonadales</td>\n",
       "      <td>Alphaproteobacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Chryseobacterium</td>\n",
       "      <td>Flavobacteriaceae</td>\n",
       "      <td>Flavobacteriales</td>\n",
       "      <td>Flavobacteria</td>\n",
       "      <td>Bacteroidetes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Bacteroides</td>\n",
       "      <td>Bacteroidaceae</td>\n",
       "      <td>Bacteroidales</td>\n",
       "      <td>Bacteroidia</td>\n",
       "      <td>Bacteroidetes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Bdellovibrio</td>\n",
       "      <td>Bdellovibrionaceae</td>\n",
       "      <td>Bdellovibrionales</td>\n",
       "      <td>Deltaproteobacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Streptophyta</td>\n",
       "      <td>Chloroplast</td>\n",
       "      <td>Chloroplast</td>\n",
       "      <td>Chloroplast</td>\n",
       "      <td>Cyanobacteria_Chloroplast</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Lachnospiracea_incertae_sedis</td>\n",
       "      <td>Lachnospiraceae</td>\n",
       "      <td>Clostridiales</td>\n",
       "      <td>Clostridia</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Paracoccus</td>\n",
       "      <td>Rhodobacteraceae</td>\n",
       "      <td>Rhodobacterales</td>\n",
       "      <td>Alphaproteobacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Fastidiosipila</td>\n",
       "      <td>Ruminococcaceae</td>\n",
       "      <td>Clostridiales</td>\n",
       "      <td>Clostridia</td>\n",
       "      <td>Firmicutes</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Pseudonocardia</td>\n",
       "      <td>Pseudonocardiaceae</td>\n",
       "      <td>Actinomycetales</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Genus                           Family  \\\n",
       "0                   Streptococcus                 Streptococcaceae   \n",
       "1                      Tropheryma                Cellulomonadaceae   \n",
       "2                     Veillonella                  Veillonellaceae   \n",
       "3                     Actinomyces                 Actinomycetaceae   \n",
       "4                  Flavobacterium                Flavobacteriaceae   \n",
       "5                      Prevotella                   Prevotellaceae   \n",
       "6                   Porphyromonas               Porphyromonadaceae   \n",
       "7                      Parvimonas  Clostridiales_Incertae_Sedis_XI   \n",
       "8                   Fusobacterium                 Fusobacteriaceae   \n",
       "9               Propionibacterium             Propionibacteriaceae   \n",
       "10                        Gemella     Bacillales_Incertae_Sedis_XI   \n",
       "11                         Rothia                   Micrococcaceae   \n",
       "12                 Granulicatella                Carnobacteriaceae   \n",
       "13                      Neisseria                    Neisseriaceae   \n",
       "14                  Lactobacillus                 Lactobacillaceae   \n",
       "15                    Megasphaera                  Veillonellaceae   \n",
       "16                      Catonella                  Lachnospiraceae   \n",
       "17                      Atopobium                Coriobacteriaceae   \n",
       "18                  Campylobacter               Campylobacteraceae   \n",
       "19                 Capnocytophaga                Flavobacteriaceae   \n",
       "20                  Solobacterium              Erysipelotrichaceae   \n",
       "21                       Moryella                  Lachnospiraceae   \n",
       "22      TM7_genera_incertae_sedis        TM7_genera_incertae_sedis   \n",
       "23                 Staphylococcus                Staphylococcaceae   \n",
       "24                     Filifactor            Peptostreptococcaceae   \n",
       "25                   Oribacterium                  Lachnospiraceae   \n",
       "26                   Burkholderia                 Burkholderiaceae   \n",
       "27                       Sneathia                 Leptotrichiaceae   \n",
       "28                      Treponema                  Spirochaetaceae   \n",
       "29                      Moraxella                    Moraxellaceae   \n",
       "30                    Haemophilus                  Pasteurellaceae   \n",
       "31                    Selenomonas                  Veillonellaceae   \n",
       "32                Corynebacterium               Corynebacteriaceae   \n",
       "33                      Rhizobium                     Rhizobiaceae   \n",
       "34                 Bradyrhizobium                Bradyrhizobiaceae   \n",
       "35               Methylobacterium              Methylobacteriaceae   \n",
       "36      OD1_genera_incertae_sedis        OD1_genera_incertae_sedis   \n",
       "37                     Finegoldia  Clostridiales_Incertae_Sedis_XI   \n",
       "38                 Microbacterium                Microbacteriaceae   \n",
       "39                   Sphingomonas                Sphingomonadaceae   \n",
       "40               Chryseobacterium                Flavobacteriaceae   \n",
       "41                    Bacteroides                   Bacteroidaceae   \n",
       "42                   Bdellovibrio               Bdellovibrionaceae   \n",
       "43                   Streptophyta                      Chloroplast   \n",
       "44  Lachnospiracea_incertae_sedis                  Lachnospiraceae   \n",
       "45                     Paracoccus                 Rhodobacteraceae   \n",
       "46                 Fastidiosipila                  Ruminococcaceae   \n",
       "47                 Pseudonocardia               Pseudonocardiaceae   \n",
       "\n",
       "                        Order                      Class  \\\n",
       "0             Lactobacillales                    Bacilli   \n",
       "1             Actinomycetales             Actinobacteria   \n",
       "2             Selenomonadales              Negativicutes   \n",
       "3             Actinomycetales             Actinobacteria   \n",
       "4            Flavobacteriales              Flavobacteria   \n",
       "5               Bacteroidales                Bacteroidia   \n",
       "6               Bacteroidales                Bacteroidia   \n",
       "7               Clostridiales                 Clostridia   \n",
       "8             Fusobacteriales               Fusobacteria   \n",
       "9             Actinomycetales             Actinobacteria   \n",
       "10                 Bacillales                    Bacilli   \n",
       "11            Actinomycetales             Actinobacteria   \n",
       "12            Lactobacillales                    Bacilli   \n",
       "13               Neisseriales         Betaproteobacteria   \n",
       "14            Lactobacillales                    Bacilli   \n",
       "15            Selenomonadales              Negativicutes   \n",
       "16              Clostridiales                 Clostridia   \n",
       "17           Coriobacteriales             Actinobacteria   \n",
       "18          Campylobacterales      Epsilonproteobacteria   \n",
       "19           Flavobacteriales              Flavobacteria   \n",
       "20         Erysipelotrichales           Erysipelotrichia   \n",
       "21              Clostridiales                 Clostridia   \n",
       "22  TM7_genera_incertae_sedis  TM7_genera_incertae_sedis   \n",
       "23                 Bacillales                    Bacilli   \n",
       "24              Clostridiales                 Clostridia   \n",
       "25              Clostridiales                 Clostridia   \n",
       "26            Burkholderiales         Betaproteobacteria   \n",
       "27            Fusobacteriales               Fusobacteria   \n",
       "28             Spirochaetales               Spirochaetes   \n",
       "29            Pseudomonadales        Gammaproteobacteria   \n",
       "30             Pasteurellales        Gammaproteobacteria   \n",
       "31            Selenomonadales              Negativicutes   \n",
       "32            Actinomycetales             Actinobacteria   \n",
       "33                Rhizobiales        Alphaproteobacteria   \n",
       "34                Rhizobiales        Alphaproteobacteria   \n",
       "35                Rhizobiales        Alphaproteobacteria   \n",
       "36  OD1_genera_incertae_sedis  OD1_genera_incertae_sedis   \n",
       "37              Clostridiales                 Clostridia   \n",
       "38            Actinomycetales             Actinobacteria   \n",
       "39           Sphingomonadales        Alphaproteobacteria   \n",
       "40           Flavobacteriales              Flavobacteria   \n",
       "41              Bacteroidales                Bacteroidia   \n",
       "42          Bdellovibrionales        Deltaproteobacteria   \n",
       "43                Chloroplast                Chloroplast   \n",
       "44              Clostridiales                 Clostridia   \n",
       "45            Rhodobacterales        Alphaproteobacteria   \n",
       "46              Clostridiales                 Clostridia   \n",
       "47            Actinomycetales             Actinobacteria   \n",
       "\n",
       "                       Phylum    Domain  \n",
       "0                  Firmicutes  Bacteria  \n",
       "1              Actinobacteria  Bacteria  \n",
       "2                  Firmicutes  Bacteria  \n",
       "3              Actinobacteria  Bacteria  \n",
       "4               Bacteroidetes  Bacteria  \n",
       "5               Bacteroidetes  Bacteria  \n",
       "6               Bacteroidetes  Bacteria  \n",
       "7                  Firmicutes  Bacteria  \n",
       "8                Fusobacteria  Bacteria  \n",
       "9              Actinobacteria  Bacteria  \n",
       "10                 Firmicutes  Bacteria  \n",
       "11             Actinobacteria  Bacteria  \n",
       "12                 Firmicutes  Bacteria  \n",
       "13             Proteobacteria  Bacteria  \n",
       "14                 Firmicutes  Bacteria  \n",
       "15                 Firmicutes  Bacteria  \n",
       "16                 Firmicutes  Bacteria  \n",
       "17             Actinobacteria  Bacteria  \n",
       "18             Proteobacteria  Bacteria  \n",
       "19              Bacteroidetes  Bacteria  \n",
       "20                 Firmicutes  Bacteria  \n",
       "21                 Firmicutes  Bacteria  \n",
       "22                        TM7  Bacteria  \n",
       "23                 Firmicutes  Bacteria  \n",
       "24                 Firmicutes  Bacteria  \n",
       "25                 Firmicutes  Bacteria  \n",
       "26             Proteobacteria  Bacteria  \n",
       "27               Fusobacteria  Bacteria  \n",
       "28               Spirochaetes  Bacteria  \n",
       "29             Proteobacteria  Bacteria  \n",
       "30             Proteobacteria  Bacteria  \n",
       "31                 Firmicutes  Bacteria  \n",
       "32             Actinobacteria  Bacteria  \n",
       "33             Proteobacteria  Bacteria  \n",
       "34             Proteobacteria  Bacteria  \n",
       "35             Proteobacteria  Bacteria  \n",
       "36                        OD1  Bacteria  \n",
       "37                 Firmicutes  Bacteria  \n",
       "38             Actinobacteria  Bacteria  \n",
       "39             Proteobacteria  Bacteria  \n",
       "40              Bacteroidetes  Bacteria  \n",
       "41              Bacteroidetes  Bacteria  \n",
       "42             Proteobacteria  Bacteria  \n",
       "43  Cyanobacteria_Chloroplast  Bacteria  \n",
       "44                 Firmicutes  Bacteria  \n",
       "45             Proteobacteria  Bacteria  \n",
       "46                 Firmicutes  Bacteria  \n",
       "47             Actinobacteria  Bacteria  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_information = pd.read_csv(resource_filename('deepbiome', 'tests/data/genus48_dic.csv'))\n",
    "tree_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of the list of the name of input files\n",
    "\n",
    "In this example. we assume that input is given by the lists of files. Each file has all sample's information for one repeatition.\n",
    "If we want to use the list of the input files, we need the make a list of the name of each input file with the format below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gcount_0001.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gcount_0002.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gcount_0003.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gcount_0004.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gcount_0005.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "0  gcount_0001.csv\n",
       "1  gcount_0002.csv\n",
       "2  gcount_0003.csv\n",
       "3  gcount_0004.csv\n",
       "4  gcount_0005.csv"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_input_files = pd.read_csv(resource_filename('deepbiome', 'tests/data/gcount_list.csv'), header=None)\n",
    "list_of_input_files.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>gcount_0996.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>gcount_0997.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>gcount_0998.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>gcount_0999.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>gcount_1000.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0\n",
       "995  gcount_0996.csv\n",
       "996  gcount_0997.csv\n",
       "997  gcount_0998.csv\n",
       "998  gcount_0999.csv\n",
       "999  gcount_1000.csv"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_input_files.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of the lists of the input files\n",
    "\n",
    "Below is an example of the each input file. This example has 1000 samples for each row, and abandunt of each microbiome for each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Streptococcus</th>\n",
       "      <th>Tropheryma</th>\n",
       "      <th>Veillonella</th>\n",
       "      <th>Actinomyces</th>\n",
       "      <th>Flavobacterium</th>\n",
       "      <th>Prevotella</th>\n",
       "      <th>Porphyromonas</th>\n",
       "      <th>Parvimonas</th>\n",
       "      <th>Fusobacterium</th>\n",
       "      <th>Propionibacterium</th>\n",
       "      <th>...</th>\n",
       "      <th>Microbacterium</th>\n",
       "      <th>Sphingomonas</th>\n",
       "      <th>Chryseobacterium</th>\n",
       "      <th>Bacteroides</th>\n",
       "      <th>Bdellovibrio</th>\n",
       "      <th>Streptophyta</th>\n",
       "      <th>Lachnospiracea_incertae_sedis</th>\n",
       "      <th>Paracoccus</th>\n",
       "      <th>Fastidiosipila</th>\n",
       "      <th>Pseudonocardia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>841</td>\n",
       "      <td>0</td>\n",
       "      <td>813</td>\n",
       "      <td>505</td>\n",
       "      <td>5</td>\n",
       "      <td>3224</td>\n",
       "      <td>0</td>\n",
       "      <td>362</td>\n",
       "      <td>11</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1445</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>573</td>\n",
       "      <td>0</td>\n",
       "      <td>1278</td>\n",
       "      <td>82</td>\n",
       "      <td>85</td>\n",
       "      <td>69</td>\n",
       "      <td>154</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1259</td>\n",
       "      <td>0</td>\n",
       "      <td>805</td>\n",
       "      <td>650</td>\n",
       "      <td>0</td>\n",
       "      <td>1088</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>982</td>\n",
       "      <td>0</td>\n",
       "      <td>327</td>\n",
       "      <td>594</td>\n",
       "      <td>0</td>\n",
       "      <td>960</td>\n",
       "      <td>81</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>157</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1162</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>969</td>\n",
       "      <td>163</td>\n",
       "      <td>1515</td>\n",
       "      <td>167</td>\n",
       "      <td>4</td>\n",
       "      <td>162</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Streptococcus  Tropheryma  Veillonella  Actinomyces  Flavobacterium  \\\n",
       "0            841           0          813          505               5   \n",
       "1           1445           0            1          573               0   \n",
       "2           1259           0          805          650               0   \n",
       "3            982           0          327          594               0   \n",
       "4           1162           0          130          969             163   \n",
       "\n",
       "   Prevotella  Porphyromonas  Parvimonas  Fusobacterium  Propionibacterium  \\\n",
       "0        3224              0         362             11                 65   \n",
       "1        1278             82          85             69                154   \n",
       "2        1088              0           0             74                  0   \n",
       "3         960             81          19              9                  0   \n",
       "4        1515            167           4            162                  3   \n",
       "\n",
       "   ...  Microbacterium  Sphingomonas  Chryseobacterium  Bacteroides  \\\n",
       "0  ...               0            87                 0            0   \n",
       "1  ...               0             1                 2            0   \n",
       "2  ...               0             2                 8            1   \n",
       "3  ...             157             1                 0            4   \n",
       "4  ...               0             9                 0            0   \n",
       "\n",
       "   Bdellovibrio  Streptophyta  Lachnospiracea_incertae_sedis  Paracoccus  \\\n",
       "0             0             0                              0           0   \n",
       "1             0             0                              0           0   \n",
       "2            39             0                              0           0   \n",
       "3            60             0                              0           0   \n",
       "4             0             0                             60           0   \n",
       "\n",
       "   Fastidiosipila  Pseudonocardia  \n",
       "0               0            2133  \n",
       "1               0            3638  \n",
       "2               0            3445  \n",
       "3               0            3507  \n",
       "4               0            3945  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1 = pd.read_csv(resource_filename('deepbiome', 'tests/data/count/%s' % list_of_input_files.iloc[0,0]))\n",
    "x_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Streptococcus</th>\n",
       "      <th>Tropheryma</th>\n",
       "      <th>Veillonella</th>\n",
       "      <th>Actinomyces</th>\n",
       "      <th>Flavobacterium</th>\n",
       "      <th>Prevotella</th>\n",
       "      <th>Porphyromonas</th>\n",
       "      <th>Parvimonas</th>\n",
       "      <th>Fusobacterium</th>\n",
       "      <th>Propionibacterium</th>\n",
       "      <th>...</th>\n",
       "      <th>Microbacterium</th>\n",
       "      <th>Sphingomonas</th>\n",
       "      <th>Chryseobacterium</th>\n",
       "      <th>Bacteroides</th>\n",
       "      <th>Bdellovibrio</th>\n",
       "      <th>Streptophyta</th>\n",
       "      <th>Lachnospiracea_incertae_sedis</th>\n",
       "      <th>Paracoccus</th>\n",
       "      <th>Fastidiosipila</th>\n",
       "      <th>Pseudonocardia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1401</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>526</td>\n",
       "      <td>0</td>\n",
       "      <td>923</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>2655</td>\n",
       "      <td>6</td>\n",
       "      <td>106</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>952</td>\n",
       "      <td>76</td>\n",
       "      <td>13</td>\n",
       "      <td>158</td>\n",
       "      <td>125</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>335</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>259</td>\n",
       "      <td>67</td>\n",
       "      <td>718</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>167</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>246</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>649</td>\n",
       "      <td>69</td>\n",
       "      <td>966</td>\n",
       "      <td>1227</td>\n",
       "      <td>0</td>\n",
       "      <td>508</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>550</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1119</td>\n",
       "      <td>0</td>\n",
       "      <td>2348</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>176</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Streptococcus  Tropheryma  Veillonella  Actinomyces  Flavobacterium  \\\n",
       "995           1401           4           30          526               0   \n",
       "996           2655           6          106           74               0   \n",
       "997            335           0           71          259              67   \n",
       "998            649          69          966         1227               0   \n",
       "999           1258           0            0         1119               0   \n",
       "\n",
       "     Prevotella  Porphyromonas  Parvimonas  Fusobacterium  Propionibacterium  \\\n",
       "995         923             25           0            127                  0   \n",
       "996         952             76          13            158                125   \n",
       "997         718              1           4              4                167   \n",
       "998         508              2          30            550                  0   \n",
       "999        2348             25           0            137                176   \n",
       "\n",
       "     ...  Microbacterium  Sphingomonas  Chryseobacterium  Bacteroides  \\\n",
       "995  ...               0             0                 7            0   \n",
       "996  ...               0             2                 0            0   \n",
       "997  ...               0           246                 0            0   \n",
       "998  ...               0             0                 0            0   \n",
       "999  ...               0             2                 0            0   \n",
       "\n",
       "     Bdellovibrio  Streptophyta  Lachnospiracea_incertae_sedis  Paracoccus  \\\n",
       "995             0             0                              0           0   \n",
       "996             0             0                              0           0   \n",
       "997             6             0                              0           0   \n",
       "998             0             6                              0           0   \n",
       "999             0             0                              0           0   \n",
       "\n",
       "     Fastidiosipila  Pseudonocardia  \n",
       "995               0            4470  \n",
       "996               0            2826  \n",
       "997               0            6527  \n",
       "998               0            4402  \n",
       "999               0            2585  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of the Y (regression)\n",
    "\n",
    "This is an example of the output file for regression problem. Below example file has 1000 samples in row, 1000 repeatition in column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>...</th>\n",
       "      <th>x991</th>\n",
       "      <th>x992</th>\n",
       "      <th>x993</th>\n",
       "      <th>x994</th>\n",
       "      <th>x995</th>\n",
       "      <th>x996</th>\n",
       "      <th>x997</th>\n",
       "      <th>x998</th>\n",
       "      <th>x999</th>\n",
       "      <th>x1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.997270</td>\n",
       "      <td>5.492354</td>\n",
       "      <td>5.473725</td>\n",
       "      <td>1.759484</td>\n",
       "      <td>5.313252</td>\n",
       "      <td>1.500044</td>\n",
       "      <td>4.949712</td>\n",
       "      <td>5.493533</td>\n",
       "      <td>3.743509</td>\n",
       "      <td>5.492373</td>\n",
       "      <td>...</td>\n",
       "      <td>2.793883</td>\n",
       "      <td>1.500004</td>\n",
       "      <td>5.487526</td>\n",
       "      <td>5.493518</td>\n",
       "      <td>3.599047</td>\n",
       "      <td>5.491461</td>\n",
       "      <td>5.486244</td>\n",
       "      <td>5.487390</td>\n",
       "      <td>5.493492</td>\n",
       "      <td>3.762523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.004092</td>\n",
       "      <td>1.500002</td>\n",
       "      <td>4.640348</td>\n",
       "      <td>1.538071</td>\n",
       "      <td>5.491065</td>\n",
       "      <td>5.481009</td>\n",
       "      <td>5.492323</td>\n",
       "      <td>2.968531</td>\n",
       "      <td>3.576358</td>\n",
       "      <td>5.491456</td>\n",
       "      <td>...</td>\n",
       "      <td>1.500033</td>\n",
       "      <td>3.369529</td>\n",
       "      <td>1.500016</td>\n",
       "      <td>3.103297</td>\n",
       "      <td>5.493214</td>\n",
       "      <td>3.831125</td>\n",
       "      <td>5.492104</td>\n",
       "      <td>5.474811</td>\n",
       "      <td>5.492416</td>\n",
       "      <td>3.268805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.485126</td>\n",
       "      <td>4.187426</td>\n",
       "      <td>5.491340</td>\n",
       "      <td>5.469662</td>\n",
       "      <td>5.490478</td>\n",
       "      <td>1.953375</td>\n",
       "      <td>5.494656</td>\n",
       "      <td>3.741680</td>\n",
       "      <td>4.862400</td>\n",
       "      <td>5.490701</td>\n",
       "      <td>...</td>\n",
       "      <td>5.491728</td>\n",
       "      <td>2.459981</td>\n",
       "      <td>5.475697</td>\n",
       "      <td>3.114158</td>\n",
       "      <td>1.500004</td>\n",
       "      <td>1.500019</td>\n",
       "      <td>4.113815</td>\n",
       "      <td>5.470539</td>\n",
       "      <td>5.494373</td>\n",
       "      <td>5.481754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.489590</td>\n",
       "      <td>4.863187</td>\n",
       "      <td>1.500003</td>\n",
       "      <td>5.484699</td>\n",
       "      <td>5.492657</td>\n",
       "      <td>5.491270</td>\n",
       "      <td>4.091023</td>\n",
       "      <td>5.495239</td>\n",
       "      <td>5.492804</td>\n",
       "      <td>1.500046</td>\n",
       "      <td>...</td>\n",
       "      <td>1.500034</td>\n",
       "      <td>1.500012</td>\n",
       "      <td>5.483070</td>\n",
       "      <td>2.475049</td>\n",
       "      <td>5.493846</td>\n",
       "      <td>3.287076</td>\n",
       "      <td>3.696412</td>\n",
       "      <td>5.487583</td>\n",
       "      <td>1.500044</td>\n",
       "      <td>2.760404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.500001</td>\n",
       "      <td>5.480769</td>\n",
       "      <td>5.489725</td>\n",
       "      <td>1.500044</td>\n",
       "      <td>2.695212</td>\n",
       "      <td>5.492262</td>\n",
       "      <td>3.381424</td>\n",
       "      <td>4.805420</td>\n",
       "      <td>1.500047</td>\n",
       "      <td>5.474376</td>\n",
       "      <td>...</td>\n",
       "      <td>1.500046</td>\n",
       "      <td>2.586990</td>\n",
       "      <td>5.440610</td>\n",
       "      <td>4.376103</td>\n",
       "      <td>1.500030</td>\n",
       "      <td>4.713223</td>\n",
       "      <td>5.491059</td>\n",
       "      <td>3.230658</td>\n",
       "      <td>1.500045</td>\n",
       "      <td>5.488727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2        x3        x4        x5        x6        x7  \\\n",
       "0  4.997270  5.492354  5.473725  1.759484  5.313252  1.500044  4.949712   \n",
       "1  5.004092  1.500002  4.640348  1.538071  5.491065  5.481009  5.492323   \n",
       "2  5.485126  4.187426  5.491340  5.469662  5.490478  1.953375  5.494656   \n",
       "3  5.489590  4.863187  1.500003  5.484699  5.492657  5.491270  4.091023   \n",
       "4  1.500001  5.480769  5.489725  1.500044  2.695212  5.492262  3.381424   \n",
       "\n",
       "         x8        x9       x10  ...      x991      x992      x993      x994  \\\n",
       "0  5.493533  3.743509  5.492373  ...  2.793883  1.500004  5.487526  5.493518   \n",
       "1  2.968531  3.576358  5.491456  ...  1.500033  3.369529  1.500016  3.103297   \n",
       "2  3.741680  4.862400  5.490701  ...  5.491728  2.459981  5.475697  3.114158   \n",
       "3  5.495239  5.492804  1.500046  ...  1.500034  1.500012  5.483070  2.475049   \n",
       "4  4.805420  1.500047  5.474376  ...  1.500046  2.586990  5.440610  4.376103   \n",
       "\n",
       "       x995      x996      x997      x998      x999     x1000  \n",
       "0  3.599047  5.491461  5.486244  5.487390  5.493492  3.762523  \n",
       "1  5.493214  3.831125  5.492104  5.474811  5.492416  3.268805  \n",
       "2  1.500004  1.500019  4.113815  5.470539  5.494373  5.481754  \n",
       "3  5.493846  3.287076  3.696412  5.487583  1.500044  2.760404  \n",
       "4  1.500030  4.713223  5.491059  3.230658  1.500045  5.488727  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.read_csv(resource_filename('deepbiome', 'tests/data/regression_y.csv'))\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>...</th>\n",
       "      <th>x991</th>\n",
       "      <th>x992</th>\n",
       "      <th>x993</th>\n",
       "      <th>x994</th>\n",
       "      <th>x995</th>\n",
       "      <th>x996</th>\n",
       "      <th>x997</th>\n",
       "      <th>x998</th>\n",
       "      <th>x999</th>\n",
       "      <th>x1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>2.609926</td>\n",
       "      <td>5.491258</td>\n",
       "      <td>3.318610</td>\n",
       "      <td>5.444070</td>\n",
       "      <td>2.884154</td>\n",
       "      <td>5.486857</td>\n",
       "      <td>5.496554</td>\n",
       "      <td>1.500019</td>\n",
       "      <td>5.482893</td>\n",
       "      <td>1.824835</td>\n",
       "      <td>...</td>\n",
       "      <td>4.478641</td>\n",
       "      <td>5.485122</td>\n",
       "      <td>4.915985</td>\n",
       "      <td>4.073239</td>\n",
       "      <td>1.500019</td>\n",
       "      <td>5.492295</td>\n",
       "      <td>1.500005</td>\n",
       "      <td>1.559586</td>\n",
       "      <td>5.496415</td>\n",
       "      <td>4.171127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>5.488959</td>\n",
       "      <td>3.739806</td>\n",
       "      <td>5.489474</td>\n",
       "      <td>1.500021</td>\n",
       "      <td>5.492632</td>\n",
       "      <td>1.500019</td>\n",
       "      <td>5.484813</td>\n",
       "      <td>5.467055</td>\n",
       "      <td>5.491282</td>\n",
       "      <td>1.874777</td>\n",
       "      <td>...</td>\n",
       "      <td>5.498820</td>\n",
       "      <td>5.493926</td>\n",
       "      <td>5.487404</td>\n",
       "      <td>3.162812</td>\n",
       "      <td>1.846298</td>\n",
       "      <td>5.492417</td>\n",
       "      <td>1.919107</td>\n",
       "      <td>5.480324</td>\n",
       "      <td>5.467765</td>\n",
       "      <td>5.457627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>3.498418</td>\n",
       "      <td>4.250451</td>\n",
       "      <td>5.488116</td>\n",
       "      <td>4.162031</td>\n",
       "      <td>5.494052</td>\n",
       "      <td>5.472900</td>\n",
       "      <td>1.500057</td>\n",
       "      <td>5.491497</td>\n",
       "      <td>5.491935</td>\n",
       "      <td>1.500033</td>\n",
       "      <td>...</td>\n",
       "      <td>1.966474</td>\n",
       "      <td>5.475258</td>\n",
       "      <td>3.848034</td>\n",
       "      <td>2.863883</td>\n",
       "      <td>4.370685</td>\n",
       "      <td>5.494647</td>\n",
       "      <td>5.478855</td>\n",
       "      <td>2.465739</td>\n",
       "      <td>1.500018</td>\n",
       "      <td>5.486403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>5.486107</td>\n",
       "      <td>1.917414</td>\n",
       "      <td>5.414975</td>\n",
       "      <td>5.492364</td>\n",
       "      <td>2.027914</td>\n",
       "      <td>5.491349</td>\n",
       "      <td>5.494135</td>\n",
       "      <td>5.491245</td>\n",
       "      <td>1.500039</td>\n",
       "      <td>1.500019</td>\n",
       "      <td>...</td>\n",
       "      <td>4.556995</td>\n",
       "      <td>5.457072</td>\n",
       "      <td>2.071106</td>\n",
       "      <td>5.417333</td>\n",
       "      <td>5.491818</td>\n",
       "      <td>5.473390</td>\n",
       "      <td>4.374154</td>\n",
       "      <td>5.489109</td>\n",
       "      <td>4.515340</td>\n",
       "      <td>1.500020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>5.319623</td>\n",
       "      <td>5.482776</td>\n",
       "      <td>1.500035</td>\n",
       "      <td>5.485141</td>\n",
       "      <td>5.491019</td>\n",
       "      <td>3.733982</td>\n",
       "      <td>5.494374</td>\n",
       "      <td>3.077159</td>\n",
       "      <td>5.493188</td>\n",
       "      <td>1.500001</td>\n",
       "      <td>...</td>\n",
       "      <td>5.485356</td>\n",
       "      <td>1.500059</td>\n",
       "      <td>5.400762</td>\n",
       "      <td>5.489606</td>\n",
       "      <td>5.494583</td>\n",
       "      <td>5.490943</td>\n",
       "      <td>5.123794</td>\n",
       "      <td>5.473465</td>\n",
       "      <td>3.274979</td>\n",
       "      <td>3.700653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x1        x2        x3        x4        x5        x6        x7  \\\n",
       "995  2.609926  5.491258  3.318610  5.444070  2.884154  5.486857  5.496554   \n",
       "996  5.488959  3.739806  5.489474  1.500021  5.492632  1.500019  5.484813   \n",
       "997  3.498418  4.250451  5.488116  4.162031  5.494052  5.472900  1.500057   \n",
       "998  5.486107  1.917414  5.414975  5.492364  2.027914  5.491349  5.494135   \n",
       "999  5.319623  5.482776  1.500035  5.485141  5.491019  3.733982  5.494374   \n",
       "\n",
       "           x8        x9       x10  ...      x991      x992      x993  \\\n",
       "995  1.500019  5.482893  1.824835  ...  4.478641  5.485122  4.915985   \n",
       "996  5.467055  5.491282  1.874777  ...  5.498820  5.493926  5.487404   \n",
       "997  5.491497  5.491935  1.500033  ...  1.966474  5.475258  3.848034   \n",
       "998  5.491245  1.500039  1.500019  ...  4.556995  5.457072  2.071106   \n",
       "999  3.077159  5.493188  1.500001  ...  5.485356  1.500059  5.400762   \n",
       "\n",
       "         x994      x995      x996      x997      x998      x999     x1000  \n",
       "995  4.073239  1.500019  5.492295  1.500005  1.559586  5.496415  4.171127  \n",
       "996  3.162812  1.846298  5.492417  1.919107  5.480324  5.467765  5.457627  \n",
       "997  2.863883  4.370685  5.494647  5.478855  2.465739  1.500018  5.486403  \n",
       "998  5.417333  5.491818  5.473390  4.374154  5.489109  4.515340  1.500020  \n",
       "999  5.489606  5.494583  5.490943  5.123794  5.473465  3.274979  3.700653  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one repeatition, the deepbiome will use the one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4.997270\n",
       "1    5.004092\n",
       "2    5.485126\n",
       "3    5.489590\n",
       "4    1.500001\n",
       "Name: x1, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.iloc[:,0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "995    2.609926\n",
       "996    5.488959\n",
       "997    3.498418\n",
       "998    5.486107\n",
       "999    5.319623\n",
       "Name: x1, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.iloc[:,0].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of the Y (classification)\n",
    "\n",
    "This is an example of the output file for classification problem. Below example file has 1000 samples in row, 1000 repeatition in column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V991</th>\n",
       "      <th>V992</th>\n",
       "      <th>V993</th>\n",
       "      <th>V994</th>\n",
       "      <th>V995</th>\n",
       "      <th>V996</th>\n",
       "      <th>V997</th>\n",
       "      <th>V998</th>\n",
       "      <th>V999</th>\n",
       "      <th>V1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    V1   V2   V3   V4   V5   V6   V7   V8   V9  V10  ...  V991  V992  V993  \\\n",
       "0  1.0  0.0  1.0  1.0  1.0  1.0  1.0  0.0  1.0  0.0  ...   1.0   1.0   0.0   \n",
       "1  1.0  1.0  1.0  1.0  0.0  1.0  0.0  1.0  1.0  0.0  ...   1.0   1.0   1.0   \n",
       "2  0.0  1.0  0.0  1.0  0.0  1.0  0.0  1.0  1.0  0.0  ...   0.0   1.0   1.0   \n",
       "3  0.0  1.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0  ...   1.0   1.0   0.0   \n",
       "4  1.0  1.0  0.0  1.0  1.0  0.0  1.0  1.0  1.0  1.0  ...   1.0   1.0   1.0   \n",
       "\n",
       "   V994  V995  V996  V997  V998  V999  V1000  \n",
       "0   0.0   1.0   0.0   0.0   0.0   0.0    1.0  \n",
       "1   1.0   0.0   1.0   0.0   1.0   0.0    1.0  \n",
       "2   1.0   1.0   1.0   1.0   1.0   0.0    0.0  \n",
       "3   1.0   0.0   1.0   1.0   0.0   1.0    1.0  \n",
       "4   1.0   1.0   1.0   0.0   1.0   1.0    0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.read_csv(resource_filename('deepbiome', 'tests/data/classification_y.csv'))\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V991</th>\n",
       "      <th>V992</th>\n",
       "      <th>V993</th>\n",
       "      <th>V994</th>\n",
       "      <th>V995</th>\n",
       "      <th>V996</th>\n",
       "      <th>V997</th>\n",
       "      <th>V998</th>\n",
       "      <th>V999</th>\n",
       "      <th>V1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      V1   V2   V3   V4   V5   V6   V7   V8   V9  V10  ...  V991  V992  V993  \\\n",
       "995  1.0  0.0  1.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  ...   1.0   1.0   1.0   \n",
       "996  0.0  1.0  0.0  1.0  0.0  1.0  1.0  1.0  0.0  1.0  ...   0.0   0.0   0.0   \n",
       "997  1.0  1.0  0.0  1.0  0.0  1.0  1.0  0.0  0.0  1.0  ...   1.0   1.0   1.0   \n",
       "998  0.0  1.0  1.0  0.0  1.0  0.0  0.0  0.0  1.0  1.0  ...   1.0   1.0   1.0   \n",
       "999  1.0  1.0  1.0  0.0  0.0  1.0  0.0  1.0  0.0  1.0  ...   0.0   1.0   1.0   \n",
       "\n",
       "     V994  V995  V996  V997  V998  V999  V1000  \n",
       "995   1.0   1.0   0.0   1.0   1.0   0.0    1.0  \n",
       "996   1.0   1.0   0.0   1.0   0.0   1.0    1.0  \n",
       "997   1.0   1.0   0.0   1.0   1.0   1.0    0.0  \n",
       "998   1.0   0.0   1.0   1.0   0.0   1.0    1.0  \n",
       "999   0.0   0.0   0.0   1.0   1.0   1.0    1.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one repeatition, the deepbiome will use the one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    1.0\n",
       "2    0.0\n",
       "3    0.0\n",
       "4    1.0\n",
       "Name: V1, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.iloc[:,0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "995    1.0\n",
       "996    0.0\n",
       "997    1.0\n",
       "998    0.0\n",
       "999    1.0\n",
       "Name: V1, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.iloc[:,0].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exmple of the training index file for repetition\n",
    "\n",
    "For each repeatition, we have to set the training and test set. If the index file is given, the deepbiome library set the training set and test set based on the index file. Below is the example of the index file. Each column has the training indexs for each repeatition. The deepbiome will only use the samples in this index set for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V991</th>\n",
       "      <th>V992</th>\n",
       "      <th>V993</th>\n",
       "      <th>V994</th>\n",
       "      <th>V995</th>\n",
       "      <th>V996</th>\n",
       "      <th>V997</th>\n",
       "      <th>V998</th>\n",
       "      <th>V999</th>\n",
       "      <th>V1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>490</td>\n",
       "      <td>690</td>\n",
       "      <td>62</td>\n",
       "      <td>703</td>\n",
       "      <td>690</td>\n",
       "      <td>845</td>\n",
       "      <td>150</td>\n",
       "      <td>268</td>\n",
       "      <td>488</td>\n",
       "      <td>179</td>\n",
       "      <td>...</td>\n",
       "      <td>675</td>\n",
       "      <td>886</td>\n",
       "      <td>225</td>\n",
       "      <td>222</td>\n",
       "      <td>781</td>\n",
       "      <td>778</td>\n",
       "      <td>603</td>\n",
       "      <td>222</td>\n",
       "      <td>254</td>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>498</td>\n",
       "      <td>968</td>\n",
       "      <td>123</td>\n",
       "      <td>913</td>\n",
       "      <td>348</td>\n",
       "      <td>262</td>\n",
       "      <td>705</td>\n",
       "      <td>239</td>\n",
       "      <td>632</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>636</td>\n",
       "      <td>216</td>\n",
       "      <td>495</td>\n",
       "      <td>557</td>\n",
       "      <td>196</td>\n",
       "      <td>516</td>\n",
       "      <td>23</td>\n",
       "      <td>351</td>\n",
       "      <td>472</td>\n",
       "      <td>945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>389</td>\n",
       "      <td>999</td>\n",
       "      <td>335</td>\n",
       "      <td>947</td>\n",
       "      <td>215</td>\n",
       "      <td>696</td>\n",
       "      <td>793</td>\n",
       "      <td>349</td>\n",
       "      <td>734</td>\n",
       "      <td>624</td>\n",
       "      <td>...</td>\n",
       "      <td>626</td>\n",
       "      <td>230</td>\n",
       "      <td>26</td>\n",
       "      <td>330</td>\n",
       "      <td>470</td>\n",
       "      <td>992</td>\n",
       "      <td>329</td>\n",
       "      <td>532</td>\n",
       "      <td>655</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51</td>\n",
       "      <td>139</td>\n",
       "      <td>843</td>\n",
       "      <td>491</td>\n",
       "      <td>47</td>\n",
       "      <td>421</td>\n",
       "      <td>892</td>\n",
       "      <td>32</td>\n",
       "      <td>438</td>\n",
       "      <td>996</td>\n",
       "      <td>...</td>\n",
       "      <td>956</td>\n",
       "      <td>706</td>\n",
       "      <td>836</td>\n",
       "      <td>151</td>\n",
       "      <td>80</td>\n",
       "      <td>409</td>\n",
       "      <td>671</td>\n",
       "      <td>772</td>\n",
       "      <td>882</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>592</td>\n",
       "      <td>83</td>\n",
       "      <td>204</td>\n",
       "      <td>810</td>\n",
       "      <td>198</td>\n",
       "      <td>955</td>\n",
       "      <td>357</td>\n",
       "      <td>125</td>\n",
       "      <td>190</td>\n",
       "      <td>162</td>\n",
       "      <td>...</td>\n",
       "      <td>542</td>\n",
       "      <td>108</td>\n",
       "      <td>959</td>\n",
       "      <td>311</td>\n",
       "      <td>771</td>\n",
       "      <td>902</td>\n",
       "      <td>986</td>\n",
       "      <td>481</td>\n",
       "      <td>922</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    V1   V2   V3   V4   V5   V6   V7   V8   V9  V10  ...  V991  V992  V993  \\\n",
       "0  490  690   62  703  690  845  150  268  488  179  ...   675   886   225   \n",
       "1  498  968  123  913  348  262  705  239  632   44  ...   636   216   495   \n",
       "2  389  999  335  947  215  696  793  349  734  624  ...   626   230    26   \n",
       "3   51  139  843  491   47  421  892   32  438  996  ...   956   706   836   \n",
       "4  592   83  204  810  198  955  357  125  190  162  ...   542   108   959   \n",
       "\n",
       "   V994  V995  V996  V997  V998  V999  V1000  \n",
       "0   222   781   778   603   222   254    407  \n",
       "1   557   196   516    23   351   472    945  \n",
       "2   330   470   992   329   532   655    426  \n",
       "3   151    80   409   671   772   882    181  \n",
       "4   311   771   902   986   481   922    305  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = pd.read_csv(resource_filename('deepbiome', 'tests/data/regression_idx.csv'), dtype=np.int)\n",
    "idxs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V991</th>\n",
       "      <th>V992</th>\n",
       "      <th>V993</th>\n",
       "      <th>V994</th>\n",
       "      <th>V995</th>\n",
       "      <th>V996</th>\n",
       "      <th>V997</th>\n",
       "      <th>V998</th>\n",
       "      <th>V999</th>\n",
       "      <th>V1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>599</td>\n",
       "      <td>824</td>\n",
       "      <td>997</td>\n",
       "      <td>216</td>\n",
       "      <td>586</td>\n",
       "      <td>796</td>\n",
       "      <td>806</td>\n",
       "      <td>39</td>\n",
       "      <td>483</td>\n",
       "      <td>518</td>\n",
       "      <td>...</td>\n",
       "      <td>573</td>\n",
       "      <td>861</td>\n",
       "      <td>366</td>\n",
       "      <td>374</td>\n",
       "      <td>585</td>\n",
       "      <td>871</td>\n",
       "      <td>140</td>\n",
       "      <td>597</td>\n",
       "      <td>795</td>\n",
       "      <td>743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>720</td>\n",
       "      <td>633</td>\n",
       "      <td>821</td>\n",
       "      <td>149</td>\n",
       "      <td>339</td>\n",
       "      <td>461</td>\n",
       "      <td>750</td>\n",
       "      <td>194</td>\n",
       "      <td>769</td>\n",
       "      <td>699</td>\n",
       "      <td>...</td>\n",
       "      <td>913</td>\n",
       "      <td>570</td>\n",
       "      <td>670</td>\n",
       "      <td>249</td>\n",
       "      <td>840</td>\n",
       "      <td>889</td>\n",
       "      <td>242</td>\n",
       "      <td>959</td>\n",
       "      <td>791</td>\n",
       "      <td>954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>80</td>\n",
       "      <td>268</td>\n",
       "      <td>661</td>\n",
       "      <td>187</td>\n",
       "      <td>929</td>\n",
       "      <td>469</td>\n",
       "      <td>481</td>\n",
       "      <td>332</td>\n",
       "      <td>781</td>\n",
       "      <td>615</td>\n",
       "      <td>...</td>\n",
       "      <td>985</td>\n",
       "      <td>459</td>\n",
       "      <td>965</td>\n",
       "      <td>888</td>\n",
       "      <td>461</td>\n",
       "      <td>551</td>\n",
       "      <td>465</td>\n",
       "      <td>827</td>\n",
       "      <td>557</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>570</td>\n",
       "      <td>32</td>\n",
       "      <td>750</td>\n",
       "      <td>332</td>\n",
       "      <td>902</td>\n",
       "      <td>107</td>\n",
       "      <td>281</td>\n",
       "      <td>667</td>\n",
       "      <td>917</td>\n",
       "      <td>793</td>\n",
       "      <td>...</td>\n",
       "      <td>924</td>\n",
       "      <td>662</td>\n",
       "      <td>975</td>\n",
       "      <td>199</td>\n",
       "      <td>32</td>\n",
       "      <td>715</td>\n",
       "      <td>668</td>\n",
       "      <td>241</td>\n",
       "      <td>299</td>\n",
       "      <td>518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>440</td>\n",
       "      <td>589</td>\n",
       "      <td>607</td>\n",
       "      <td>597</td>\n",
       "      <td>380</td>\n",
       "      <td>961</td>\n",
       "      <td>747</td>\n",
       "      <td>396</td>\n",
       "      <td>649</td>\n",
       "      <td>974</td>\n",
       "      <td>...</td>\n",
       "      <td>867</td>\n",
       "      <td>839</td>\n",
       "      <td>234</td>\n",
       "      <td>99</td>\n",
       "      <td>901</td>\n",
       "      <td>19</td>\n",
       "      <td>821</td>\n",
       "      <td>450</td>\n",
       "      <td>780</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      V1   V2   V3   V4   V5   V6   V7   V8   V9  V10  ...  V991  V992  V993  \\\n",
       "745  599  824  997  216  586  796  806   39  483  518  ...   573   861   366   \n",
       "746  720  633  821  149  339  461  750  194  769  699  ...   913   570   670   \n",
       "747   80  268  661  187  929  469  481  332  781  615  ...   985   459   965   \n",
       "748  570   32  750  332  902  107  281  667  917  793  ...   924   662   975   \n",
       "749  440  589  607  597  380  961  747  396  649  974  ...   867   839   234   \n",
       "\n",
       "     V994  V995  V996  V997  V998  V999  V1000  \n",
       "745   374   585   871   140   597   795    743  \n",
       "746   249   840   889   242   959   791    954  \n",
       "747   888   461   551   465   827   557    662  \n",
       "748   199    32   715   668   241   299    518  \n",
       "749    99   901    19   821   450   780    326  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the index set for 1st repeatition. From 1000 samples above, it uses 750 samples for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    490\n",
       "1    498\n",
       "2    389\n",
       "3     51\n",
       "4    592\n",
       "Name: V1, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs.iloc[:,0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "745    599\n",
       "746    720\n",
       "747     80\n",
       "748    570\n",
       "749    440\n",
       "Name: V1, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs.iloc[:,0].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare the configuration\n",
    "\n",
    "For detailed configuration, we used python dictionary as inputs for the main training function.\n",
    "You can build the configuration information for the network training by:\n",
    "1. the python dictionary format\n",
    "1. the configufation file (.cfg).\n",
    "\n",
    "In this notebook, we showed the dictionary python dictionary format configuration.\n",
    "\n",
    "Please check the detailed information about each options in the [documantation](https://young-won.github.io/deepbiome/prerequisites.html#configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For preparing the configuration about the network information (`network_info`)\n",
    "\n",
    "For giving the information about the training hyper-parameter, you have to provide the dictionary for configuration to netowrk_info field.\n",
    "Your configuration for the network training should include the information about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network_info = {\n",
    "    'architecture_info': {\n",
    "        'batch_normalization': 'False',\n",
    "        'drop_out': '0',\n",
    "        'weight_initial': 'glorot_uniform',\n",
    "        'weight_l1_penalty':'0.01',\n",
    "        'weight_decay': 'phylogenetic_tree',\n",
    "    },\n",
    "    'model_info': {\n",
    "        'lr': '0.01',\n",
    "        'decay': '0.001',\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'metrics': 'binary_accuracy, sensitivity, specificity, gmeasure, auc',\n",
    "        'texa_selection_metrics': 'accuracy, sensitivity, specificity, gmeasure',\n",
    "        'network_class': 'DeepBiomeNetwork',\n",
    "        'optimizer': 'adam',\n",
    "        'reader_class': 'MicroBiomeClassificationReader',\n",
    "        'normalizer': 'normalize_minmax',\n",
    "    },\n",
    "    'training_info': {\n",
    "        'batch_size': '50', \n",
    "        'epochs': '100'\n",
    "    },\n",
    "    'validation_info': {\n",
    "        'batch_size': 'None', \n",
    "        'validation_size': '0.2'\n",
    "    },\n",
    "    'test_info': {\n",
    "        'batch_size': 'None'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For preparing the configuration about the path information (`path_info`)\n",
    "\n",
    "For giving the information about the path of dataset, paths for saving the trained weight and the evaluation results, you have to provide the dictionary for configuration to path_info feild.\n",
    "Your configuration for the path information should include the information about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_info = {\n",
    "    'data_info': {\n",
    "        'count_list_path': resource_filename('deepbiome', 'tests/data/gcount_list.csv'),\n",
    "        'count_path': resource_filename('deepbiome', 'tests/data/count'),\n",
    "        'data_path': resource_filename('deepbiome', 'tests/data'),\n",
    "        'idx_path': resource_filename('deepbiome', 'tests/data/classification_idx.csv'),\n",
    "        'tree_info_path': resource_filename('deepbiome', 'tests/data/genus48_dic.csv'),\n",
    "        'x_path': '',\n",
    "        'y_path': 'classification_y.csv'\n",
    "    },\n",
    "    'model_info': {\n",
    "        'evaluation': 'eval.npy',\n",
    "        'history': 'hist.json',\n",
    "        'model_dir': './example_result/',\n",
    "        'weight': 'weight.h5'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deepbiome Training\n",
    "\n",
    "Now we can train the deepbiome network base on the configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For logging, we used the python logging library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format = '[%(name)-8s|%(levelname)s|%(filename)s:%(lineno)s] %(message)s',\n",
    "                    level=logging.DEBUG)\n",
    "log = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deeobiome_train function provide the test evaluation, train evaluation and the deepbiome network instance.\n",
    "\n",
    "If we set `number_of_fold`, then the deepbiome package do the cross-validation based on that value. If not, the deepbiome package do the cross-validation based on the index file. If both `number_of_fold` option and the index file is not given, then the library do leave-one-out-cross-validation (LOOCV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|deepbiome.py:100] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:137] -------1 simulation start!----------------------------------\n",
      "[root    |INFO|readers.py:58] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:59] Construct Dataset\n",
      "[root    |INFO|readers.py:60] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:61] Load data\n",
      "[root    |INFO|deepbiome.py:147] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:148] Build network for 1 simulation\n",
      "[root    |INFO|build_network.py:513] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:514] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv\n",
      "[root    |INFO|build_network.py:518] Phylogenetic tree level list: ['Genus', 'Family', 'Order', 'Class', 'Phylum']\n",
      "[root    |INFO|build_network.py:519] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:526]      Genus: 48\n",
      "[root    |INFO|build_network.py:526]     Family: 40\n",
      "[root    |INFO|build_network.py:526]      Order: 23\n",
      "[root    |INFO|build_network.py:526]      Class: 17\n",
      "[root    |INFO|build_network.py:526]     Phylum: 9\n",
      "[root    |INFO|build_network.py:529] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:530] Phylogenetic_tree_dict info: ['Genus', 'Number', 'Order', 'Class', 'Phylum', 'Family']\n",
      "[root    |INFO|build_network.py:531] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [ Genus, Family]\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [Family,  Order]\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [ Order,  Class]\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [ Class, Phylum]\n",
      "[root    |INFO|build_network.py:554] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:570] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:571] Build network based on phylogenetic tree information\n",
      "[root    |INFO|build_network.py:572] ------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:432: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[tensorflow|WARNING|deprecation.py:328] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:432: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "[root    |INFO|build_network.py:648] ------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "l1_dense (Dense_with_tree)   (None, 40)                1960      \n",
      "_________________________________________________________________\n",
      "l1_activation (Activation)   (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "l2_dense (Dense_with_tree)   (None, 23)                943       \n",
      "_________________________________________________________________\n",
      "l2_activation (Activation)   (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "l3_dense (Dense_with_tree)   (None, 17)                408       \n",
      "_________________________________________________________________\n",
      "l3_activation (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "l4_dense (Dense_with_tree)   (None, 9)                 162       \n",
      "_________________________________________________________________\n",
      "l4_activation (Activation)   (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "last_dense_h (Dense)         (None, 1)                 10        \n",
      "_________________________________________________________________\n",
      "p_hat (Activation)           (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,483\n",
      "Trainable params: 3,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:57] Build Network\n",
      "[root    |INFO|build_network.py:58] Optimizer = adam\n",
      "[root    |INFO|build_network.py:59] Loss = binary_crossentropy\n",
      "[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc\n",
      "[root    |INFO|deepbiome.py:157] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:158] 1 fold computing start!----------------------------------\n",
      "[root    |INFO|build_network.py:141] Training start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:2862: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[tensorflow|WARNING|deprecation.py:328] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:2862: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 150 samples\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.6576 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5630 - val_loss: 0.6175 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5267\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 0s 198us/step - loss: 0.6214 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6014 - val_loss: 0.6127 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5567\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 0s 196us/step - loss: 0.6238 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6623 - val_loss: 0.6111 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6093\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 0s 174us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7046 - val_loss: 0.6114 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6240\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 0s 203us/step - loss: 0.6209 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7137 - val_loss: 0.6116 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6208\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 0s 189us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7245 - val_loss: 0.6111 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6041\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 0s 194us/step - loss: 0.6209 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7234 - val_loss: 0.6108 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6189\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 0s 192us/step - loss: 0.6209 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7370 - val_loss: 0.6112 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6249\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 0s 205us/step - loss: 0.6208 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7297 - val_loss: 0.6112 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6374\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 0s 197us/step - loss: 0.6201 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7240 - val_loss: 0.6107 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6425\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.6201 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7257 - val_loss: 0.6101 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6467\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 0s 189us/step - loss: 0.6212 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7192 - val_loss: 0.6103 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6459\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 0s 210us/step - loss: 0.6185 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7220 - val_loss: 0.6088 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6560\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 0s 191us/step - loss: 0.6178 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7234 - val_loss: 0.6078 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6582\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 0s 174us/step - loss: 0.6155 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7313 - val_loss: 0.6061 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6627\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 0s 178us/step - loss: 0.6128 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7211 - val_loss: 0.6048 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6617\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 0s 201us/step - loss: 0.6094 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7245 - val_loss: 0.6012 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6652\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.6051 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7240 - val_loss: 0.5978 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6675\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 0s 200us/step - loss: 0.6000 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7317 - val_loss: 0.5937 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6670\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 0s 193us/step - loss: 0.5930 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7236 - val_loss: 0.5892 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6681\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 0s 184us/step - loss: 0.5828 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7364 - val_loss: 0.5857 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6706\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 0s 211us/step - loss: 0.5696 - binary_accuracy: 0.7167 - sensitivity: 0.9898 - specificity: 0.1067 - gmeasure: 0.2599 - auc: 0.7340 - val_loss: 0.5781 - val_binary_accuracy: 0.6800 - val_sensitivity: 0.9278 - val_specificity: 0.0874 - val_gmeasure: 0.2839 - val_auc: 0.6735\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 0s 216us/step - loss: 0.5606 - binary_accuracy: 0.7183 - sensitivity: 0.9634 - specificity: 0.1826 - gmeasure: 0.4093 - auc: 0.7270 - val_loss: 0.5774 - val_binary_accuracy: 0.6800 - val_sensitivity: 0.9370 - val_specificity: 0.0708 - val_gmeasure: 0.2565 - val_auc: 0.6789\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - ETA: 0s - loss: 0.5374 - binary_accuracy: 0.7371 - sensitivity: 0.9557 - specificity: 0.2258 - gmeasure: 0.4586 - auc: 0.73 - 0s 211us/step - loss: 0.5482 - binary_accuracy: 0.7217 - sensitivity: 0.9570 - specificity: 0.2095 - gmeasure: 0.4416 - auc: 0.7392 - val_loss: 0.5711 - val_binary_accuracy: 0.7067 - val_sensitivity: 0.9167 - val_specificity: 0.1916 - val_gmeasure: 0.4149 - val_auc: 0.6809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "600/600 [==============================] - 0s 190us/step - loss: 0.5410 - binary_accuracy: 0.7283 - sensitivity: 0.9541 - specificity: 0.2243 - gmeasure: 0.4527 - auc: 0.7495 - val_loss: 0.5706 - val_binary_accuracy: 0.7067 - val_sensitivity: 0.8996 - val_specificity: 0.2385 - val_gmeasure: 0.4568 - val_auc: 0.6852\n",
      "Epoch 26/100\n",
      "600/600 [==============================] - 0s 188us/step - loss: 0.5349 - binary_accuracy: 0.7267 - sensitivity: 0.9477 - specificity: 0.2443 - gmeasure: 0.4747 - auc: 0.7590 - val_loss: 0.5638 - val_binary_accuracy: 0.7067 - val_sensitivity: 0.9167 - val_specificity: 0.1916 - val_gmeasure: 0.4149 - val_auc: 0.6937\n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.5259 - binary_accuracy: 0.7333 - sensitivity: 0.9486 - specificity: 0.2729 - gmeasure: 0.4970 - auc: 0.7647 - val_loss: 0.5645 - val_binary_accuracy: 0.6933 - val_sensitivity: 0.8799 - val_specificity: 0.2385 - val_gmeasure: 0.4512 - val_auc: 0.6994\n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 0s 206us/step - loss: 0.5193 - binary_accuracy: 0.7417 - sensitivity: 0.9419 - specificity: 0.3002 - gmeasure: 0.5128 - auc: 0.7719 - val_loss: 0.5646 - val_binary_accuracy: 0.7067 - val_sensitivity: 0.9167 - val_specificity: 0.1916 - val_gmeasure: 0.4149 - val_auc: 0.7094\n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 0s 194us/step - loss: 0.5195 - binary_accuracy: 0.7450 - sensitivity: 0.9371 - specificity: 0.3220 - gmeasure: 0.5334 - auc: 0.7779 - val_loss: 0.5617 - val_binary_accuracy: 0.7000 - val_sensitivity: 0.9174 - val_specificity: 0.1613 - val_gmeasure: 0.3738 - val_auc: 0.7151\n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 0s 197us/step - loss: 0.5083 - binary_accuracy: 0.7450 - sensitivity: 0.9377 - specificity: 0.3321 - gmeasure: 0.5519 - auc: 0.7883 - val_loss: 0.5526 - val_binary_accuracy: 0.7067 - val_sensitivity: 0.9174 - val_specificity: 0.1916 - val_gmeasure: 0.4149 - val_auc: 0.7189\n",
      "Epoch 31/100\n",
      "600/600 [==============================] - 0s 211us/step - loss: 0.5204 - binary_accuracy: 0.7367 - sensitivity: 0.8967 - specificity: 0.4135 - gmeasure: 0.5758 - auc: 0.7954 - val_loss: 0.5814 - val_binary_accuracy: 0.7000 - val_sensitivity: 0.9370 - val_specificity: 0.1279 - val_gmeasure: 0.3431 - val_auc: 0.7222\n",
      "Epoch 32/100\n",
      "600/600 [==============================] - 0s 212us/step - loss: 0.5274 - binary_accuracy: 0.7150 - sensitivity: 0.8618 - specificity: 0.3909 - gmeasure: 0.5340 - auc: 0.7910 - val_loss: 0.5414 - val_binary_accuracy: 0.7200 - val_sensitivity: 0.8707 - val_specificity: 0.3361 - val_gmeasure: 0.5352 - val_auc: 0.7297\n",
      "Epoch 33/100\n",
      "600/600 [==============================] - 0s 198us/step - loss: 0.5197 - binary_accuracy: 0.7317 - sensitivity: 0.9171 - specificity: 0.3443 - gmeasure: 0.5242 - auc: 0.8053 - val_loss: 0.5558 - val_binary_accuracy: 0.6867 - val_sensitivity: 0.7608 - val_specificity: 0.5015 - val_gmeasure: 0.6170 - val_auc: 0.7414\n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 0s 189us/step - loss: 0.5024 - binary_accuracy: 0.7550 - sensitivity: 0.9402 - specificity: 0.3513 - gmeasure: 0.5525 - auc: 0.8037 - val_loss: 0.5329 - val_binary_accuracy: 0.7200 - val_sensitivity: 0.8707 - val_specificity: 0.3361 - val_gmeasure: 0.5352 - val_auc: 0.7439\n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 0s 199us/step - loss: 0.4963 - binary_accuracy: 0.7567 - sensitivity: 0.9143 - specificity: 0.4218 - gmeasure: 0.5999 - auc: 0.8171 - val_loss: 0.5310 - val_binary_accuracy: 0.7067 - val_sensitivity: 0.9174 - val_specificity: 0.1916 - val_gmeasure: 0.4149 - val_auc: 0.7458\n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 0s 206us/step - loss: 0.4801 - binary_accuracy: 0.7650 - sensitivity: 0.9325 - specificity: 0.3923 - gmeasure: 0.5957 - auc: 0.8124 - val_loss: 0.5239 - val_binary_accuracy: 0.7333 - val_sensitivity: 0.8977 - val_specificity: 0.3195 - val_gmeasure: 0.5313 - val_auc: 0.7512\n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 0s 211us/step - loss: 0.4753 - binary_accuracy: 0.7567 - sensitivity: 0.9431 - specificity: 0.3619 - gmeasure: 0.5743 - auc: 0.8261 - val_loss: 0.5185 - val_binary_accuracy: 0.7267 - val_sensitivity: 0.8799 - val_specificity: 0.3361 - val_gmeasure: 0.5378 - val_auc: 0.7591\n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 0s 198us/step - loss: 0.4705 - binary_accuracy: 0.7633 - sensitivity: 0.9178 - specificity: 0.4264 - gmeasure: 0.6163 - auc: 0.8272 - val_loss: 0.5171 - val_binary_accuracy: 0.7267 - val_sensitivity: 0.9174 - val_specificity: 0.2558 - val_gmeasure: 0.4801 - val_auc: 0.7662\n",
      "Epoch 39/100\n",
      "600/600 [==============================] - 0s 179us/step - loss: 0.4643 - binary_accuracy: 0.7667 - sensitivity: 0.9175 - specificity: 0.4337 - gmeasure: 0.6232 - auc: 0.8392 - val_loss: 0.5136 - val_binary_accuracy: 0.7333 - val_sensitivity: 0.9285 - val_specificity: 0.2558 - val_gmeasure: 0.4834 - val_auc: 0.7775\n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 0s 191us/step - loss: 0.4549 - binary_accuracy: 0.7817 - sensitivity: 0.9429 - specificity: 0.4292 - gmeasure: 0.6255 - auc: 0.8349 - val_loss: 0.5045 - val_binary_accuracy: 0.7200 - val_sensitivity: 0.8083 - val_specificity: 0.5015 - val_gmeasure: 0.6358 - val_auc: 0.7827\n",
      "Epoch 41/100\n",
      "600/600 [==============================] - 0s 183us/step - loss: 0.4534 - binary_accuracy: 0.7733 - sensitivity: 0.9169 - specificity: 0.4668 - gmeasure: 0.6395 - auc: 0.8438 - val_loss: 0.4935 - val_binary_accuracy: 0.7400 - val_sensitivity: 0.8654 - val_specificity: 0.4236 - val_gmeasure: 0.6013 - val_auc: 0.7896\n",
      "Epoch 42/100\n",
      "600/600 [==============================] - 0s 196us/step - loss: 0.4449 - binary_accuracy: 0.7900 - sensitivity: 0.9140 - specificity: 0.5159 - gmeasure: 0.6761 - auc: 0.8460 - val_loss: 0.4974 - val_binary_accuracy: 0.7400 - val_sensitivity: 0.9285 - val_specificity: 0.2725 - val_gmeasure: 0.4970 - val_auc: 0.7917\n",
      "Epoch 43/100\n",
      "600/600 [==============================] - 0s 206us/step - loss: 0.4470 - binary_accuracy: 0.7683 - sensitivity: 0.8836 - specificity: 0.5144 - gmeasure: 0.6561 - auc: 0.8522 - val_loss: 0.4973 - val_binary_accuracy: 0.7467 - val_sensitivity: 0.9507 - val_specificity: 0.2558 - val_gmeasure: 0.4898 - val_auc: 0.7995\n",
      "Epoch 44/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.4405 - binary_accuracy: 0.7750 - sensitivity: 0.9100 - specificity: 0.4737 - gmeasure: 0.6308 - auc: 0.8621 - val_loss: 0.4784 - val_binary_accuracy: 0.7333 - val_sensitivity: 0.8298 - val_specificity: 0.5015 - val_gmeasure: 0.6448 - val_auc: 0.8074\n",
      "Epoch 45/100\n",
      "600/600 [==============================] - 0s 201us/step - loss: 0.4265 - binary_accuracy: 0.8017 - sensitivity: 0.9268 - specificity: 0.5117 - gmeasure: 0.6771 - auc: 0.8634 - val_loss: 0.4723 - val_binary_accuracy: 0.7400 - val_sensitivity: 0.8943 - val_specificity: 0.3528 - val_gmeasure: 0.5551 - val_auc: 0.8162\n",
      "Epoch 46/100\n",
      "600/600 [==============================] - 0s 211us/step - loss: 0.4245 - binary_accuracy: 0.8017 - sensitivity: 0.9123 - specificity: 0.5588 - gmeasure: 0.7101 - auc: 0.8679 - val_loss: 0.4650 - val_binary_accuracy: 0.7267 - val_sensitivity: 0.8383 - val_specificity: 0.4474 - val_gmeasure: 0.6102 - val_auc: 0.8232\n",
      "Epoch 47/100\n",
      "600/600 [==============================] - 0s 198us/step - loss: 0.4153 - binary_accuracy: 0.8033 - sensitivity: 0.9115 - specificity: 0.5528 - gmeasure: 0.6995 - auc: 0.8728 - val_loss: 0.4702 - val_binary_accuracy: 0.7533 - val_sensitivity: 0.9396 - val_specificity: 0.2892 - val_gmeasure: 0.5135 - val_auc: 0.8276\n",
      "Epoch 48/100\n",
      "600/600 [==============================] - 0s 212us/step - loss: 0.4065 - binary_accuracy: 0.8200 - sensitivity: 0.9286 - specificity: 0.5740 - gmeasure: 0.7270 - auc: 0.8812 - val_loss: 0.4793 - val_binary_accuracy: 0.7400 - val_sensitivity: 0.7449 - val_specificity: 0.7377 - val_gmeasure: 0.7409 - val_auc: 0.8383\n",
      "Epoch 49/100\n",
      "600/600 [==============================] - 0s 193us/step - loss: 0.4357 - binary_accuracy: 0.7800 - sensitivity: 0.8681 - specificity: 0.5711 - gmeasure: 0.6786 - auc: 0.8751 - val_loss: 0.5041 - val_binary_accuracy: 0.7533 - val_sensitivity: 0.7067 - val_specificity: 0.8864 - val_gmeasure: 0.7894 - val_auc: 0.8374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100\n",
      "600/600 [==============================] - 0s 203us/step - loss: 0.4252 - binary_accuracy: 0.7867 - sensitivity: 0.8722 - specificity: 0.5805 - gmeasure: 0.6600 - auc: 0.8785 - val_loss: 0.4504 - val_binary_accuracy: 0.7600 - val_sensitivity: 0.8298 - val_specificity: 0.5890 - val_gmeasure: 0.6986 - val_auc: 0.8453\n",
      "Epoch 51/100\n",
      "600/600 [==============================] - 0s 199us/step - loss: 0.4026 - binary_accuracy: 0.8050 - sensitivity: 0.9074 - specificity: 0.5862 - gmeasure: 0.7222 - auc: 0.8882 - val_loss: 0.4401 - val_binary_accuracy: 0.7600 - val_sensitivity: 0.8580 - val_specificity: 0.5110 - val_gmeasure: 0.6594 - val_auc: 0.8482\n",
      "Epoch 52/100\n",
      "600/600 [==============================] - 0s 212us/step - loss: 0.4085 - binary_accuracy: 0.8067 - sensitivity: 0.8934 - specificity: 0.6049 - gmeasure: 0.7200 - auc: 0.8825 - val_loss: 0.4404 - val_binary_accuracy: 0.7667 - val_sensitivity: 0.8409 - val_specificity: 0.5890 - val_gmeasure: 0.7035 - val_auc: 0.8531\n",
      "Epoch 53/100\n",
      "600/600 [==============================] - 0s 195us/step - loss: 0.3928 - binary_accuracy: 0.8167 - sensitivity: 0.8953 - specificity: 0.6509 - gmeasure: 0.7542 - auc: 0.8907 - val_loss: 0.4340 - val_binary_accuracy: 0.7733 - val_sensitivity: 0.9251 - val_specificity: 0.3998 - val_gmeasure: 0.6024 - val_auc: 0.8549\n",
      "Epoch 54/100\n",
      "600/600 [==============================] - 0s 200us/step - loss: 0.3890 - binary_accuracy: 0.8250 - sensitivity: 0.9336 - specificity: 0.5893 - gmeasure: 0.7371 - auc: 0.8957 - val_loss: 0.4241 - val_binary_accuracy: 0.7667 - val_sensitivity: 0.8665 - val_specificity: 0.5110 - val_gmeasure: 0.6625 - val_auc: 0.8602\n",
      "Epoch 55/100\n",
      "600/600 [==============================] - 0s 206us/step - loss: 0.3844 - binary_accuracy: 0.8350 - sensitivity: 0.9057 - specificity: 0.6767 - gmeasure: 0.7751 - auc: 0.8998 - val_loss: 0.4247 - val_binary_accuracy: 0.7733 - val_sensitivity: 0.9066 - val_specificity: 0.4331 - val_gmeasure: 0.6180 - val_auc: 0.8614\n",
      "Epoch 56/100\n",
      "600/600 [==============================] - 0s 216us/step - loss: 0.3823 - binary_accuracy: 0.8350 - sensitivity: 0.8976 - specificity: 0.6914 - gmeasure: 0.7784 - auc: 0.8991 - val_loss: 0.4730 - val_binary_accuracy: 0.7467 - val_sensitivity: 0.9711 - val_specificity: 0.2089 - val_gmeasure: 0.4388 - val_auc: 0.8620\n",
      "Epoch 57/100\n",
      "600/600 [==============================] - 0s 214us/step - loss: 0.3906 - binary_accuracy: 0.8250 - sensitivity: 0.8966 - specificity: 0.6561 - gmeasure: 0.7449 - auc: 0.9015 - val_loss: 0.4831 - val_binary_accuracy: 0.7400 - val_sensitivity: 0.9711 - val_specificity: 0.1851 - val_gmeasure: 0.4152 - val_auc: 0.8634\n",
      "Epoch 58/100\n",
      "600/600 [==============================] - 0s 204us/step - loss: 0.3848 - binary_accuracy: 0.8200 - sensitivity: 0.8965 - specificity: 0.6419 - gmeasure: 0.7430 - auc: 0.9059 - val_loss: 0.4332 - val_binary_accuracy: 0.7667 - val_sensitivity: 0.9514 - val_specificity: 0.3195 - val_gmeasure: 0.5489 - val_auc: 0.8683\n",
      "Epoch 59/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.3827 - binary_accuracy: 0.8100 - sensitivity: 0.8930 - specificity: 0.6513 - gmeasure: 0.7500 - auc: 0.9118 - val_loss: 0.4755 - val_binary_accuracy: 0.7467 - val_sensitivity: 0.9711 - val_specificity: 0.2154 - val_gmeasure: 0.4563 - val_auc: 0.8662\n",
      "Epoch 60/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.4265 - binary_accuracy: 0.7967 - sensitivity: 0.8874 - specificity: 0.5990 - gmeasure: 0.6891 - auc: 0.9077 - val_loss: 0.5220 - val_binary_accuracy: 0.7400 - val_sensitivity: 0.9822 - val_specificity: 0.1684 - val_gmeasure: 0.4004 - val_auc: 0.8705\n",
      "Epoch 61/100\n",
      "600/600 [==============================] - ETA: 0s - loss: 0.4746 - binary_accuracy: 0.7657 - sensitivity: 0.8017 - specificity: 0.7252 - gmeasure: 0.7270 - auc: 0.90 - 0s 213us/step - loss: 0.4165 - binary_accuracy: 0.7950 - sensitivity: 0.8647 - specificity: 0.6438 - gmeasure: 0.7185 - auc: 0.9103 - val_loss: 0.4776 - val_binary_accuracy: 0.7467 - val_sensitivity: 0.9711 - val_specificity: 0.2154 - val_gmeasure: 0.4563 - val_auc: 0.8723\n",
      "Epoch 62/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.3678 - binary_accuracy: 0.8300 - sensitivity: 0.9207 - specificity: 0.6399 - gmeasure: 0.7579 - auc: 0.9118 - val_loss: 0.4014 - val_binary_accuracy: 0.8000 - val_sensitivity: 0.9073 - val_specificity: 0.5413 - val_gmeasure: 0.6984 - val_auc: 0.8800\n",
      "Epoch 63/100\n",
      "600/600 [==============================] - 0s 194us/step - loss: 0.3555 - binary_accuracy: 0.8517 - sensitivity: 0.9193 - specificity: 0.7055 - gmeasure: 0.8025 - auc: 0.9104 - val_loss: 0.3994 - val_binary_accuracy: 0.7933 - val_sensitivity: 0.9158 - val_specificity: 0.4872 - val_gmeasure: 0.6633 - val_auc: 0.8820\n",
      "Epoch 64/100\n",
      "600/600 [==============================] - 0s 192us/step - loss: 0.3604 - binary_accuracy: 0.8250 - sensitivity: 0.8891 - specificity: 0.6938 - gmeasure: 0.7796 - auc: 0.9182 - val_loss: 0.4044 - val_binary_accuracy: 0.8067 - val_sensitivity: 0.8316 - val_specificity: 0.7478 - val_gmeasure: 0.7875 - val_auc: 0.8851\n",
      "Epoch 65/100\n",
      "600/600 [==============================] - 0s 194us/step - loss: 0.3497 - binary_accuracy: 0.8500 - sensitivity: 0.9262 - specificity: 0.6821 - gmeasure: 0.7891 - auc: 0.9169 - val_loss: 0.3982 - val_binary_accuracy: 0.8067 - val_sensitivity: 0.8402 - val_specificity: 0.7312 - val_gmeasure: 0.7822 - val_auc: 0.8874\n",
      "Epoch 66/100\n",
      "600/600 [==============================] - 0s 193us/step - loss: 0.3575 - binary_accuracy: 0.8300 - sensitivity: 0.8935 - specificity: 0.6902 - gmeasure: 0.7719 - auc: 0.9128 - val_loss: 0.3931 - val_binary_accuracy: 0.8000 - val_sensitivity: 0.8494 - val_specificity: 0.6907 - val_gmeasure: 0.7652 - val_auc: 0.8893\n",
      "Epoch 67/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.3528 - binary_accuracy: 0.8433 - sensitivity: 0.8999 - specificity: 0.7291 - gmeasure: 0.8052 - auc: 0.9176 - val_loss: 0.3912 - val_binary_accuracy: 0.7867 - val_sensitivity: 0.9158 - val_specificity: 0.4641 - val_gmeasure: 0.6488 - val_auc: 0.8892\n",
      "Epoch 68/100\n",
      "600/600 [==============================] - 0s 201us/step - loss: 0.3404 - binary_accuracy: 0.8617 - sensitivity: 0.9370 - specificity: 0.7022 - gmeasure: 0.8043 - auc: 0.9186 - val_loss: 0.3831 - val_binary_accuracy: 0.8000 - val_sensitivity: 0.8980 - val_specificity: 0.5652 - val_gmeasure: 0.7117 - val_auc: 0.8944\n",
      "Epoch 69/100\n",
      "600/600 [==============================] - 0s 201us/step - loss: 0.3413 - binary_accuracy: 0.8600 - sensitivity: 0.9172 - specificity: 0.7319 - gmeasure: 0.8140 - auc: 0.9269 - val_loss: 0.3794 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8802 - val_specificity: 0.6604 - val_gmeasure: 0.7603 - val_auc: 0.8965\n",
      "Epoch 70/100\n",
      "600/600 [==============================] - 0s 200us/step - loss: 0.3450 - binary_accuracy: 0.8450 - sensitivity: 0.9060 - specificity: 0.7206 - gmeasure: 0.8033 - auc: 0.9224 - val_loss: 0.3816 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8494 - val_specificity: 0.7615 - val_gmeasure: 0.8035 - val_auc: 0.9009\n",
      "Epoch 71/100\n",
      "600/600 [==============================] - 0s 199us/step - loss: 0.3442 - binary_accuracy: 0.8433 - sensitivity: 0.9012 - specificity: 0.7112 - gmeasure: 0.7965 - auc: 0.9258 - val_loss: 0.3755 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8802 - val_specificity: 0.6842 - val_gmeasure: 0.7725 - val_auc: 0.9011\n",
      "Epoch 72/100\n",
      "600/600 [==============================] - 0s 196us/step - loss: 0.3367 - binary_accuracy: 0.8483 - sensitivity: 0.9155 - specificity: 0.7152 - gmeasure: 0.8020 - auc: 0.9289 - val_loss: 0.3750 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8802 - val_specificity: 0.7615 - val_gmeasure: 0.8173 - val_auc: 0.9037\n",
      "Epoch 73/100\n",
      "600/600 [==============================] - 0s 193us/step - loss: 0.3377 - binary_accuracy: 0.8567 - sensitivity: 0.9134 - specificity: 0.7242 - gmeasure: 0.8051 - auc: 0.9304 - val_loss: 0.3696 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8980 - val_specificity: 0.6128 - val_gmeasure: 0.7415 - val_auc: 0.9037\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 0s 207us/step - loss: 0.3236 - binary_accuracy: 0.8700 - sensitivity: 0.9366 - specificity: 0.7099 - gmeasure: 0.8112 - auc: 0.9267 - val_loss: 0.3763 - val_binary_accuracy: 0.7733 - val_sensitivity: 0.9066 - val_specificity: 0.4474 - val_gmeasure: 0.6350 - val_auc: 0.9044\n",
      "Epoch 75/100\n",
      "600/600 [==============================] - 0s 210us/step - loss: 0.3180 - binary_accuracy: 0.8550 - sensitivity: 0.9260 - specificity: 0.7070 - gmeasure: 0.8075 - auc: 0.9312 - val_loss: 0.3761 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.8494 - val_specificity: 0.8561 - val_gmeasure: 0.8512 - val_auc: 0.9067\n",
      "Epoch 76/100\n",
      "600/600 [==============================] - 0s 190us/step - loss: 0.3287 - binary_accuracy: 0.8550 - sensitivity: 0.8982 - specificity: 0.7656 - gmeasure: 0.8258 - auc: 0.9329 - val_loss: 0.3720 - val_binary_accuracy: 0.7800 - val_sensitivity: 0.9066 - val_specificity: 0.4641 - val_gmeasure: 0.6456 - val_auc: 0.9076\n",
      "Epoch 77/100\n",
      "600/600 [==============================] - 0s 206us/step - loss: 0.3274 - binary_accuracy: 0.8550 - sensitivity: 0.9280 - specificity: 0.6790 - gmeasure: 0.7736 - auc: 0.9322 - val_loss: 0.3632 - val_binary_accuracy: 0.8067 - val_sensitivity: 0.9066 - val_specificity: 0.5658 - val_gmeasure: 0.7143 - val_auc: 0.9062\n",
      "Epoch 78/100\n",
      "600/600 [==============================] - 0s 199us/step - loss: 0.3119 - binary_accuracy: 0.8750 - sensitivity: 0.9331 - specificity: 0.7385 - gmeasure: 0.8254 - auc: 0.9336 - val_loss: 0.3598 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8895 - val_specificity: 0.7442 - val_gmeasure: 0.8124 - val_auc: 0.9067\n",
      "Epoch 79/100\n",
      "600/600 [==============================] - 0s 206us/step - loss: 0.3192 - binary_accuracy: 0.8583 - sensitivity: 0.9052 - specificity: 0.7642 - gmeasure: 0.8255 - auc: 0.9365 - val_loss: 0.3565 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8895 - val_specificity: 0.7442 - val_gmeasure: 0.8124 - val_auc: 0.9085\n",
      "Epoch 80/100\n",
      "600/600 [==============================] - 0s 194us/step - loss: 0.3134 - binary_accuracy: 0.8667 - sensitivity: 0.9246 - specificity: 0.7507 - gmeasure: 0.8302 - auc: 0.9336 - val_loss: 0.3579 - val_binary_accuracy: 0.7933 - val_sensitivity: 0.9066 - val_specificity: 0.5117 - val_gmeasure: 0.6774 - val_auc: 0.9079\n",
      "Epoch 81/100\n",
      "600/600 [==============================] - 0s 217us/step - loss: 0.3016 - binary_accuracy: 0.8733 - sensitivity: 0.9288 - specificity: 0.7390 - gmeasure: 0.8248 - auc: 0.9371 - val_loss: 0.3594 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.8605 - val_specificity: 0.8489 - val_gmeasure: 0.8540 - val_auc: 0.9115\n",
      "Epoch 82/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.3021 - binary_accuracy: 0.8717 - sensitivity: 0.9201 - specificity: 0.7699 - gmeasure: 0.8398 - auc: 0.9350 - val_loss: 0.3505 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8895 - val_specificity: 0.7305 - val_gmeasure: 0.8060 - val_auc: 0.9119\n",
      "Epoch 83/100\n",
      "600/600 [==============================] - 0s 190us/step - loss: 0.3103 - binary_accuracy: 0.8700 - sensitivity: 0.9204 - specificity: 0.7775 - gmeasure: 0.8434 - auc: 0.9434 - val_loss: 0.3623 - val_binary_accuracy: 0.7867 - val_sensitivity: 0.9262 - val_specificity: 0.4545 - val_gmeasure: 0.6471 - val_auc: 0.9118\n",
      "Epoch 84/100\n",
      "600/600 [==============================] - 0s 202us/step - loss: 0.2970 - binary_accuracy: 0.8833 - sensitivity: 0.9317 - specificity: 0.7774 - gmeasure: 0.8497 - auc: 0.9394 - val_loss: 0.3504 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.9066 - val_specificity: 0.5896 - val_gmeasure: 0.7279 - val_auc: 0.9124\n",
      "Epoch 85/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.2989 - binary_accuracy: 0.8750 - sensitivity: 0.9237 - specificity: 0.7640 - gmeasure: 0.8364 - auc: 0.9432 - val_loss: 0.3454 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.8895 - val_specificity: 0.7543 - val_gmeasure: 0.8189 - val_auc: 0.9118\n",
      "Epoch 86/100\n",
      "600/600 [==============================] - 0s 190us/step - loss: 0.2909 - binary_accuracy: 0.8817 - sensitivity: 0.9216 - specificity: 0.7906 - gmeasure: 0.8508 - auc: 0.9437 - val_loss: 0.3441 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8980 - val_specificity: 0.6835 - val_gmeasure: 0.7833 - val_auc: 0.9123\n",
      "Epoch 87/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.2881 - binary_accuracy: 0.8750 - sensitivity: 0.9159 - specificity: 0.7817 - gmeasure: 0.8426 - auc: 0.9396 - val_loss: 0.3818 - val_binary_accuracy: 0.7933 - val_sensitivity: 0.9459 - val_specificity: 0.4379 - val_gmeasure: 0.6416 - val_auc: 0.9138\n",
      "Epoch 88/100\n",
      "600/600 [==============================] - 0s 207us/step - loss: 0.3188 - binary_accuracy: 0.8367 - sensitivity: 0.9039 - specificity: 0.7118 - gmeasure: 0.7916 - auc: 0.9502 - val_loss: 0.3550 - val_binary_accuracy: 0.8000 - val_sensitivity: 0.9262 - val_specificity: 0.5087 - val_gmeasure: 0.6852 - val_auc: 0.9138\n",
      "Epoch 89/100\n",
      "600/600 [==============================] - 0s 201us/step - loss: 0.2883 - binary_accuracy: 0.8800 - sensitivity: 0.9090 - specificity: 0.8045 - gmeasure: 0.8514 - auc: 0.9437 - val_loss: 0.3414 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.9066 - val_specificity: 0.5896 - val_gmeasure: 0.7279 - val_auc: 0.9149\n",
      "Epoch 90/100\n",
      "600/600 [==============================] - 0s 231us/step - loss: 0.2894 - binary_accuracy: 0.8717 - sensitivity: 0.9246 - specificity: 0.7667 - gmeasure: 0.8389 - auc: 0.9475 - val_loss: 0.3408 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.9177 - val_specificity: 0.5896 - val_gmeasure: 0.7324 - val_auc: 0.9163\n",
      "Epoch 91/100\n",
      "600/600 [==============================] - 0s 203us/step - loss: 0.3045 - binary_accuracy: 0.8683 - sensitivity: 0.9183 - specificity: 0.7585 - gmeasure: 0.8249 - auc: 0.9488 - val_loss: 0.3685 - val_binary_accuracy: 0.8000 - val_sensitivity: 0.9459 - val_specificity: 0.4682 - val_gmeasure: 0.6647 - val_auc: 0.9156\n",
      "Epoch 92/100\n",
      "600/600 [==============================] - 0s 217us/step - loss: 0.2883 - binary_accuracy: 0.8733 - sensitivity: 0.9116 - specificity: 0.7920 - gmeasure: 0.8452 - auc: 0.9451 - val_loss: 0.3391 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.9177 - val_specificity: 0.6437 - val_gmeasure: 0.7656 - val_auc: 0.9173\n",
      "Epoch 93/100\n",
      "600/600 [==============================] - 0s 200us/step - loss: 0.2859 - binary_accuracy: 0.8767 - sensitivity: 0.9191 - specificity: 0.7788 - gmeasure: 0.8421 - auc: 0.9486 - val_loss: 0.3457 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.9262 - val_specificity: 0.5420 - val_gmeasure: 0.7073 - val_auc: 0.9163\n",
      "Epoch 94/100\n",
      "600/600 [==============================] - 0s 200us/step - loss: 0.3216 - binary_accuracy: 0.8517 - sensitivity: 0.9146 - specificity: 0.7188 - gmeasure: 0.7958 - auc: 0.9505 - val_loss: 0.3349 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8698 - val_specificity: 0.8656 - val_gmeasure: 0.8675 - val_auc: 0.9184\n",
      "Epoch 95/100\n",
      "600/600 [==============================] - 0s 202us/step - loss: 0.3055 - binary_accuracy: 0.8733 - sensitivity: 0.9111 - specificity: 0.8139 - gmeasure: 0.8546 - auc: 0.9492 - val_loss: 0.3435 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8520 - val_specificity: 0.9197 - val_gmeasure: 0.8842 - val_auc: 0.9190\n",
      "Epoch 96/100\n",
      "600/600 [==============================] - 0s 194us/step - loss: 0.2751 - binary_accuracy: 0.8783 - sensitivity: 0.9206 - specificity: 0.7799 - gmeasure: 0.8436 - auc: 0.9456 - val_loss: 0.3332 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8613 - val_specificity: 0.8656 - val_gmeasure: 0.8632 - val_auc: 0.9214\n",
      "Epoch 97/100\n",
      "600/600 [==============================] - 0s 191us/step - loss: 0.2769 - binary_accuracy: 0.8683 - sensitivity: 0.9184 - specificity: 0.7673 - gmeasure: 0.8360 - auc: 0.9560 - val_loss: 0.3525 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8434 - val_specificity: 0.9364 - val_gmeasure: 0.8881 - val_auc: 0.9199\n",
      "Epoch 98/100\n",
      "600/600 [==============================] - 0s 201us/step - loss: 0.2814 - binary_accuracy: 0.8717 - sensitivity: 0.9236 - specificity: 0.7561 - gmeasure: 0.8274 - auc: 0.9562 - val_loss: 0.3576 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8342 - val_specificity: 0.9667 - val_gmeasure: 0.8966 - val_auc: 0.9188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100\n",
      "600/600 [==============================] - 0s 210us/step - loss: 0.2682 - binary_accuracy: 0.8783 - sensitivity: 0.9277 - specificity: 0.7606 - gmeasure: 0.8372 - auc: 0.9497 - val_loss: 0.3329 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8613 - val_specificity: 0.8656 - val_gmeasure: 0.8632 - val_auc: 0.9214\n",
      "Epoch 100/100\n",
      "600/600 [==============================] - 0s 207us/step - loss: 0.2683 - binary_accuracy: 0.8850 - sensitivity: 0.9215 - specificity: 0.8035 - gmeasure: 0.8568 - auc: 0.9540 - val_loss: 0.3433 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8613 - val_specificity: 0.9061 - val_gmeasure: 0.8831 - val_auc: 0.9216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:151] Training end with time 15.206775903701782!\n",
      "[root    |INFO|build_network.py:79] Saved trained model weight at ./example_result/weight_0.h5 \n",
      "[root    |DEBUG|deepbiome.py:166] Save weight at ./example_result/weight_0.h5\n",
      "[root    |DEBUG|deepbiome.py:169] Save history at ./example_result/hist_0.json\n",
      "[root    |INFO|build_network.py:177] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "750/750 [==============================] - 0s 6us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:182] Evaluation end with time 0.012238025665283203!\n",
      "[root    |INFO|build_network.py:183] Evaluation: [0.28811851143836975, 0.8826666474342346, 0.8803088665008545, 0.8879310488700867, 0.884111762046814, 0.9488500356674194]\n",
      "[root    |INFO|build_network.py:177] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "250/250 [==============================] - 0s 22us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:182] Evaluation end with time 0.013002872467041016!\n",
      "[root    |INFO|build_network.py:183] Evaluation: [0.32086846232414246, 0.8479999899864197, 0.8639053106307983, 0.8148148059844971, 0.8390011191368103, 0.928482711315155]\n",
      "[root    |INFO|deepbiome.py:179] Compute time : 18.531410217285156\n",
      "[root    |INFO|deepbiome.py:180] 1 fold computing end!---------------------------------------------\n",
      "[root    |INFO|deepbiome.py:137] -------2 simulation start!----------------------------------\n",
      "[root    |INFO|readers.py:58] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:59] Construct Dataset\n",
      "[root    |INFO|readers.py:60] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:61] Load data\n",
      "[root    |INFO|deepbiome.py:147] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:148] Build network for 2 simulation\n",
      "[root    |INFO|build_network.py:513] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:514] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv\n",
      "[root    |INFO|build_network.py:518] Phylogenetic tree level list: ['Genus', 'Family', 'Order', 'Class', 'Phylum']\n",
      "[root    |INFO|build_network.py:519] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:526]      Genus: 48\n",
      "[root    |INFO|build_network.py:526]     Family: 40\n",
      "[root    |INFO|build_network.py:526]      Order: 23\n",
      "[root    |INFO|build_network.py:526]      Class: 17\n",
      "[root    |INFO|build_network.py:526]     Phylum: 9\n",
      "[root    |INFO|build_network.py:529] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:530] Phylogenetic_tree_dict info: ['Genus', 'Number', 'Order', 'Class', 'Phylum', 'Family']\n",
      "[root    |INFO|build_network.py:531] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [ Genus, Family]\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [Family,  Order]\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [ Order,  Class]\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [ Class, Phylum]\n",
      "[root    |INFO|build_network.py:554] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:570] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:571] Build network based on phylogenetic tree information\n",
      "[root    |INFO|build_network.py:572] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:648] ------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "l1_dense (Dense_with_tree)   (None, 40)                1960      \n",
      "_________________________________________________________________\n",
      "l1_activation (Activation)   (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "l2_dense (Dense_with_tree)   (None, 23)                943       \n",
      "_________________________________________________________________\n",
      "l2_activation (Activation)   (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "l3_dense (Dense_with_tree)   (None, 17)                408       \n",
      "_________________________________________________________________\n",
      "l3_activation (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "l4_dense (Dense_with_tree)   (None, 9)                 162       \n",
      "_________________________________________________________________\n",
      "l4_activation (Activation)   (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "last_dense_h (Dense)         (None, 1)                 10        \n",
      "_________________________________________________________________\n",
      "p_hat (Activation)           (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,483\n",
      "Trainable params: 3,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:57] Build Network\n",
      "[root    |INFO|build_network.py:58] Optimizer = adam\n",
      "[root    |INFO|build_network.py:59] Loss = binary_crossentropy\n",
      "[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc\n",
      "[root    |INFO|deepbiome.py:157] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:158] 2 fold computing start!----------------------------------\n",
      "[root    |INFO|build_network.py:141] Training start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 150 samples\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 1s 1ms/step - loss: 0.6748 - binary_accuracy: 0.7150 - sensitivity: 0.9401 - specificity: 0.0694 - gmeasure: 0.0403 - auc: 0.4768 - val_loss: 0.6556 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5487\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 0s 217us/step - loss: 0.6400 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.4948 - val_loss: 0.6234 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5511\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.6096 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.4915 - val_loss: 0.6018 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5747\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 0s 214us/step - loss: 0.5932 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.4703 - val_loss: 0.5900 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5622\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 0s 211us/step - loss: 0.5862 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.4978 - val_loss: 0.5867 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5617\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 0s 228us/step - loss: 0.5852 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5087 - val_loss: 0.5867 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5888\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 0s 226us/step - loss: 0.5852 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5378 - val_loss: 0.5867 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6049\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 0s 196us/step - loss: 0.5855 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5658 - val_loss: 0.5867 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6290\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 0s 196us/step - loss: 0.5851 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5707 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6489\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 0s 231us/step - loss: 0.5850 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5912 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6555\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 0s 205us/step - loss: 0.5851 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6007 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6609\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 0s 206us/step - loss: 0.5850 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6039 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6673\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 0s 216us/step - loss: 0.5851 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6073 - val_loss: 0.5866 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6761\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 0s 204us/step - loss: 0.5850 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6273 - val_loss: 0.5866 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6799\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 0s 216us/step - loss: 0.5854 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6281 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6887\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 0s 226us/step - loss: 0.5853 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6401 - val_loss: 0.5866 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6883\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 0s 206us/step - loss: 0.5850 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6386 - val_loss: 0.5866 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6957\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 0s 201us/step - loss: 0.5850 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6617 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7064\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 0s 210us/step - loss: 0.5850 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6467 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7129\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 0s 203us/step - loss: 0.5850 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6634 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7193\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 0s 194us/step - loss: 0.5849 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6677 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7251\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 0s 198us/step - loss: 0.5854 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6765 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7390\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.5849 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6904 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7384\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 0s 203us/step - loss: 0.5852 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6941 - val_loss: 0.5866 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "600/600 [==============================] - 0s 215us/step - loss: 0.5857 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6852 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7438\n",
      "Epoch 26/100\n",
      "600/600 [==============================] - 0s 212us/step - loss: 0.5858 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6832 - val_loss: 0.5866 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7445\n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 0s 185us/step - loss: 0.5850 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6890 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7536\n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 0s 203us/step - loss: 0.5851 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6966 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7555\n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 0s 206us/step - loss: 0.5849 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7159 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7603\n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 0s 230us/step - loss: 0.5849 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7035 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7627\n",
      "Epoch 31/100\n",
      "600/600 [==============================] - 0s 223us/step - loss: 0.5851 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7222 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7658\n",
      "Epoch 32/100\n",
      "600/600 [==============================] - 0s 207us/step - loss: 0.5850 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7149 - val_loss: 0.5864 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7742\n",
      "Epoch 33/100\n",
      "600/600 [==============================] - 0s 201us/step - loss: 0.5851 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7015 - val_loss: 0.5864 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7798\n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 0s 220us/step - loss: 0.5851 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7173 - val_loss: 0.5864 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7816\n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.5852 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7254 - val_loss: 0.5864 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7869\n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 0s 210us/step - loss: 0.5849 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7346 - val_loss: 0.5864 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7927\n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 0s 207us/step - loss: 0.5848 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7466 - val_loss: 0.5848 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8130\n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 0s 205us/step - loss: 0.5820 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8258 - val_loss: 0.5798 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8508\n",
      "Epoch 39/100\n",
      "600/600 [==============================] - 0s 196us/step - loss: 0.5741 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8366 - val_loss: 0.5715 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8628\n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 0s 189us/step - loss: 0.5634 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8389 - val_loss: 0.5540 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8606\n",
      "Epoch 41/100\n",
      "600/600 [==============================] - 0s 207us/step - loss: 0.5503 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8414 - val_loss: 0.5367 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8596\n",
      "Epoch 42/100\n",
      "600/600 [==============================] - 0s 210us/step - loss: 0.5342 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8417 - val_loss: 0.5184 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8583\n",
      "Epoch 43/100\n",
      "600/600 [==============================] - 0s 200us/step - loss: 0.5169 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8411 - val_loss: 0.5000 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8564\n",
      "Epoch 44/100\n",
      "600/600 [==============================] - 0s 210us/step - loss: 0.4984 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8439 - val_loss: 0.4819 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8570\n",
      "Epoch 45/100\n",
      "600/600 [==============================] - 0s 202us/step - loss: 0.4807 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8465 - val_loss: 0.4675 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8597\n",
      "Epoch 46/100\n",
      "600/600 [==============================] - 0s 197us/step - loss: 0.4681 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8479 - val_loss: 0.4569 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8605\n",
      "Epoch 47/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.4504 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8466 - val_loss: 0.4479 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8617\n",
      "Epoch 48/100\n",
      "600/600 [==============================] - 0s 197us/step - loss: 0.4371 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8543 - val_loss: 0.4506 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100\n",
      "600/600 [==============================] - 0s 216us/step - loss: 0.4322 - binary_accuracy: 0.7333 - sensitivity: 0.9599 - specificity: 0.1712 - gmeasure: 0.1893 - auc: 0.8509 - val_loss: 0.4266 - val_binary_accuracy: 0.7800 - val_sensitivity: 0.8649 - val_specificity: 0.5689 - val_gmeasure: 0.6944 - val_auc: 0.8544\n",
      "Epoch 50/100\n",
      "600/600 [==============================] - 0s 232us/step - loss: 0.4191 - binary_accuracy: 0.7783 - sensitivity: 0.8645 - specificity: 0.5634 - gmeasure: 0.6816 - auc: 0.8608 - val_loss: 0.4174 - val_binary_accuracy: 0.7867 - val_sensitivity: 0.8461 - val_specificity: 0.6501 - val_gmeasure: 0.7324 - val_auc: 0.8616\n",
      "Epoch 51/100\n",
      "600/600 [==============================] - 0s 200us/step - loss: 0.4152 - binary_accuracy: 0.8067 - sensitivity: 0.8519 - specificity: 0.6743 - gmeasure: 0.7529 - auc: 0.8576 - val_loss: 0.4426 - val_binary_accuracy: 0.7667 - val_sensitivity: 0.8649 - val_specificity: 0.5224 - val_gmeasure: 0.6648 - val_auc: 0.8586\n",
      "Epoch 52/100\n",
      "600/600 [==============================] - 0s 191us/step - loss: 0.4046 - binary_accuracy: 0.7950 - sensitivity: 0.8547 - specificity: 0.6314 - gmeasure: 0.7316 - auc: 0.8628 - val_loss: 0.4088 - val_binary_accuracy: 0.7933 - val_sensitivity: 0.8461 - val_specificity: 0.6709 - val_gmeasure: 0.7470 - val_auc: 0.8703\n",
      "Epoch 53/100\n",
      "600/600 [==============================] - 0s 200us/step - loss: 0.4143 - binary_accuracy: 0.8000 - sensitivity: 0.8517 - specificity: 0.6926 - gmeasure: 0.7631 - auc: 0.8695 - val_loss: 0.4146 - val_binary_accuracy: 0.7800 - val_sensitivity: 0.8461 - val_specificity: 0.6223 - val_gmeasure: 0.7166 - val_auc: 0.8602\n",
      "Epoch 54/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.4065 - binary_accuracy: 0.8133 - sensitivity: 0.8479 - specificity: 0.7194 - gmeasure: 0.7788 - auc: 0.8653 - val_loss: 0.4290 - val_binary_accuracy: 0.7667 - val_sensitivity: 0.8461 - val_specificity: 0.5689 - val_gmeasure: 0.6867 - val_auc: 0.8696\n",
      "Epoch 55/100\n",
      "600/600 [==============================] - 0s 216us/step - loss: 0.3975 - binary_accuracy: 0.8050 - sensitivity: 0.8508 - specificity: 0.6738 - gmeasure: 0.7527 - auc: 0.8580 - val_loss: 0.4000 - val_binary_accuracy: 0.7933 - val_sensitivity: 0.8461 - val_specificity: 0.6709 - val_gmeasure: 0.7470 - val_auc: 0.8630\n",
      "Epoch 56/100\n",
      "600/600 [==============================] - 0s 204us/step - loss: 0.3961 - binary_accuracy: 0.8133 - sensitivity: 0.8534 - specificity: 0.6956 - gmeasure: 0.7669 - auc: 0.8600 - val_loss: 0.4039 - val_binary_accuracy: 0.7867 - val_sensitivity: 0.8461 - val_specificity: 0.6501 - val_gmeasure: 0.7324 - val_auc: 0.8629\n",
      "Epoch 57/100\n",
      "600/600 [==============================] - 0s 195us/step - loss: 0.3969 - binary_accuracy: 0.8100 - sensitivity: 0.8378 - specificity: 0.7136 - gmeasure: 0.7681 - auc: 0.8647 - val_loss: 0.4011 - val_binary_accuracy: 0.7933 - val_sensitivity: 0.8461 - val_specificity: 0.6709 - val_gmeasure: 0.7470 - val_auc: 0.8708\n",
      "Epoch 58/100\n",
      "600/600 [==============================] - 0s 197us/step - loss: 0.3895 - binary_accuracy: 0.8133 - sensitivity: 0.8491 - specificity: 0.7210 - gmeasure: 0.7800 - auc: 0.8701 - val_loss: 0.3973 - val_binary_accuracy: 0.7933 - val_sensitivity: 0.8461 - val_specificity: 0.6709 - val_gmeasure: 0.7470 - val_auc: 0.8739\n",
      "Epoch 59/100\n",
      "600/600 [==============================] - 0s 179us/step - loss: 0.3858 - binary_accuracy: 0.8233 - sensitivity: 0.8464 - specificity: 0.7657 - gmeasure: 0.8027 - auc: 0.8701 - val_loss: 0.4002 - val_binary_accuracy: 0.7933 - val_sensitivity: 0.8461 - val_specificity: 0.6709 - val_gmeasure: 0.7470 - val_auc: 0.8717\n",
      "Epoch 60/100\n",
      "600/600 [==============================] - 0s 189us/step - loss: 0.3844 - binary_accuracy: 0.8233 - sensitivity: 0.8426 - specificity: 0.7720 - gmeasure: 0.8027 - auc: 0.8715 - val_loss: 0.4042 - val_binary_accuracy: 0.7867 - val_sensitivity: 0.8461 - val_specificity: 0.6501 - val_gmeasure: 0.7324 - val_auc: 0.8711\n",
      "Epoch 61/100\n",
      "600/600 [==============================] - 0s 174us/step - loss: 0.3860 - binary_accuracy: 0.8150 - sensitivity: 0.8466 - specificity: 0.7363 - gmeasure: 0.7857 - auc: 0.8746 - val_loss: 0.3853 - val_binary_accuracy: 0.8067 - val_sensitivity: 0.8461 - val_specificity: 0.7174 - val_gmeasure: 0.7729 - val_auc: 0.8752\n",
      "Epoch 62/100\n",
      "600/600 [==============================] - 0s 197us/step - loss: 0.3899 - binary_accuracy: 0.8067 - sensitivity: 0.8407 - specificity: 0.7186 - gmeasure: 0.7705 - auc: 0.8730 - val_loss: 0.3876 - val_binary_accuracy: 0.8000 - val_sensitivity: 0.8185 - val_specificity: 0.7660 - val_gmeasure: 0.7876 - val_auc: 0.8785\n",
      "Epoch 63/100\n",
      "600/600 [==============================] - 0s 201us/step - loss: 0.3965 - binary_accuracy: 0.8150 - sensitivity: 0.8318 - specificity: 0.7590 - gmeasure: 0.7881 - auc: 0.8742 - val_loss: 0.4021 - val_binary_accuracy: 0.7933 - val_sensitivity: 0.8461 - val_specificity: 0.6709 - val_gmeasure: 0.7470 - val_auc: 0.8678\n",
      "Epoch 64/100\n",
      "600/600 [==============================] - 0s 217us/step - loss: 0.3937 - binary_accuracy: 0.8050 - sensitivity: 0.8169 - specificity: 0.7757 - gmeasure: 0.7910 - auc: 0.8712 - val_loss: 0.4175 - val_binary_accuracy: 0.7800 - val_sensitivity: 0.8461 - val_specificity: 0.6223 - val_gmeasure: 0.7166 - val_auc: 0.8677\n",
      "Epoch 65/100\n",
      "600/600 [==============================] - 0s 188us/step - loss: 0.3793 - binary_accuracy: 0.8183 - sensitivity: 0.8509 - specificity: 0.7099 - gmeasure: 0.7693 - auc: 0.8701 - val_loss: 0.3803 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8461 - val_specificity: 0.7452 - val_gmeasure: 0.7878 - val_auc: 0.8767\n",
      "Epoch 66/100\n",
      "600/600 [==============================] - 0s 195us/step - loss: 0.3736 - binary_accuracy: 0.8200 - sensitivity: 0.8423 - specificity: 0.7697 - gmeasure: 0.8023 - auc: 0.8802 - val_loss: 0.3812 - val_binary_accuracy: 0.8067 - val_sensitivity: 0.8461 - val_specificity: 0.7174 - val_gmeasure: 0.7729 - val_auc: 0.8771\n",
      "Epoch 67/100\n",
      "600/600 [==============================] - 0s 210us/step - loss: 0.3702 - binary_accuracy: 0.8200 - sensitivity: 0.8422 - specificity: 0.7605 - gmeasure: 0.7981 - auc: 0.8773 - val_loss: 0.3905 - val_binary_accuracy: 0.8000 - val_sensitivity: 0.8461 - val_specificity: 0.6918 - val_gmeasure: 0.7606 - val_auc: 0.8730\n",
      "Epoch 68/100\n",
      "600/600 [==============================] - 0s 202us/step - loss: 0.3686 - binary_accuracy: 0.8233 - sensitivity: 0.8368 - specificity: 0.7784 - gmeasure: 0.8039 - auc: 0.8766 - val_loss: 0.3804 - val_binary_accuracy: 0.8067 - val_sensitivity: 0.8461 - val_specificity: 0.7174 - val_gmeasure: 0.7729 - val_auc: 0.8764\n",
      "Epoch 69/100\n",
      "600/600 [==============================] - 0s 207us/step - loss: 0.3668 - binary_accuracy: 0.8283 - sensitivity: 0.8464 - specificity: 0.7856 - gmeasure: 0.8130 - auc: 0.8805 - val_loss: 0.3789 - val_binary_accuracy: 0.8067 - val_sensitivity: 0.8461 - val_specificity: 0.7174 - val_gmeasure: 0.7729 - val_auc: 0.8771\n",
      "Epoch 70/100\n",
      "600/600 [==============================] - 0s 210us/step - loss: 0.3639 - binary_accuracy: 0.8267 - sensitivity: 0.8435 - specificity: 0.7863 - gmeasure: 0.8135 - auc: 0.8844 - val_loss: 0.3832 - val_binary_accuracy: 0.8067 - val_sensitivity: 0.8461 - val_specificity: 0.7174 - val_gmeasure: 0.7729 - val_auc: 0.8727\n",
      "Epoch 71/100\n",
      "600/600 [==============================] - 0s 203us/step - loss: 0.3628 - binary_accuracy: 0.8283 - sensitivity: 0.8401 - specificity: 0.8057 - gmeasure: 0.8211 - auc: 0.8854 - val_loss: 0.3837 - val_binary_accuracy: 0.8067 - val_sensitivity: 0.8461 - val_specificity: 0.7174 - val_gmeasure: 0.7729 - val_auc: 0.8752\n",
      "Epoch 72/100\n",
      "600/600 [==============================] - 0s 191us/step - loss: 0.3615 - binary_accuracy: 0.8217 - sensitivity: 0.8478 - specificity: 0.7529 - gmeasure: 0.7955 - auc: 0.8836 - val_loss: 0.3692 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8371 - val_specificity: 0.7660 - val_gmeasure: 0.7959 - val_auc: 0.8799\n",
      "Epoch 73/100\n",
      "600/600 [==============================] - 0s 193us/step - loss: 0.3618 - binary_accuracy: 0.8283 - sensitivity: 0.8454 - specificity: 0.7907 - gmeasure: 0.8127 - auc: 0.8901 - val_loss: 0.3802 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8461 - val_specificity: 0.7452 - val_gmeasure: 0.7878 - val_auc: 0.8741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100\n",
      "600/600 [==============================] - 0s 202us/step - loss: 0.3519 - binary_accuracy: 0.8333 - sensitivity: 0.8468 - specificity: 0.8078 - gmeasure: 0.8249 - auc: 0.8912 - val_loss: 0.3672 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8371 - val_specificity: 0.7660 - val_gmeasure: 0.7959 - val_auc: 0.8849\n",
      "Epoch 75/100\n",
      "600/600 [==============================] - 0s 212us/step - loss: 0.3545 - binary_accuracy: 0.8250 - sensitivity: 0.8373 - specificity: 0.8000 - gmeasure: 0.8170 - auc: 0.8907 - val_loss: 0.3812 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8461 - val_specificity: 0.7452 - val_gmeasure: 0.7878 - val_auc: 0.8725\n",
      "Epoch 76/100\n",
      "600/600 [==============================] - 0s 211us/step - loss: 0.3530 - binary_accuracy: 0.8333 - sensitivity: 0.8454 - specificity: 0.8078 - gmeasure: 0.8232 - auc: 0.8907 - val_loss: 0.3658 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8371 - val_specificity: 0.7660 - val_gmeasure: 0.7959 - val_auc: 0.8811\n",
      "Epoch 77/100\n",
      "600/600 [==============================] - 0s 185us/step - loss: 0.3518 - binary_accuracy: 0.8233 - sensitivity: 0.8322 - specificity: 0.7906 - gmeasure: 0.8093 - auc: 0.8917 - val_loss: 0.3951 - val_binary_accuracy: 0.8000 - val_sensitivity: 0.8461 - val_specificity: 0.6918 - val_gmeasure: 0.7606 - val_auc: 0.8743\n",
      "Epoch 78/100\n",
      "600/600 [==============================] - 0s 186us/step - loss: 0.3619 - binary_accuracy: 0.8200 - sensitivity: 0.8307 - specificity: 0.7989 - gmeasure: 0.8104 - auc: 0.8843 - val_loss: 0.3909 - val_binary_accuracy: 0.8000 - val_sensitivity: 0.8461 - val_specificity: 0.6918 - val_gmeasure: 0.7606 - val_auc: 0.8737\n",
      "Epoch 79/100\n",
      "600/600 [==============================] - 0s 195us/step - loss: 0.3469 - binary_accuracy: 0.8283 - sensitivity: 0.8476 - specificity: 0.7962 - gmeasure: 0.8185 - auc: 0.8892 - val_loss: 0.3623 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8371 - val_specificity: 0.7660 - val_gmeasure: 0.7959 - val_auc: 0.8833\n",
      "Epoch 80/100\n",
      "600/600 [==============================] - 0s 184us/step - loss: 0.3465 - binary_accuracy: 0.8283 - sensitivity: 0.8391 - specificity: 0.8099 - gmeasure: 0.8210 - auc: 0.8953 - val_loss: 0.3758 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8461 - val_specificity: 0.7660 - val_gmeasure: 0.8007 - val_auc: 0.8791\n",
      "Epoch 81/100\n",
      "600/600 [==============================] - 0s 202us/step - loss: 0.3413 - binary_accuracy: 0.8317 - sensitivity: 0.8322 - specificity: 0.8160 - gmeasure: 0.8218 - auc: 0.8892 - val_loss: 0.3740 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8461 - val_specificity: 0.7660 - val_gmeasure: 0.8007 - val_auc: 0.8752\n",
      "Epoch 82/100\n",
      "600/600 [==============================] - 0s 189us/step - loss: 0.3439 - binary_accuracy: 0.8283 - sensitivity: 0.8415 - specificity: 0.7888 - gmeasure: 0.8125 - auc: 0.8883 - val_loss: 0.3615 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8371 - val_specificity: 0.7660 - val_gmeasure: 0.7959 - val_auc: 0.8826\n",
      "Epoch 83/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.3363 - binary_accuracy: 0.8350 - sensitivity: 0.8455 - specificity: 0.8044 - gmeasure: 0.8232 - auc: 0.8963 - val_loss: 0.3628 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8461 - val_specificity: 0.7660 - val_gmeasure: 0.8007 - val_auc: 0.8819\n",
      "Epoch 84/100\n",
      "600/600 [==============================] - 0s 200us/step - loss: 0.3345 - binary_accuracy: 0.8367 - sensitivity: 0.8411 - specificity: 0.8245 - gmeasure: 0.8301 - auc: 0.8939 - val_loss: 0.3715 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8461 - val_specificity: 0.7660 - val_gmeasure: 0.8007 - val_auc: 0.8817\n",
      "Epoch 85/100\n",
      "600/600 [==============================] - 0s 203us/step - loss: 0.3347 - binary_accuracy: 0.8417 - sensitivity: 0.8414 - specificity: 0.8414 - gmeasure: 0.8400 - auc: 0.8920 - val_loss: 0.3569 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8371 - val_specificity: 0.7869 - val_gmeasure: 0.8082 - val_auc: 0.8819\n",
      "Epoch 86/100\n",
      "600/600 [==============================] - 0s 211us/step - loss: 0.3308 - binary_accuracy: 0.8400 - sensitivity: 0.8476 - specificity: 0.8215 - gmeasure: 0.8327 - auc: 0.8895 - val_loss: 0.3605 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8371 - val_specificity: 0.7660 - val_gmeasure: 0.7959 - val_auc: 0.8825\n",
      "Epoch 87/100\n",
      "600/600 [==============================] - 0s 215us/step - loss: 0.3293 - binary_accuracy: 0.8417 - sensitivity: 0.8485 - specificity: 0.8259 - gmeasure: 0.8351 - auc: 0.8975 - val_loss: 0.3623 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8461 - val_specificity: 0.7660 - val_gmeasure: 0.8007 - val_auc: 0.8819\n",
      "Epoch 88/100\n",
      "600/600 [==============================] - 0s 210us/step - loss: 0.3256 - binary_accuracy: 0.8417 - sensitivity: 0.8446 - specificity: 0.8370 - gmeasure: 0.8395 - auc: 0.8965 - val_loss: 0.3607 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8461 - val_specificity: 0.7660 - val_gmeasure: 0.8007 - val_auc: 0.8826\n",
      "Epoch 89/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.3285 - binary_accuracy: 0.8400 - sensitivity: 0.8491 - specificity: 0.8067 - gmeasure: 0.8241 - auc: 0.8969 - val_loss: 0.3535 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8283 - val_specificity: 0.7869 - val_gmeasure: 0.8039 - val_auc: 0.8835\n",
      "Epoch 90/100\n",
      "600/600 [==============================] - 0s 204us/step - loss: 0.3326 - binary_accuracy: 0.8400 - sensitivity: 0.8437 - specificity: 0.8403 - gmeasure: 0.8404 - auc: 0.8932 - val_loss: 0.3546 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8371 - val_specificity: 0.7869 - val_gmeasure: 0.8082 - val_auc: 0.8876\n",
      "Epoch 91/100\n",
      "600/600 [==============================] - 0s 201us/step - loss: 0.3214 - binary_accuracy: 0.8483 - sensitivity: 0.8424 - specificity: 0.8690 - gmeasure: 0.8546 - auc: 0.9004 - val_loss: 0.3707 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8461 - val_specificity: 0.7660 - val_gmeasure: 0.8007 - val_auc: 0.8904\n",
      "Epoch 92/100\n",
      "600/600 [==============================] - 0s 202us/step - loss: 0.3197 - binary_accuracy: 0.8417 - sensitivity: 0.8410 - specificity: 0.8423 - gmeasure: 0.8404 - auc: 0.8940 - val_loss: 0.3552 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8371 - val_specificity: 0.7869 - val_gmeasure: 0.8082 - val_auc: 0.8839\n",
      "Epoch 93/100\n",
      "600/600 [==============================] - 0s 214us/step - loss: 0.3221 - binary_accuracy: 0.8433 - sensitivity: 0.8415 - specificity: 0.8437 - gmeasure: 0.8392 - auc: 0.9042 - val_loss: 0.3694 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8461 - val_specificity: 0.7660 - val_gmeasure: 0.8007 - val_auc: 0.8817\n",
      "Epoch 94/100\n",
      "600/600 [==============================] - 0s 207us/step - loss: 0.3180 - binary_accuracy: 0.8450 - sensitivity: 0.8405 - specificity: 0.8654 - gmeasure: 0.8504 - auc: 0.9027 - val_loss: 0.3601 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8371 - val_specificity: 0.7869 - val_gmeasure: 0.8082 - val_auc: 0.8817\n",
      "Epoch 95/100\n",
      "600/600 [==============================] - 0s 203us/step - loss: 0.3114 - binary_accuracy: 0.8500 - sensitivity: 0.8490 - specificity: 0.8480 - gmeasure: 0.8472 - auc: 0.9000 - val_loss: 0.3493 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8371 - val_specificity: 0.7869 - val_gmeasure: 0.8082 - val_auc: 0.8857\n",
      "Epoch 96/100\n",
      "600/600 [==============================] - 0s 190us/step - loss: 0.3118 - binary_accuracy: 0.8417 - sensitivity: 0.8415 - specificity: 0.8427 - gmeasure: 0.8379 - auc: 0.9075 - val_loss: 0.3853 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8461 - val_specificity: 0.7382 - val_gmeasure: 0.7858 - val_auc: 0.8862\n",
      "Epoch 97/100\n",
      "600/600 [==============================] - 0s 202us/step - loss: 0.3190 - binary_accuracy: 0.8417 - sensitivity: 0.8378 - specificity: 0.8604 - gmeasure: 0.8461 - auc: 0.9055 - val_loss: 0.3702 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8461 - val_specificity: 0.7660 - val_gmeasure: 0.8007 - val_auc: 0.8855\n",
      "Epoch 98/100\n",
      "600/600 [==============================] - 0s 212us/step - loss: 0.3108 - binary_accuracy: 0.8483 - sensitivity: 0.8422 - specificity: 0.8636 - gmeasure: 0.8510 - auc: 0.9016 - val_loss: 0.3504 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8371 - val_specificity: 0.7869 - val_gmeasure: 0.8082 - val_auc: 0.8848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100\n",
      "600/600 [==============================] - 0s 206us/step - loss: 0.3078 - binary_accuracy: 0.8483 - sensitivity: 0.8474 - specificity: 0.8468 - gmeasure: 0.8457 - auc: 0.9029 - val_loss: 0.3527 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8371 - val_specificity: 0.7869 - val_gmeasure: 0.8082 - val_auc: 0.8839\n",
      "Epoch 100/100\n",
      "600/600 [==============================] - 0s 204us/step - loss: 0.3054 - binary_accuracy: 0.8500 - sensitivity: 0.8483 - specificity: 0.8591 - gmeasure: 0.8523 - auc: 0.9031 - val_loss: 0.3503 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8371 - val_specificity: 0.7869 - val_gmeasure: 0.8082 - val_auc: 0.8848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:151] Training end with time 14.606053829193115!\n",
      "[root    |INFO|build_network.py:79] Saved trained model weight at ./example_result/weight_1.h5 \n",
      "[root    |DEBUG|deepbiome.py:166] Save weight at ./example_result/weight_1.h5\n",
      "[root    |DEBUG|deepbiome.py:169] Save history at ./example_result/hist_1.json\n",
      "[root    |INFO|build_network.py:177] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "750/750 [==============================] - 0s 8us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:182] Evaluation end with time 0.013545036315917969!\n",
      "[root    |INFO|build_network.py:183] Evaluation: [0.3132685422897339, 0.8493333458900452, 0.8424908518791199, 0.8676470518112183, 0.8549764156341553, 0.9006454944610596]\n",
      "[root    |INFO|build_network.py:177] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "250/250 [==============================] - 0s 24us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:182] Evaluation end with time 0.012819051742553711!\n",
      "[root    |INFO|build_network.py:183] Evaluation: [0.37076306343078613, 0.8240000009536743, 0.8448275923728943, 0.7763158082962036, 0.8098475337028503, 0.8821082711219788]\n",
      "[root    |INFO|deepbiome.py:179] Compute time : 16.150416612625122\n",
      "[root    |INFO|deepbiome.py:180] 2 fold computing end!---------------------------------------------\n",
      "[root    |INFO|deepbiome.py:137] -------3 simulation start!----------------------------------\n",
      "[root    |INFO|readers.py:58] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:59] Construct Dataset\n",
      "[root    |INFO|readers.py:60] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:61] Load data\n",
      "[root    |INFO|deepbiome.py:147] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:148] Build network for 3 simulation\n",
      "[root    |INFO|build_network.py:513] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:514] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv\n",
      "[root    |INFO|build_network.py:518] Phylogenetic tree level list: ['Genus', 'Family', 'Order', 'Class', 'Phylum']\n",
      "[root    |INFO|build_network.py:519] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:526]      Genus: 48\n",
      "[root    |INFO|build_network.py:526]     Family: 40\n",
      "[root    |INFO|build_network.py:526]      Order: 23\n",
      "[root    |INFO|build_network.py:526]      Class: 17\n",
      "[root    |INFO|build_network.py:526]     Phylum: 9\n",
      "[root    |INFO|build_network.py:529] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:530] Phylogenetic_tree_dict info: ['Genus', 'Number', 'Order', 'Class', 'Phylum', 'Family']\n",
      "[root    |INFO|build_network.py:531] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [ Genus, Family]\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [Family,  Order]\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [ Order,  Class]\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [ Class, Phylum]\n",
      "[root    |INFO|build_network.py:554] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:570] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:571] Build network based on phylogenetic tree information\n",
      "[root    |INFO|build_network.py:572] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:648] ------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "l1_dense (Dense_with_tree)   (None, 40)                1960      \n",
      "_________________________________________________________________\n",
      "l1_activation (Activation)   (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "l2_dense (Dense_with_tree)   (None, 23)                943       \n",
      "_________________________________________________________________\n",
      "l2_activation (Activation)   (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "l3_dense (Dense_with_tree)   (None, 17)                408       \n",
      "_________________________________________________________________\n",
      "l3_activation (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "l4_dense (Dense_with_tree)   (None, 9)                 162       \n",
      "_________________________________________________________________\n",
      "l4_activation (Activation)   (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "last_dense_h (Dense)         (None, 1)                 10        \n",
      "_________________________________________________________________\n",
      "p_hat (Activation)           (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,483\n",
      "Trainable params: 3,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:57] Build Network\n",
      "[root    |INFO|build_network.py:58] Optimizer = adam\n",
      "[root    |INFO|build_network.py:59] Loss = binary_crossentropy\n",
      "[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc\n",
      "[root    |INFO|deepbiome.py:157] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:158] 3 fold computing start!----------------------------------\n",
      "[root    |INFO|build_network.py:141] Training start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 150 samples\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 1s 945us/step - loss: 0.6727 - binary_accuracy: 0.6700 - sensitivity: 0.9571 - specificity: 0.0294 - gmeasure: 0.0345 - auc: 0.5526 - val_loss: 0.6562 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5027\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 0s 212us/step - loss: 0.6327 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5571 - val_loss: 0.6471 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5108\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 0s 211us/step - loss: 0.6201 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5831 - val_loss: 0.6511 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5466\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 0s 202us/step - loss: 0.6213 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6055 - val_loss: 0.6493 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5886\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 0s 205us/step - loss: 0.6207 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6283 - val_loss: 0.6494 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6166\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 0s 221us/step - loss: 0.6242 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6610 - val_loss: 0.6460 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6523\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 0s 204us/step - loss: 0.6212 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6802 - val_loss: 0.6485 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6555\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 0s 165us/step - loss: 0.6208 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6604 - val_loss: 0.6487 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6724\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6819 - val_loss: 0.6475 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6786\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6902 - val_loss: 0.6478 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6921\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.6214 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7055 - val_loss: 0.6507 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6955\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.6210 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7163 - val_loss: 0.6474 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7108\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 0s 200us/step - loss: 0.6214 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7424 - val_loss: 0.6469 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7362\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.6210 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7609 - val_loss: 0.6490 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7546\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 0s 211us/step - loss: 0.6209 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7896 - val_loss: 0.6483 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7750\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 0s 200us/step - loss: 0.6196 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8109 - val_loss: 0.6468 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7822\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 0s 207us/step - loss: 0.6197 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8398 - val_loss: 0.6454 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7954\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 0s 218us/step - loss: 0.6186 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8429 - val_loss: 0.6461 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8070\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 0s 195us/step - loss: 0.6180 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8581 - val_loss: 0.6461 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8090\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 0s 193us/step - loss: 0.6168 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8637 - val_loss: 0.6444 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8159\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.6152 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8666 - val_loss: 0.6439 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8237\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 0s 205us/step - loss: 0.6125 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8643 - val_loss: 0.6409 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8274\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 0s 205us/step - loss: 0.6088 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8666 - val_loss: 0.6376 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8309\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 0s 211us/step - loss: 0.6052 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8658 - val_loss: 0.6340 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.5990 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8621 - val_loss: 0.6355 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8311\n",
      "Epoch 26/100\n",
      "600/600 [==============================] - 0s 215us/step - loss: 0.5922 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8571 - val_loss: 0.6257 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8350\n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 0s 190us/step - loss: 0.5834 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8621 - val_loss: 0.6234 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8313\n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 0s 196us/step - loss: 0.5703 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8611 - val_loss: 0.6218 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8326\n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 0s 206us/step - loss: 0.5568 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8695 - val_loss: 0.6038 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8427\n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 0s 217us/step - loss: 0.5402 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8784 - val_loss: 0.6115 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8486\n",
      "Epoch 31/100\n",
      "600/600 [==============================] - 0s 214us/step - loss: 0.5244 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8889 - val_loss: 0.5794 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8590\n",
      "Epoch 32/100\n",
      "600/600 [==============================] - 0s 212us/step - loss: 0.5045 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.9018 - val_loss: 0.5636 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8688\n",
      "Epoch 33/100\n",
      "600/600 [==============================] - 0s 225us/step - loss: 0.4838 - binary_accuracy: 0.7467 - sensitivity: 0.9901 - specificity: 0.2166 - gmeasure: 0.4113 - auc: 0.8978 - val_loss: 0.5656 - val_binary_accuracy: 0.7200 - val_sensitivity: 1.0000 - val_specificity: 0.2022 - val_gmeasure: 0.4388 - val_auc: 0.8721\n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.4699 - binary_accuracy: 0.7817 - sensitivity: 0.9828 - specificity: 0.3467 - gmeasure: 0.5740 - auc: 0.9062 - val_loss: 0.5281 - val_binary_accuracy: 0.7600 - val_sensitivity: 1.0000 - val_specificity: 0.3342 - val_gmeasure: 0.5680 - val_auc: 0.8860\n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 0s 210us/step - loss: 0.4492 - binary_accuracy: 0.8017 - sensitivity: 0.9904 - specificity: 0.3937 - gmeasure: 0.6147 - auc: 0.9158 - val_loss: 0.5507 - val_binary_accuracy: 0.7467 - val_sensitivity: 1.0000 - val_specificity: 0.2881 - val_gmeasure: 0.5307 - val_auc: 0.8858\n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 0s 179us/step - loss: 0.4478 - binary_accuracy: 0.8200 - sensitivity: 0.9852 - specificity: 0.4695 - gmeasure: 0.6449 - auc: 0.9294 - val_loss: 0.4947 - val_binary_accuracy: 0.7800 - val_sensitivity: 1.0000 - val_specificity: 0.3854 - val_gmeasure: 0.6162 - val_auc: 0.8947\n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 0s 184us/step - loss: 0.4121 - binary_accuracy: 0.8017 - sensitivity: 0.9882 - specificity: 0.3980 - gmeasure: 0.6138 - auc: 0.9307 - val_loss: 0.4736 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.9660 - val_specificity: 0.6095 - val_gmeasure: 0.7660 - val_auc: 0.9047\n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.4105 - binary_accuracy: 0.8400 - sensitivity: 0.9864 - specificity: 0.5395 - gmeasure: 0.7159 - auc: 0.9408 - val_loss: 0.4539 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.9660 - val_specificity: 0.6095 - val_gmeasure: 0.7660 - val_auc: 0.9083\n",
      "Epoch 39/100\n",
      "600/600 [==============================] - 0s 199us/step - loss: 0.3914 - binary_accuracy: 0.8367 - sensitivity: 0.9907 - specificity: 0.5032 - gmeasure: 0.6967 - auc: 0.9467 - val_loss: 0.4417 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.9565 - val_specificity: 0.6462 - val_gmeasure: 0.7855 - val_auc: 0.9118\n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 0s 204us/step - loss: 0.3771 - binary_accuracy: 0.8517 - sensitivity: 0.9815 - specificity: 0.5919 - gmeasure: 0.7529 - auc: 0.9539 - val_loss: 0.4259 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.9877 - val_specificity: 0.5805 - val_gmeasure: 0.7566 - val_auc: 0.9148\n",
      "Epoch 41/100\n",
      "600/600 [==============================] - 0s 187us/step - loss: 0.3744 - binary_accuracy: 0.8517 - sensitivity: 0.9748 - specificity: 0.5967 - gmeasure: 0.7536 - auc: 0.9517 - val_loss: 0.4203 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.9877 - val_specificity: 0.5438 - val_gmeasure: 0.7312 - val_auc: 0.9170\n",
      "Epoch 42/100\n",
      "600/600 [==============================] - 0s 211us/step - loss: 0.3696 - binary_accuracy: 0.8733 - sensitivity: 0.9674 - specificity: 0.6891 - gmeasure: 0.8062 - auc: 0.9602 - val_loss: 0.4314 - val_binary_accuracy: 0.8133 - val_sensitivity: 1.0000 - val_specificity: 0.4671 - val_gmeasure: 0.6832 - val_auc: 0.9152\n",
      "Epoch 43/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.3451 - binary_accuracy: 0.8867 - sensitivity: 0.9789 - specificity: 0.6729 - gmeasure: 0.8028 - auc: 0.9577 - val_loss: 0.4361 - val_binary_accuracy: 0.8000 - val_sensitivity: 1.0000 - val_specificity: 0.4288 - val_gmeasure: 0.6544 - val_auc: 0.9202\n",
      "Epoch 44/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.3369 - binary_accuracy: 0.8850 - sensitivity: 0.9759 - specificity: 0.6827 - gmeasure: 0.8103 - auc: 0.9609 - val_loss: 0.4210 - val_binary_accuracy: 0.8200 - val_sensitivity: 1.0000 - val_specificity: 0.4816 - val_gmeasure: 0.6939 - val_auc: 0.9232\n",
      "Epoch 45/100\n",
      "600/600 [==============================] - 0s 217us/step - loss: 0.3284 - binary_accuracy: 0.8717 - sensitivity: 0.9810 - specificity: 0.6311 - gmeasure: 0.7737 - auc: 0.9663 - val_loss: 0.3899 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.9877 - val_specificity: 0.5438 - val_gmeasure: 0.7312 - val_auc: 0.9259\n",
      "Epoch 46/100\n",
      "600/600 [==============================] - 0s 176us/step - loss: 0.3125 - binary_accuracy: 0.9067 - sensitivity: 0.9812 - specificity: 0.7430 - gmeasure: 0.8518 - auc: 0.9646 - val_loss: 0.4486 - val_binary_accuracy: 0.8067 - val_sensitivity: 1.0000 - val_specificity: 0.4433 - val_gmeasure: 0.6657 - val_auc: 0.9294\n",
      "Epoch 47/100\n",
      "600/600 [==============================] - 0s 175us/step - loss: 0.3150 - binary_accuracy: 0.8917 - sensitivity: 0.9783 - specificity: 0.6934 - gmeasure: 0.8143 - auc: 0.9655 - val_loss: 0.3610 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.9658 - val_specificity: 0.6317 - val_gmeasure: 0.7808 - val_auc: 0.9327\n",
      "Epoch 48/100\n",
      "600/600 [==============================] - 0s 192us/step - loss: 0.2954 - binary_accuracy: 0.9000 - sensitivity: 0.9757 - specificity: 0.7291 - gmeasure: 0.8372 - auc: 0.9656 - val_loss: 0.3510 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.9658 - val_specificity: 0.6684 - val_gmeasure: 0.8030 - val_auc: 0.9364\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 0s 220us/step - loss: 0.2864 - binary_accuracy: 0.9100 - sensitivity: 0.9764 - specificity: 0.7661 - gmeasure: 0.8629 - auc: 0.9690 - val_loss: 0.3460 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.9658 - val_specificity: 0.6462 - val_gmeasure: 0.7893 - val_auc: 0.9388\n",
      "Epoch 50/100\n",
      "600/600 [==============================] - 0s 216us/step - loss: 0.2779 - binary_accuracy: 0.9150 - sensitivity: 0.9692 - specificity: 0.8009 - gmeasure: 0.8797 - auc: 0.9745 - val_loss: 0.3476 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.9781 - val_specificity: 0.6317 - val_gmeasure: 0.7859 - val_auc: 0.9368\n",
      "Epoch 51/100\n",
      "600/600 [==============================] - 0s 199us/step - loss: 0.2694 - binary_accuracy: 0.9117 - sensitivity: 0.9723 - specificity: 0.7651 - gmeasure: 0.8605 - auc: 0.9716 - val_loss: 0.3379 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.9658 - val_specificity: 0.6317 - val_gmeasure: 0.7808 - val_auc: 0.9408\n",
      "Epoch 52/100\n",
      "600/600 [==============================] - 0s 200us/step - loss: 0.2641 - binary_accuracy: 0.9200 - sensitivity: 0.9709 - specificity: 0.8161 - gmeasure: 0.8867 - auc: 0.9754 - val_loss: 0.3404 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.9781 - val_specificity: 0.6317 - val_gmeasure: 0.7859 - val_auc: 0.9429\n",
      "Epoch 53/100\n",
      "600/600 [==============================] - 0s 205us/step - loss: 0.2620 - binary_accuracy: 0.9117 - sensitivity: 0.9662 - specificity: 0.7884 - gmeasure: 0.8681 - auc: 0.9772 - val_loss: 0.3261 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.9563 - val_specificity: 0.6684 - val_gmeasure: 0.7991 - val_auc: 0.9450\n",
      "Epoch 54/100\n",
      "600/600 [==============================] - 0s 207us/step - loss: 0.2560 - binary_accuracy: 0.9167 - sensitivity: 0.9698 - specificity: 0.7896 - gmeasure: 0.8696 - auc: 0.9738 - val_loss: 0.3195 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.9563 - val_specificity: 0.6829 - val_gmeasure: 0.8073 - val_auc: 0.9467\n",
      "Epoch 55/100\n",
      "600/600 [==============================] - 0s 194us/step - loss: 0.2519 - binary_accuracy: 0.9267 - sensitivity: 0.9670 - specificity: 0.8420 - gmeasure: 0.9004 - auc: 0.9798 - val_loss: 0.3144 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.9439 - val_specificity: 0.7289 - val_gmeasure: 0.8291 - val_auc: 0.9474\n",
      "Epoch 56/100\n",
      "600/600 [==============================] - 0s 207us/step - loss: 0.2410 - binary_accuracy: 0.9233 - sensitivity: 0.9723 - specificity: 0.8135 - gmeasure: 0.8876 - auc: 0.9769 - val_loss: 0.3050 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.9316 - val_specificity: 0.7765 - val_gmeasure: 0.8503 - val_auc: 0.9500\n",
      "Epoch 57/100\n",
      "600/600 [==============================] - 0s 198us/step - loss: 0.2459 - binary_accuracy: 0.9233 - sensitivity: 0.9620 - specificity: 0.8446 - gmeasure: 0.8999 - auc: 0.9790 - val_loss: 0.2995 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.9316 - val_specificity: 0.7765 - val_gmeasure: 0.8503 - val_auc: 0.9525\n",
      "Epoch 58/100\n",
      "600/600 [==============================] - 0s 198us/step - loss: 0.2425 - binary_accuracy: 0.9283 - sensitivity: 0.9743 - specificity: 0.8324 - gmeasure: 0.8974 - auc: 0.9780 - val_loss: 0.2976 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9128 - val_specificity: 0.8371 - val_gmeasure: 0.8740 - val_auc: 0.9538\n",
      "Epoch 59/100\n",
      "600/600 [==============================] - 0s 212us/step - loss: 0.2341 - binary_accuracy: 0.9233 - sensitivity: 0.9729 - specificity: 0.8176 - gmeasure: 0.8899 - auc: 0.9795 - val_loss: 0.2913 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9316 - val_specificity: 0.7988 - val_gmeasure: 0.8626 - val_auc: 0.9555\n",
      "Epoch 60/100\n",
      "600/600 [==============================] - 0s 211us/step - loss: 0.2234 - binary_accuracy: 0.9317 - sensitivity: 0.9647 - specificity: 0.8515 - gmeasure: 0.9042 - auc: 0.9777 - val_loss: 0.3086 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.9686 - val_specificity: 0.6462 - val_gmeasure: 0.7908 - val_auc: 0.9542\n",
      "Epoch 61/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.2173 - binary_accuracy: 0.9383 - sensitivity: 0.9662 - specificity: 0.8869 - gmeasure: 0.9230 - auc: 0.9811 - val_loss: 0.3353 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.9781 - val_specificity: 0.6095 - val_gmeasure: 0.7713 - val_auc: 0.9534\n",
      "Epoch 62/100\n",
      "600/600 [==============================] - 0s 210us/step - loss: 0.2263 - binary_accuracy: 0.9267 - sensitivity: 0.9670 - specificity: 0.8481 - gmeasure: 0.9029 - auc: 0.9786 - val_loss: 0.2922 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.9439 - val_specificity: 0.7305 - val_gmeasure: 0.8299 - val_auc: 0.9549\n",
      "Epoch 63/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.2155 - binary_accuracy: 0.9350 - sensitivity: 0.9682 - specificity: 0.8658 - gmeasure: 0.9138 - auc: 0.9810 - val_loss: 0.2812 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9316 - val_specificity: 0.7988 - val_gmeasure: 0.8626 - val_auc: 0.9557\n",
      "Epoch 64/100\n",
      "600/600 [==============================] - 0s 227us/step - loss: 0.2108 - binary_accuracy: 0.9333 - sensitivity: 0.9714 - specificity: 0.8520 - gmeasure: 0.9075 - auc: 0.9784 - val_loss: 0.2812 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9128 - val_specificity: 0.8371 - val_gmeasure: 0.8740 - val_auc: 0.9548\n",
      "Epoch 65/100\n",
      "600/600 [==============================] - 0s 234us/step - loss: 0.2220 - binary_accuracy: 0.9333 - sensitivity: 0.9640 - specificity: 0.8665 - gmeasure: 0.9106 - auc: 0.9813 - val_loss: 0.2871 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.9439 - val_specificity: 0.7382 - val_gmeasure: 0.8348 - val_auc: 0.9574\n",
      "Epoch 66/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.2127 - binary_accuracy: 0.9300 - sensitivity: 0.9582 - specificity: 0.8632 - gmeasure: 0.9069 - auc: 0.9821 - val_loss: 0.3111 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.9686 - val_specificity: 0.6700 - val_gmeasure: 0.8053 - val_auc: 0.9584\n",
      "Epoch 67/100\n",
      "600/600 [==============================] - 0s 202us/step - loss: 0.1997 - binary_accuracy: 0.9450 - sensitivity: 0.9660 - specificity: 0.8976 - gmeasure: 0.9300 - auc: 0.9814 - val_loss: 0.3083 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.9686 - val_specificity: 0.6700 - val_gmeasure: 0.8053 - val_auc: 0.9578\n",
      "Epoch 68/100\n",
      "600/600 [==============================] - 0s 205us/step - loss: 0.2006 - binary_accuracy: 0.9467 - sensitivity: 0.9703 - specificity: 0.8902 - gmeasure: 0.9277 - auc: 0.9827 - val_loss: 0.2855 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.9563 - val_specificity: 0.7382 - val_gmeasure: 0.8402 - val_auc: 0.9579\n",
      "Epoch 69/100\n",
      "600/600 [==============================] - 0s 207us/step - loss: 0.1968 - binary_accuracy: 0.9433 - sensitivity: 0.9690 - specificity: 0.8936 - gmeasure: 0.9297 - auc: 0.9861 - val_loss: 0.2958 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9686 - val_specificity: 0.7382 - val_gmeasure: 0.8455 - val_auc: 0.9574\n",
      "Epoch 70/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.1914 - binary_accuracy: 0.9350 - sensitivity: 0.9663 - specificity: 0.8612 - gmeasure: 0.9111 - auc: 0.9806 - val_loss: 0.2761 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9439 - val_specificity: 0.7765 - val_gmeasure: 0.8560 - val_auc: 0.9581\n",
      "Epoch 71/100\n",
      "600/600 [==============================] - 0s 203us/step - loss: 0.1855 - binary_accuracy: 0.9433 - sensitivity: 0.9696 - specificity: 0.8715 - gmeasure: 0.9185 - auc: 0.9817 - val_loss: 0.2642 - val_binary_accuracy: 0.9000 - val_sensitivity: 0.9223 - val_specificity: 0.8516 - val_gmeasure: 0.8858 - val_auc: 0.9589\n",
      "Epoch 72/100\n",
      "600/600 [==============================] - 0s 211us/step - loss: 0.1876 - binary_accuracy: 0.9433 - sensitivity: 0.9665 - specificity: 0.8968 - gmeasure: 0.9303 - auc: 0.9828 - val_loss: 0.2695 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9439 - val_specificity: 0.7765 - val_gmeasure: 0.8560 - val_auc: 0.9594\n",
      "Epoch 73/100\n",
      "600/600 [==============================] - 0s 210us/step - loss: 0.1838 - binary_accuracy: 0.9467 - sensitivity: 0.9679 - specificity: 0.9025 - gmeasure: 0.9333 - auc: 0.9831 - val_loss: 0.2681 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.9439 - val_specificity: 0.7988 - val_gmeasure: 0.8683 - val_auc: 0.9584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.1825 - binary_accuracy: 0.9400 - sensitivity: 0.9682 - specificity: 0.8806 - gmeasure: 0.9223 - auc: 0.9822 - val_loss: 0.2622 - val_binary_accuracy: 0.9000 - val_sensitivity: 0.9128 - val_specificity: 0.8754 - val_gmeasure: 0.8934 - val_auc: 0.9578\n",
      "Epoch 75/100\n",
      "600/600 [==============================] - 0s 214us/step - loss: 0.1825 - binary_accuracy: 0.9333 - sensitivity: 0.9657 - specificity: 0.8641 - gmeasure: 0.9126 - auc: 0.9853 - val_loss: 0.2787 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9686 - val_specificity: 0.7382 - val_gmeasure: 0.8455 - val_auc: 0.9604\n",
      "Epoch 76/100\n",
      "600/600 [==============================] - 0s 194us/step - loss: 0.1794 - binary_accuracy: 0.9483 - sensitivity: 0.9687 - specificity: 0.9020 - gmeasure: 0.9339 - auc: 0.9848 - val_loss: 0.3300 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.9781 - val_specificity: 0.6555 - val_gmeasure: 0.8004 - val_auc: 0.9592\n",
      "Epoch 77/100\n",
      "600/600 [==============================] - 0s 192us/step - loss: 0.1827 - binary_accuracy: 0.9433 - sensitivity: 0.9647 - specificity: 0.9030 - gmeasure: 0.9320 - auc: 0.9823 - val_loss: 0.3038 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9781 - val_specificity: 0.7160 - val_gmeasure: 0.8365 - val_auc: 0.9605\n",
      "Epoch 78/100\n",
      "600/600 [==============================] - 0s 219us/step - loss: 0.1862 - binary_accuracy: 0.9383 - sensitivity: 0.9671 - specificity: 0.8742 - gmeasure: 0.9165 - auc: 0.9846 - val_loss: 0.2766 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9686 - val_specificity: 0.7382 - val_gmeasure: 0.8455 - val_auc: 0.9611\n",
      "Epoch 79/100\n",
      "600/600 [==============================] - 0s 199us/step - loss: 0.1971 - binary_accuracy: 0.9333 - sensitivity: 0.9607 - specificity: 0.8784 - gmeasure: 0.9168 - auc: 0.9824 - val_loss: 0.2595 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9439 - val_specificity: 0.7765 - val_gmeasure: 0.8560 - val_auc: 0.9611\n",
      "Epoch 80/100\n",
      "600/600 [==============================] - 0s 206us/step - loss: 0.1824 - binary_accuracy: 0.9400 - sensitivity: 0.9703 - specificity: 0.8695 - gmeasure: 0.9160 - auc: 0.9821 - val_loss: 0.2501 - val_binary_accuracy: 0.9000 - val_sensitivity: 0.9128 - val_specificity: 0.8754 - val_gmeasure: 0.8934 - val_auc: 0.9601\n",
      "Epoch 81/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.1698 - binary_accuracy: 0.9433 - sensitivity: 0.9685 - specificity: 0.8874 - gmeasure: 0.9257 - auc: 0.9850 - val_loss: 0.2483 - val_binary_accuracy: 0.9000 - val_sensitivity: 0.9223 - val_specificity: 0.8516 - val_gmeasure: 0.8858 - val_auc: 0.9606\n",
      "Epoch 82/100\n",
      "600/600 [==============================] - 0s 199us/step - loss: 0.1690 - binary_accuracy: 0.9533 - sensitivity: 0.9708 - specificity: 0.9219 - gmeasure: 0.9450 - auc: 0.9855 - val_loss: 0.2471 - val_binary_accuracy: 0.9133 - val_sensitivity: 0.9439 - val_specificity: 0.8516 - val_gmeasure: 0.8963 - val_auc: 0.9613\n",
      "Epoch 83/100\n",
      "600/600 [==============================] - 0s 210us/step - loss: 0.1671 - binary_accuracy: 0.9500 - sensitivity: 0.9663 - specificity: 0.9151 - gmeasure: 0.9397 - auc: 0.9858 - val_loss: 0.2789 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9686 - val_specificity: 0.7382 - val_gmeasure: 0.8455 - val_auc: 0.9634\n",
      "Epoch 84/100\n",
      "600/600 [==============================] - 0s 211us/step - loss: 0.1699 - binary_accuracy: 0.9450 - sensitivity: 0.9606 - specificity: 0.9153 - gmeasure: 0.9362 - auc: 0.9867 - val_loss: 0.2655 - val_binary_accuracy: 0.9000 - val_sensitivity: 0.9686 - val_specificity: 0.7765 - val_gmeasure: 0.8670 - val_auc: 0.9629\n",
      "Epoch 85/100\n",
      "600/600 [==============================] - 0s 210us/step - loss: 0.1679 - binary_accuracy: 0.9450 - sensitivity: 0.9700 - specificity: 0.8891 - gmeasure: 0.9268 - auc: 0.9875 - val_loss: 0.2474 - val_binary_accuracy: 0.9133 - val_sensitivity: 0.9439 - val_specificity: 0.8516 - val_gmeasure: 0.8963 - val_auc: 0.9625\n",
      "Epoch 86/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.1597 - binary_accuracy: 0.9533 - sensitivity: 0.9713 - specificity: 0.9162 - gmeasure: 0.9428 - auc: 0.9856 - val_loss: 0.2502 - val_binary_accuracy: 0.9000 - val_sensitivity: 0.9439 - val_specificity: 0.8226 - val_gmeasure: 0.8809 - val_auc: 0.9625\n",
      "Epoch 87/100\n",
      "600/600 [==============================] - 0s 205us/step - loss: 0.1568 - binary_accuracy: 0.9517 - sensitivity: 0.9706 - specificity: 0.9132 - gmeasure: 0.9407 - auc: 0.9871 - val_loss: 0.2563 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9439 - val_specificity: 0.7765 - val_gmeasure: 0.8560 - val_auc: 0.9635\n",
      "Epoch 88/100\n",
      "600/600 [==============================] - 0s 219us/step - loss: 0.1593 - binary_accuracy: 0.9550 - sensitivity: 0.9685 - specificity: 0.9223 - gmeasure: 0.9441 - auc: 0.9868 - val_loss: 0.2485 - val_binary_accuracy: 0.9067 - val_sensitivity: 0.9439 - val_specificity: 0.8371 - val_gmeasure: 0.8887 - val_auc: 0.9631\n",
      "Epoch 89/100\n",
      "600/600 [==============================] - 0s 205us/step - loss: 0.1523 - binary_accuracy: 0.9500 - sensitivity: 0.9713 - specificity: 0.9040 - gmeasure: 0.9362 - auc: 0.9887 - val_loss: 0.2770 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.9686 - val_specificity: 0.7620 - val_gmeasure: 0.8586 - val_auc: 0.9636\n",
      "Epoch 90/100\n",
      "600/600 [==============================] - 0s 191us/step - loss: 0.1606 - binary_accuracy: 0.9467 - sensitivity: 0.9691 - specificity: 0.9015 - gmeasure: 0.9337 - auc: 0.9868 - val_loss: 0.2698 - val_binary_accuracy: 0.9000 - val_sensitivity: 0.9686 - val_specificity: 0.7765 - val_gmeasure: 0.8670 - val_auc: 0.9641\n",
      "Epoch 91/100\n",
      "600/600 [==============================] - 0s 201us/step - loss: 0.1522 - binary_accuracy: 0.9467 - sensitivity: 0.9713 - specificity: 0.8975 - gmeasure: 0.9330 - auc: 0.9896 - val_loss: 0.2484 - val_binary_accuracy: 0.9067 - val_sensitivity: 0.9439 - val_specificity: 0.8371 - val_gmeasure: 0.8887 - val_auc: 0.9636\n",
      "Epoch 92/100\n",
      "600/600 [==============================] - 0s 208us/step - loss: 0.1505 - binary_accuracy: 0.9483 - sensitivity: 0.9662 - specificity: 0.9097 - gmeasure: 0.9368 - auc: 0.9884 - val_loss: 0.2395 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9439 - val_specificity: 0.8754 - val_gmeasure: 0.9085 - val_auc: 0.9643\n",
      "Epoch 93/100\n",
      "600/600 [==============================] - 0s 213us/step - loss: 0.1490 - binary_accuracy: 0.9500 - sensitivity: 0.9715 - specificity: 0.9028 - gmeasure: 0.9353 - auc: 0.9870 - val_loss: 0.2480 - val_binary_accuracy: 0.9067 - val_sensitivity: 0.9439 - val_specificity: 0.8371 - val_gmeasure: 0.8887 - val_auc: 0.9654\n",
      "Epoch 94/100\n",
      "600/600 [==============================] - 0s 209us/step - loss: 0.1479 - binary_accuracy: 0.9533 - sensitivity: 0.9703 - specificity: 0.9117 - gmeasure: 0.9400 - auc: 0.9865 - val_loss: 0.2493 - val_binary_accuracy: 0.9067 - val_sensitivity: 0.9439 - val_specificity: 0.8371 - val_gmeasure: 0.8887 - val_auc: 0.9631\n",
      "Epoch 95/100\n",
      "600/600 [==============================] - 0s 214us/step - loss: 0.1477 - binary_accuracy: 0.9517 - sensitivity: 0.9718 - specificity: 0.9124 - gmeasure: 0.9411 - auc: 0.9864 - val_loss: 0.2448 - val_binary_accuracy: 0.9067 - val_sensitivity: 0.9439 - val_specificity: 0.8371 - val_gmeasure: 0.8887 - val_auc: 0.9643\n",
      "Epoch 96/100\n",
      "600/600 [==============================] - 0s 200us/step - loss: 0.1449 - binary_accuracy: 0.9500 - sensitivity: 0.9669 - specificity: 0.9066 - gmeasure: 0.9356 - auc: 0.9884 - val_loss: 0.2648 - val_binary_accuracy: 0.9000 - val_sensitivity: 0.9686 - val_specificity: 0.7765 - val_gmeasure: 0.8670 - val_auc: 0.9659\n",
      "Epoch 97/100\n",
      "600/600 [==============================] - 0s 201us/step - loss: 0.1471 - binary_accuracy: 0.9550 - sensitivity: 0.9705 - specificity: 0.9218 - gmeasure: 0.9453 - auc: 0.9873 - val_loss: 0.2602 - val_binary_accuracy: 0.9000 - val_sensitivity: 0.9686 - val_specificity: 0.7765 - val_gmeasure: 0.8670 - val_auc: 0.9653\n",
      "Epoch 98/100\n",
      "600/600 [==============================] - 0s 199us/step - loss: 0.1464 - binary_accuracy: 0.9517 - sensitivity: 0.9615 - specificity: 0.9249 - gmeasure: 0.9426 - auc: 0.9851 - val_loss: 0.2350 - val_binary_accuracy: 0.9267 - val_sensitivity: 0.9439 - val_specificity: 0.8976 - val_gmeasure: 0.9203 - val_auc: 0.9629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100\n",
      "600/600 [==============================] - 0s 197us/step - loss: 0.1455 - binary_accuracy: 0.9483 - sensitivity: 0.9713 - specificity: 0.9031 - gmeasure: 0.9356 - auc: 0.9886 - val_loss: 0.2334 - val_binary_accuracy: 0.9267 - val_sensitivity: 0.9439 - val_specificity: 0.8976 - val_gmeasure: 0.9203 - val_auc: 0.9607\n",
      "Epoch 100/100\n",
      "600/600 [==============================] - 0s 216us/step - loss: 0.1571 - binary_accuracy: 0.9567 - sensitivity: 0.9737 - specificity: 0.9207 - gmeasure: 0.9460 - auc: 0.9895 - val_loss: 0.2467 - val_binary_accuracy: 0.9067 - val_sensitivity: 0.9035 - val_specificity: 0.9121 - val_gmeasure: 0.9075 - val_auc: 0.9590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:151] Training end with time 14.564016819000244!\n",
      "[root    |INFO|build_network.py:79] Saved trained model weight at ./example_result/weight_2.h5 \n",
      "[root    |DEBUG|deepbiome.py:166] Save weight at ./example_result/weight_2.h5\n",
      "[root    |DEBUG|deepbiome.py:169] Save history at ./example_result/hist_2.json\n",
      "[root    |INFO|build_network.py:177] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "750/750 [==============================] - 0s 8us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:182] Evaluation end with time 0.013907432556152344!\n",
      "[root    |INFO|build_network.py:183] Evaluation: [0.1908942461013794, 0.9333333373069763, 0.9217221140861511, 0.9581589698791504, 0.9397639632225037, 0.9826658964157104]\n",
      "[root    |INFO|build_network.py:177] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "250/250 [==============================] - 0s 22us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:182] Evaluation end with time 0.013725757598876953!\n",
      "[root    |INFO|build_network.py:183] Evaluation: [0.3346303701400757, 0.8519999980926514, 0.8314606547355652, 0.9027777910232544, 0.8663857579231262, 0.930282473564148]\n",
      "[root    |INFO|deepbiome.py:179] Compute time : 16.148853302001953\n",
      "[root    |INFO|deepbiome.py:180] 3 fold computing end!---------------------------------------------\n",
      "[root    |INFO|deepbiome.py:183] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:185] Train Evaluation : ['loss' 'binary_accuracy' 'sensitivity' 'specificity' 'gmeasure' 'auc']\n",
      "[root    |INFO|deepbiome.py:188]       mean : [0.26409377 0.88844444 0.88150728 0.90457902 0.89295071 0.94405381]\n",
      "[root    |INFO|deepbiome.py:189]        std : [0.05276841 0.03453536 0.03235713 0.03878116 0.03517411 0.033656  ]\n",
      "[root    |INFO|deepbiome.py:190] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:192] Test Evaluation : ['loss' 'binary_accuracy' 'sensitivity' 'specificity' 'gmeasure' 'auc']\n",
      "[root    |INFO|deepbiome.py:195]       mean : [0.3420873  0.84133333 0.84673119 0.8313028  0.83841147 0.91362449]\n",
      "[root    |INFO|deepbiome.py:196]        std : [0.02104079 0.01236482 0.01331369 0.05292793 0.0230854  0.02229744]\n",
      "[root    |INFO|deepbiome.py:197] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:206] Total Computing Ended\n",
      "[root    |INFO|deepbiome.py:207] -----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_evaluation, train_evaluation, network = deepbiome.deepbiome_train(log, network_info, path_info, number_of_fold=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `deepbiome_train` save the trained model weights, evaluation results and history based on the path information from the configuration.\n",
    "\n",
    "From the example above, we can check that `hist_*.json`, `weight_*.h5`, `test_eval.npy`, `train_eval.npy` files were saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hist_0.json',\n",
       " 'weight_2.h5',\n",
       " 'test_eval.npy',\n",
       " 'weight_0.h5',\n",
       " 'train_eval.npy',\n",
       " 'hist_2.json',\n",
       " 'weight_1.h5',\n",
       " 'hist_1.json']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path_info['model_info']['model_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the history files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzs3Xd4VFX6wPHvmbRJ74FUEopAaEkI\nTZAiiKALCCIrAooN5aeLrq6uuq4F61oQddldBRsWEEERFUVEUFApoddQQgstlfSe8/vjTkJ6QjJD\ngLyf55knmTPn3nvggbw57T1Ka40QQghRF1NzN0AIIcTFT4KFEEKIekmwEEIIUS8JFkIIIeolwUII\nIUS9JFgIIYSolwQLIYQQ9ZJgIYQQol4SLIQQQtTLvrkbYC1+fn46PDy8uZshhBCXlM2bN6dorf3r\nq3fZBIvw8HDi4uKauxlCCHFJUUodbUg9GYYSQghRLwkWQggh6iXBQgghRL0umzkLIcTlo6ioiMTE\nRPLz85u7KZcNs9lMSEgIDg4OjbpegoUQ4qKTmJiIu7s74eHhKKWauzmXPK01qampJCYmEhER0ah7\nyDCUEOKik5+fj6+vrwQKK1FK4evr26SemgQLIcRFSQKFdTX171OCRV46rPkXnNjS3C0RQoiLlk2D\nhVJqhFIqXil1UCn1WC11Jiil9iildiulPqtQXqKU2mZ5LbNdI02w5kVIWG2zRwghLi1DhgxhxYoV\nlcpmz57N9OnTa73Gzc0NgJMnTzJ+/Pga6wwePLjezcOzZ88mNze3/P11113H2bNnG9p0m7FZsFBK\n2QFzgJFAJDBRKRVZpU4H4HGgv9a6C/BghY/ztNZRltdoW7UTsyd4BEPSPps9QghxaZk4cSILFy6s\nVLZw4UImTpxY77VBQUEsXry40c+uGiyWL1+Ol5dXo+9nLbbsWfQGDmqtE7TWhcBCYEyVOncDc7TW\n6QBa6yQbtqd2/p0gWYKFEMIwfvx4vvvuOwoLCwE4cuQIJ0+eJDo6mqFDhxITE0O3bt34+uuvq117\n5MgRunbtCkBeXh4333wznTt3ZuzYseTl5ZXXmz59OrGxsXTp0oWnn34agLfeeouTJ08yZMgQhgwZ\nAhipjFJSUgCYNWsWXbt2pWvXrsyePbv8eZ07d+buu++mS5cuDB8+vNJzrMWWS2eDgeMV3icCfarU\nuQJAKfUbYAc8o7X+wfKZWSkVBxQDL2utl9qspQGdYdNvUFoCJjubPUYIcf6e/WY3e05mWvWekUEe\nPD2qS62f+/j40Lt3b77//nvGjBnDwoULmTBhAs7Oznz11Vd4eHiQkpJC3759GT16dK2Tx//9739x\ncXFh79697Nixg5iYmPLPXnjhBXx8fCgpKWHo0KHs2LGDGTNmMGvWLFavXo2fn1+le23evJkPPviA\nDRs2oLWmT58+DBo0CG9vbw4cOMCCBQuYO3cuEyZMYMmSJUyePNk6f1kWzT3BbQ90AAYDE4G5Sqmy\n/lYbrXUscAswWynVrurFSqlpSqk4pVRccnJy41vh3wmK8yH9SOPvIYS4rFQciiobgtJa88QTT9C9\ne3eGDRvGiRMnOHPmTK33+PXXX8t/aHfv3p3u3buXf7Zo0SJiYmKIjo5m9+7d7Nmzp872rFu3jrFj\nx+Lq6oqbmxvjxo1j7dq1AERERBAVFQVAz549OXLkSFP+6DWyZc/iBBBa4X2IpayiRGCD1roIOKyU\n2o8RPDZprU8AaK0TlFJrgGjgUMWLtdbvAu8CxMbG6ka31L+T8TU5HnyrxSQhRDOqqwdgS2PGjOGv\nf/0rW7ZsITc3l549e/Lhhx+SnJzM5s2bcXBwIDw8vFF7Fw4fPsxrr73Gpk2b8Pb2ZurUqU3aA+Hk\n5FT+vZ2dnU2GoWzZs9gEdFBKRSilHIGbgaqrmpZi9CpQSvlhDEslKKW8lVJOFcr7A3WH3abw72h8\nTd5rs0cIIS4tbm5uDBkyhDvuuKN8YjsjI4OAgAAcHBxYvXo1R4/Wnd174MCBfPaZschz165d7Nix\nA4DMzExcXV3x9PTkzJkzfP/99+XXuLu7k5WVVe1eV111FUuXLiU3N5ecnBy++uorrrrqKmv9cetl\ns56F1rpYKXU/sAJjPuJ9rfVupdRMIE5rvczy2XCl1B6gBHhEa52qlLoSeEcpVYoR0F7WWtsuWJg9\nwCNEVkQJISqZOHEiY8eOLR+OmjRpEqNGjaJbt27ExsbSqVOnOq+fPn06t99+O507d6Zz58707NkT\ngB49ehAdHU2nTp0IDQ2lf//+5ddMmzaNESNGEBQUxOrV55b0x8TEMHXqVHr37g3AXXfdRXR0tE2G\nnGqitG786M3FJDY2Vjfp8KNPboTsM3DvOus1SgjRKHv37qVz587N3YzLTk1/r0qpzZb54To19wT3\nxcO/E6QcMFZECSGEqESCRRlZESWEELWSYFEmwNI1k815QghRjQSLMmUropJkRZQQQlQlwaKMkzt4\nhhp7LYQQQlQiwaIi/46y10IIIWogwaIiWRElhABSU1OJiooiKiqK1q1bExwcXP6+LLlgfW6//Xbi\n4+seqZgzZw6ffvqpNZpsc3IGd0UBnc+tiPKOgJNbwDME3Fs3d8uEEBeQr68v27ZtA+CZZ57Bzc2N\nv/3tb5XqaK3RWmMy1fw79wcffFDvc+67776mN/YCkZ5FRf6WFVE/Pw9vx8C8ofBmD/jxn5CbZvvn\nS49GiIvawYMHiYyMZNKkSXTp0oVTp04xbdq08lTjM2fOLK87YMAAtm3bRnFxMV5eXjz22GP06NGD\nfv36kZRknMbw5JNPlqcaHzBgAI899hi9e/emY8eO/P777wDk5ORw4403EhkZyfjx44mNjS0PZBeS\n9Cwq8u8Iyg52fwlt+sNVD8PR3+D3tyHuAwiKgoIsKMwBpcDJw0gVokxGeUE2FOWCLjVeKLBzADtH\ncHQFjyCjp+IWYHwGRv3Tu+D0Dsg8CZGj4coZEBxTV0uFaDm+fwxO77TuPVt3g5EvN+rSffv2MX/+\nfGJjjU3PL7/8Mj4+PhQXFzNkyBDGjx9PZGSlc97IyMhg0KBBvPzyyzz00EO8//77PPZY9cNDtdZs\n3LiRZcuWMXPmTH744QfefvttWrduzZIlS9i+fXulNOcXkgQLYOuxdHqEeGFycoOp34GLL/hfYXwY\nMwX6PwBrX4eME8YPekdXIxjkZ0J+hvG9kwe4+oODsxFwlAnQUFJovAqyjGW5B38yAkQ5Bb7tIaQX\nOHvDzi9g91fQZoARMDyCjQATPgCcm/+0LCFaunbt2pUHCoAFCxbw3nvvUVxczMmTJ9mzZ0+1YOHs\n7MzIkSMBI4V4WWrxqsaNG1depyzn07p16/j73/8OGDmlunRpniy8LT5YHEzK5qb//cHAK/x5489R\neLbpV71SQGe4cZ51Hqh1hWChwGQP9o7nPh/2DGz+ELZ+DBv+ZwQaADsn6HQ99JgI7YfKIU2i5Whk\nD8BWXF1dy78/cOAAb775Jhs3bsTLy4vJkyfXmGrc0fHc/3E7OzuKi4trvHdZqvG66jSXFj9n0c7f\nladHRfLr/mTG/Hsd8aerpwa2KqWMnomjKzi6VA4UYAxr9Z8B92+CJ5PgkUNwxwroeRskrIbPbjLm\nUza8Ywx7CSGaTWZmJu7u7nh4eHDq1ClWrFhh9Wf079+fRYsWAbBz5856D0mylRbfs1BKMaVfOJ0D\nPZj+6RZumPMbk/qEMaCDH30ifHF2tENrTUFxKXmFJeQXl1BQVIqjvQkfV0fMDjb8DV8pcPUzXmF9\nYfgLEL8c1v8Hvn8UVr8A/f4CV95vDH8JIS6omJgYIiMj6dSpE23atKmUatxa/vKXv3DrrbcSGRlZ\n/vL09LT6c+ojKcorSMrM5x9Ld/HL/mQKi0txtDNhdjCRU1hCSWnNf0/uTvZ4ujjg6miPs6MRODLz\nijibV0RuYTEmpTAphYOdwtvVER8XR3xcHfFzd8LfzQlvFweKSzX5RSVoDT1CvegV7lN+r1od3wTr\n3oD474yzOK55FrreaAQYIS5xkqL8nOLiYoqLizGbzRw4cIDhw4dz4MAB7O3P/3f9pqQob/E9i4oC\nPMzMvTWWvMISNh1J47dDKeQXluDqZI+rkz0ujnaYHexwsjdRUFxKanYBKdmFZOYVkVtYQk6hMcYY\n4u2Ml4sDLo72aK0p1VBQXEJ6ThFpOYUcSc0h7mg66bmF1BSrHewU0aHedAp0J8LPlXb+bkSHeeFu\ndjhXKbQXTPwMjqyDHx6HJXfClo/ghv+BZ/AF+hsTQthadnY2Q4cOpbi4GK0177zzTqMCRVNJz6IZ\nFZeUkplfjIOdwsnejqKSUjYdSeP3Q6lsOJxGQlI2WQVGALI3KXq28WZwxwD6tvWhS5AnjvaWKafS\nEiNQrHgS7OzhT7Oh67hm/JMJ0TTSs7CNi7ZnoZQaAbyJcazqPK11tWUNSqkJwDOABrZrrW+xlN8G\nPGmp9rzW+iNbtrU52NsZ8x5lHO1NDO4YwOCOAYCx5jolu5D9Z7JYeyCFX/Yn868fjBTqTvYmuod4\nclNsKONjQjDF3gERg+DLabD4djj0M1w/q/oEuhCXCK01SoZVraapHQOb9SyUUnbAfuAaIBHYBEys\neJa2UqoDsAi4WmudrpQK0FonKaV8gDggFiOIbAZ6aq3Ta3vepdizaIykzHw2H01n89F01h1MYd/p\nLHqEevHs6C5EhXpBSTGseQnWvmZsLJzwMbj6NnezhTgvhw8fxt3dHV9fXwkYVqC1JjU1laysLCIi\nIip91tCehS2DRT/gGa31tZb3jwNorV+qUOcVYL/Wel6VaycCg7XW91jevwOs0VovqO15LSVYVFRa\nqlm67QQvfb+P5KwCbowJ4dERHWnlYYadi2Hp/4FHINyy6Nx5HUJcAoqKikhMTKxxz4JoHLPZTEhI\nCA4ODpXKL4ZhqGDgeIX3iUCfKnWuAFBK/YYxVPWM1vqHWq6tNmurlJoGTAMICwuzWsMvFSaTYlxM\nCNdEtuLfPx/kg9+OsHznKe4d1I5pA8fiPLUNLLwFPvwT3PUTeLdp7iYL0SAODg7VfgMWzau5N+XZ\nAx2AwcBEYK5SqsE5LbTW72qtY7XWsf7+/jZq4sXP3ezA49d15qeHBjGkkz9v/LSfa2f/ysbidjD1\nWygpgM8mQN7Z5m6qEOISZctgcQIIrfA+xFJWUSKwTGtdpLU+jDHH0aGB14oqwnxd+M+kniyc1heA\nP7/7By9sLKHwxvmQeggWTYHihuXiF0KIimwZLDYBHZRSEUopR+BmYFmVOksxehUopfwwhqUSgBXA\ncKWUt1LKGxhuKRMN0LetL98/cBW39A5j7trD/OlbxalBr8LhX2H53+q/gRBCVGGzYKG1Lgbux/gh\nvxdYpLXerZSaqZQabam2AkhVSu0BVgOPaK1TtdZpwHMYAWcTMNNSJhrI1cmeF8Z248Pbe5GWU8Tg\nla3ZHTHV2I9x9Pfmbp4Q4hIjm/JagOSsAh7+Yjsb9yfym+vfcff2x/H/1krmWiFEg1dDNfcEt7gA\n/N2d+HBqL/52fRQzCyfhmLKbVZ+8TH6RnMwnhGgYCRYthMmkuOuqtvz9oUeJd46m56E5THpzORl5\nRc3dNCHEJUCCRQsT5O1Cx9v/i4cpnxszPuChz7dRWktGXSGEKCPBoiUK6Iyp113cbL+G+PjdzP5p\n/wV9/Ksr9vHllsQL+kwhRNNIsGip+j+AUoqXAn/lrZ8Psmz7SQqLS23+2MLiUuauPcwXcRIshLiU\nyHkWLZVnMKrbBAbsWcqA4JuYsWArAH5ujnQO9GDWhCj83Z2s/tj401kUFpeSkCJHwgpxKZGeRUt2\n5V9QRbnM67yNV27szl+HXcE1ka2IO5LOlPc2cDbX+ru9tyUaKUfOZBaQXXBxHUgvhKidBIuWrFUk\ndLgW85Z5TIjy44FhHXhpXHfm3hpLQkoOt72/kax8666W2n78XH6qIyk5Vr23EMJ2JFi0dP0fgNwU\n2PZZedGADn78d1IMu09mMnHuel75YR/z1ibw4+7TTV45tf34WUK8nQE4lCxDUUJcKiRYtHRtroTg\nWPj9LSg6d3bA0M6teHtiNKnZhbzzawLPf7eXaR9v5sXlexv9qKz8Ig4mZzOqRxBKQUKy9CyEuFTI\nBHdLpxRc/Q/4eKxxwt41z5Z/NLJbICO7BaK1JjOvmDd+2s+8dYdp7Wnmrqvanvejdp7IQGvoE+HD\nN9tPkiDDUEJcMiRYCGh3NURPMXoXnUdBSOU0MUopPF0ceOpPkSRl5fP8d3tp5WFmRNfWJGcVkJxV\nQF5RCUUlpZSUanq28cbd7FDtMduPZwDQI8SLtv5uJMgwlBCXDAkWwnDtC3BoNSydDvesBQdztSom\nk2LWhChSsjYyY6Gx1LamPJQeZnum9o/g9ivD8XZ1LC/ffvwsbXxd8HZ1pK2fK3FH0tBayxnLQlwC\nJFgIg9kTRr8Fn4yD1S/A8OdqruZgx9xbY5m7NgE7k6KVh5kAdydcHO1wsDeRX1TCJ+uP8taqA8xb\nm8CsCT0Y0TUQgB2JZ4kN9wGgrb8ruYUlnMksoLVn9cAkhLi4SLAQ57QfCjG3wR//hphbwa9DjdU8\nXRz427Uda73NVR38iT+dxSOLt/Po4h1Eh3mjgJMZ+fQINU7NbevnBkBCcrYECyEuAbIaSlR29T/B\nzgnWvt6k23Rs7c6bN0dTWFLKE1/uZJtlf0VUqCdg9CwADskktxCXBJsGC6XUCKVUvFLqoFLqsRo+\nn6qUSlZKbbO87qrwWUmF8qrHsQpbcfOHXnfCjkWQltCkW0X4ufLItZ1YtS+J13/cj51JERloBIvW\nHmacHewqTXK/88shXlsR36RnCiFsw2bBQillB8wBRgKRwESlVGQNVT/XWkdZXvMqlOdVKB9dw3XC\nVq78C5jsYe2sJt/q9ivD6RXuTfyZLDq2csfZ0Tidz2RShPu5ctjSs8jIK2L2Twd4b91hCorlUCYh\nLja27Fn0Bg5qrRO01oXAQmCMDZ8nrMW9NfScCtsXwNljTbqVyaR4dXwPzA4mYsO9K33W1t+1fGPe\n4s2J5BWVkFdUwtZjZ2u6lRCiGdkyWAQDxyu8T7SUVXWjUmqHUmqxUiq0QrlZKRWnlFqvlLrBhu0U\nNen/ACgTrHujybcK93PlxwcH8UiVSfF2fq4kpueWr6DqHOiBScFvB1Oa/EwhhHU19wT3N0C41ro7\nsBL4qMJnbSyHiN8CzFZKtat6sVJqmiWgxCUnJ1+YFrcUnsEQPRm2fgKndzX5dmG+LtU26rX1d6NU\nwyfrj3I4JYd7Brale4iXBAshLkK2DBYngIo9hRBLWTmtdarWusDydh7Qs8JnJyxfE4A1QHTVB2it\n39Vax2qtY/39/a3begGDHgMXX1hwM2QnWf32EX7Giqg3Vx3Az82Rkd1aM6C9H9sTM8i0crZbIUTT\n2DJYbAI6KKUilFKOwM1ApVVNSqnACm9HA3st5d5KKSfL935Af2CPDdsqauLeCiYugJwU+HwyFBfU\nf815KFs+m5VfzM29wnCyt6N/ez9KSjUbEtKs+iwhRNPYLFhorYuB+4EVGEFgkdZ6t1JqplKqbHXT\nDKXUbqXUdmAGMNVS3hmIs5SvBl7WWkuwaA5B0TD2v3B8A3zzQM35PRrJ3eyAv7sTJgW39AkDIKaN\nF2YHkwxFCXGRsekObq31cmB5lbKnKnz/OPB4Ddf9DnSzZdvEeegyFlIOGGlAnH2MPFJWyud0dccA\nTCZFkJdxxoWTvR29wn0kWAhxkZF0H6JhBj4Cuamwfg6g4doXrRIw/jW+e7WyAe39eOn7fZzJzKeV\nh6QCEeJi0NyrocSlQikY8TL0mQ7r/wM/PG7VIamK+rf3A2QJrRAXEwkWouGUghEvGQFjw39h07z6\nr2mEyEAPvF0cWHdAgoUQFwsZhhLnpyxgpOyHlU9B+2HgE2HVR5hMisEdA/hy6wl2nczg2i6tGRMV\nTPsAN6s+RwjRcNKzEOdPKePsC5M9fH0flJZa/REzx3Th6VGReLs4Mmf1Qa57ay1bjqVb/TlCiIaR\nYCEaxzPE6GEc/Q02vtv4+yTtg9RD1YrdzQ7c3j+Cz+/px++PDaW1h5lp8+NITM9tQqOFEI0lwUI0\nXtQk6DAcfnrG+KF/vk5th7lXw7xhdSYsbO1p5v2psRQUl3Lnh3Fkye5uIS44CRai8ZSCUW+Bkxt8\nOh4yTtR/TZmMRPh0Ajh7QWkJLJwERXm1Vm8f4M5/J/XkYHI29322lfwiSWMuxIUkwUI0jUcgTF4C\neWeN87tzG5CmIz8DPr0JinJh0mIY9y6c3gHfPFjnctwBHfx44Yau/Lo/mSnvbeBsbqEV/yCXl9Xx\nSXy97TyCtxD1kGAhmi6wB0z8zDhZ77M/w95vYMcXsPVTI69UVV/da6ymmjAfWkVCxxEw+AnYsRA2\nzq3zUTf3DuPtidFsP57B+P/9IXMYtfjv6kO8KqcOCiuSpbPCOiIGwo3z4IupRtLBMp3+BDd/eu79\nkd8gfjkMexbaDTlXPvAROPYHrHkJYm8Hu8rpzCsa1SMIf3cnps2P4/q31jGggx+xbbzp29aXzoEe\n1v+zXYKOp+dyJjOfwuJSHO3ld0LRdPKvSFhP5Bh4YDvcsxbuj4MBD8G+b+HYhnN11rwEbq2gzz2V\nrzWZoPc0yEuDhDX1PqpvW1+WTL+SwR392XbsLM9+s4eRb67lno/jOJ7WsnsbBcUlnM7Mp1TDibO1\nzwMJcT4kWAjr8gqDwO7g1wEG/s0IDCufMuYijqyDI2uh/4Pg4Fz92vZDwewJO79o0KM6tHLnzZuj\n+e2xq/n9sat55NqO/Lo/hWGzfmH2T/spLrH+/o9LwYn0vPKpn6OpOc3bGHHZkGAhbMfRFQY/BsfX\nG0NPa142gkfs7TXXt3cyeif7voPC8+sdBHk5c9+Q9vz8t0EM79Ka2T8d4H+/VN+/0RIcTz/XmzjW\nwntZwnokWAjbip4Cvu1h2QyjVzHgrzX3Ksp0HQ+F2bD/h0Y9LtDTmbcnRvOn7oG8ueoA+05nNrLh\nl66KAeJoqgQLYR0SLIRt2TnA0KchNwXcWkPPqXXXDx9g1Nu1pEmPnTmmKx5mB/72xXaKWthwVGJa\nLo52JtoHuEnPQliNBAthe51HQa+74LpX6u5VAJjsoOs4OPCjsXejkXxcHXn+hq7sOpHJOy1sOOp4\nei4h3s6E+7pyTHoWwkpsGiyUUiOUUvFKqYNKqcdq+HyqUipZKbXN8rqrwme3KaUOWF632bKdwsaU\ngutfN+YjGqLbeCgpNPZrNMHIboHlw1H7z2Q16V4NdTApm083HL0gz6rNsbRcQnxcaOPrwrG0XLSN\nzh0RLYvNgoVSyg6YA4wEIoGJSqnIGqp+rrWOsrzmWa71AZ4G+gC9gaeVUt62aqu4yATFgHcEbP0Y\nSoqbdKtnR3fB2cGOF5fvtVLj6va/Xw7xj692NWs6kuNpeYT5ONPG14W8ohKSswqarS3i8mHLnkVv\n4KDWOkFrXQgsBBr4qyXXAiu11mla63RgJTDCRu0UFxuloP8DcHwDLL23SQHD182J+4a0Z0188gU5\neW/jYSPdyamMfJs/qyYZeUVk5BUR6u1CmI8LAEdl3kJYgS2DRTBwvML7REtZVTcqpXYopRYrpULP\n51ql1DSlVJxSKi45Odla7RYXg9jbjYnxnV80OWDcdmU4wV7OvLh8L6Wl54ZkUrMLOJySw/G0XE5l\n5DV5uOZURl75hPKpZtoMV7YhMczHhTa+roCsiBLW0dzpPr4BFmitC5RS9wAfAVc39GKt9bvAuwCx\nsbEyMHu5ueohQMOqmWDnBDfMadRtzA52PHJtRx78fBtfbz/ByK6B/Pvng/zvl0MUVwgeM4Z24KFr\nrmh0c8t6FQAnm6lnUZYrK9THhWAvZ0wKjsnGPGEFtuxZnABCK7wPsZSV01qnaq3LBlTnAT0beq1o\nIa56GPrdD9s+qfGQpIYa3SOIrsEe/Ov7eK57ay3/Xn2Q0VFBzP5zFK/d1IPuIZ4s23aiSb2L9Qlp\nuDraAc3ZszCeG+rtgqO9iUBPZxmGElZhy2CxCeiglIpQSjkCNwPLKlZQSgVWeDsaKJuFXAEMV0p5\nWya2h1vKREvU735QJtj2WaNvYTIpnhjZmdOW5Hrz7+jNrAlR3BAdzPieIfy5VyhHUnOJb8KqqQ2H\nU+nT1hcfV8dm61kcS8vFw2yPp4uRiLGNr4sMQwmrsFmw0FoXA/dj/JDfCyzSWu9WSs1USo22VJuh\nlNqtlNoOzACmWq5NA57DCDibgJmWMtESeQRCu6th+8Imnfd9ZXs/vrl/AD/+dSADr/Cv9Nk1ka1Q\nCn7YdbpR907OKiAhOYfeET4Eepo5ndFMPYv0XEItE9tA+fJZIZrKpvsstNbLtdZXaK3baa1fsJQ9\npbVeZvn+ca11F611D631EK31vgrXvq+1bm95fWDLdopLQNQtkJkIR35t0m26hXji4lh9qi7A3Uxs\nG+9GB4uy+Yo+ET4Eejo322qoY2m5hHqfCxZhPq6k5RTKUbSiyWQHt7g0dLwenDybNBRVn2u7tGbf\n6SyOpJz/hPDGw6m4ONrRNdiTQE8zJ5thzqK0VJOYnkeYb+WeBciKKNF0EizEpcHBbKQB2bMM8m2T\nHPDaLq0BWLH7/HsXGw6n0bONNw52JgK9zGTmF5NT0LQNhecrObuAwuJSQr3PpVQp22shQ1GiqSRY\niEtH1CQozoM9X9vk9qE+LnQN9uCH8wwWZ3ML2Xc6i97hPgAEeRo/rE/ZYN6iuKSUOz/cxBdxx6t9\nVhYQQnykZyGsT4KFuHSExIJvB9j6Cdgo39HIroFsPXaW0+cx51A2X9E7wggWgZ5mwDa7uD/64yir\n9iWxOj6p2mcVN+SVcTc74OPqyLE02WshmqZBwUIp1U4p5WT5frBSaoZSysu2TROiCqWMnd3H18OX\nd0Oh9X8Alg1Ffb/rVIOvWbnnDI72JnqEGv8lgrwsPYuz1g0WSVn5zF65HzBOw6uqbI9FsFflzL5h\nPrJ8VjRdQ3sWS4ASpVR7jB3ToYDtZhqFqE2f6XD1P2HnYnhvOKQlWPX27QPc6BHqxasr4ivtyK7N\n+oRUvticyKQ+YZgdjA15rTyb0vNEAAAgAElEQVTMKAUnrTwM9fLyfRQUl9KvrW+NZ2sfS8ullYdT\neTvKhPm4cDxdgoVomoYGi1LLvomxwNta60eAwHquEcL6TCbjbO/JiyEjEd4dDAdWWvURc2/tSaCn\nmds/2EjckdoDRm5hMY8u3kEbXxceubZjebmjvQk/Nyer9iw2Hk7jy60nuHtgBFe28yUlu7BaZtvj\n6ZWXzZbxd3ciNbvQam0RLVNDg0WRUmoicBvwraXMwTZNEqIB2g+DaWvAMww+vQl+fbVJG/YqCnA3\ns+DuvrTyMHPb+xvZfDS9xnqv/BDPsbRcXrmxe7W9G4GeZqv2LJ5ZtpsgTzP3DWlPiI8xzFS1d3E0\nNac8eWBFPq6O5BaWNGvadHHpa2iwuB3oB7ygtT6slIoAPrZds4RoAJ8IuPNH47Ckn5+HRVOgyDo/\noAM8zCyY1hd/dyfu+mgTR6sk49uQkMqHvx9h6pXh9GnrW+36QE+z1Sa4M/KK2HMqkyn9wnFxtCfY\ny+g9JFaYt8gtLOZMZgERftV7Fj6ujgCk5UjvQjReg4KF1nqP1nqG1nqBJVeTu9b6XzZumxD1c3SB\ncXPh2pdg33ewcBIUW+ewn1YeZj68vTcauPOjODItu6DXHkjmro/iCPNx4dERHWu8NtDT+bxWVNWl\nLJNsuGUZbLBlH0XFSe4jKZY6ftV7Ft4uEixE0zV0NdQapZSH5QS7LcBcpdQs2zZNiAZSCvr9H4x+\nCw6tgi9uhxLrpLcI93Plv5N6ciQlh/s+3cIn648y9YNNBHs7s2Ba3xpThwAEeZnJLiguDzBNUbbK\nKcQyH9HK3Qk7k+LE2XOT1mU9n/AahqF83SRYiKZr6DCUp9Y6ExgHzNda9wGG2a5ZQjRCzK1w3WsQ\n/52xtLbUOmP0/dr58sLYrqw9kMKTS3cxoL0fX9zbr9oS1YoCPa23fLasZxFi6VHY25lo7WGu1LM4\nXBYs6uhZpOdKsBCN19DDj+wt6cQnAP+wYXuEaJred0NRLqx8yjjLu/8Mq9z2z73COJtbRFZ+MQ8O\n64C9Xd2/ZwV5GRvzTmbk0bG1e5OenZieh5uTPV4u59aUBHs7V5rgPpKSg5+bE25O1f9Ly5yFsIaG\nBouZGKnGf9Nab1JKtQUO2K5ZQjTBlTPg2HpY/SJ0/hP4tLXKbe8Z1K7BdVtbtWeRR4i3M0qp8rIQ\nL2fWJ6SWvz+Smls+p1GVp7MDJiXBQjRNQye4v9Bad9daT7e8T9Ba32jbpgnRSErB9a+DnQMsm2Gz\n1CB1aeXuhElZJz9UYnpu+RBUmWBvZ05n5lNUYiwXPpKSU+MQFICdSeHl4ijBQjRJQye4Q5RSXyml\nkiyvJUqpEFs3TohG8wiCa56FI2thy/wL/nh7OxMB7k1fPqu15kR6XvnkdplgL2dKNZzOyCenoJik\nrAIiagkWYAxFyZyFaIqGTnB/gHEkapDl9Y2lrE5KqRFKqXil1EGl1GN11LtRKaWVUrGW9+FKqTyl\n1DbL638NbKcQ58RMhTb94cd/GgEj9dAF7WUEepmb3LPIzCsmq6C4Ws+iLHicOJtXnvepppVQZXxc\nHGUXt2iShgYLf631B1rrYsvrQ8C/rguUUnbAHGAkEAlMVEpF1lDPHXgA2FDlo0Na6yjL694GtlOI\nc0wmGP02mD1g2V/g7RiYFQmHm3baXkMFeTo3ec7ieJWVUGUq7rU4YlkJ1aaWOQsAb1cH6VmIJmlo\nsEhVSk1WStlZXpOB1Hqu6Q0ctMxvFAILgTE11HsO+BfQPOdQisubbzt4cCfctxH+9AY4OMPiOyE7\n2eaPLkv5oZvQmzm3bLZyIChLg37i7LlgUducBYCPqxNpOXK0qmi8hgaLOzCWzZ4GTgHjgan1XBMM\nVDyhJdFSVk4pFQOEaq2/q+H6CKXUVqXUL0qpqxrYTiGqUwr8O0LsHTBhPuRnwLL7bT4k1cbPlfyi\nUrYcqzm3VEOUpfSo2rMwO9jh7+5EYnouR1Jy8HevedlsGR9Lz6K09MJP9ovLQ0NXQx3VWo/WWvtr\nrQO01jcATVoNpZQyAbOAh2v4+BQQprWOBh4CPlNKedRwj2lKqTilVFxysu1/UxSXgdZd4ZqZsP8H\n2DTPCBgntxnLbBPjrPqocdHBtPJw4pllexr9QzoxPQ93J3s8navn7Qz2MvZaHEmpfdlsGW8XR0pK\nNVn5F/aoV3H5aMpJeQ/V8/kJjHMvyoRYysq4A12BNUqpI0BfYJlSKlZrXaC1TgXQWm8GDgFXVH2A\n1vpdrXWs1jrW37/OKRQhzulzD7S/Bn58Et7uCe8Ogl/+BR+PhVPbrfYYVyd7nriuMztPZLCohmNQ\nGyIxPZfgKnssygR7O5fPWdQ1uQ0VUn7IvIVopKYEi+r/eivbBHRQSkUopRyBmzFWVAGgtc7QWvtp\nrcO11uHAemC01jpOKeVvmSDHsgGwA2DdU25Ey6UU3PAf8GoDniEw6k24dx2YPeHjcZBivf2mo3sE\nEdvGm1dWxJORd/5zBok1LJstE+LlTGJ6HklZBXXOV0DFZILWSbIoWp6mBIs6+9WWw5Lux9j5vRdY\npLXerZSaqZQaXc+9BwI7lFLbgMXAvVrr+o8tE6Kh3ALg/o1w2zLoORVad4MpS41AMv8G42AlK1BK\n8czoLqTnFvLainiOp+Wy91QmB5Oy671Wa12+e7smwd7OFFuGt+rrWZxL+SGT3KJx6kz3oZTKouag\noIDas6hZaK2XA8urlD1VS93BFb5fgnGUqxAXjl97mPwlfHg9fPc3uGWhVW7bNdiTib3D+Hj9UT5e\nf7S8/JFrO3LfkPa1XpeRV0R2DXssylRMZBhewzkWFZUFi3TZxS0aqc5gobVuWgY0IS41gd2h73T4\n5RVjE59vw/NB1eWJ6zoTGeiBk70JNyd7lu86zasr4nE323Nrv/Aarzm3EqqWYagK5Q3tWaRKsBCN\n1NBEgkK0HLF3wNpZsHEujHz5/K6N/8HYLX7Th2DvWF7s5mTP5L5tyt8Pi2xFflEJT329G1dHe27s\nWT17zvG0mjfklSnbmOfv7oRrHctmAZwd7HCyN8nGPNFoTZmzEOLy5N4auoyFrZ9AQdb5Xbvjc+M8\njQ11Z6hxsDPx9sRo+rf35ZHF29mQUH2Pa1nPIrSWnoWbZUltRD29CjDmTnxdJZmgaDwJFkLUpM+9\nUJgF2z47v+tObjG+/vIKZJ2ps6rZwY53p8QS5OXMP5buorC4tNLniem5uJvt8XSpvseizOgeQYzs\n1rpBTfOWYCGaQIKFEDUJ6QkhvWDDO1BaWn99gNw0SD8C0VOgOB9+nlnvJa5O9jw3pisHk7KZu7by\n6vC6ls2Wee6GrtzeP6JBzfORYCGaQIKFELXpcy+kHYKDPzWsflmvottN0Pde2PopnNhS72VDOgUw\noktr3v75QPk8BVDnstl6FeYaaU0qkDTloikkWAhRm8gx4B4En0+Cz26GbQvqnsMoCwxBUTDwUXD1\ngx8ea1AOqqdGRWJSiqeX7SYzv4iDSVk1HnrUYMv+Au8OhqJzKdK9XRxJkzTlopEkWAhRGzsHY9Ne\nr7vh9E5Yei+8PwKKa9kFfWIL+HYwdoKbPYyAcXwDJO2t91FBXs78ddgV/Lwvie7P/MiwWb+SU1hC\nO3+3xrX99E5IS4B1s8uLfFwdySoorjY3IkRDyNJZIeri1wFGvAjXvgC7v4TFd8Cvr8LVT1aup7Ux\nDNV28LmyjiPh+0cgYQ20qnaUSzW39w+nVGtMShHg4URrDzM923iff5tLS425E5M9rHsDevwZfNqW\n77U4m1tIgIf5/O8rWjTpWQjREEpB1xuhxy3GHoyqCQczT0L2GQiKOVfmFQo+7Yxg0QD2dibuGdSO\nuwe2ZUxUMH3a+mJv14j/olknoaQABjxk9I6+/ztoLRvzRJNIsBDifIx4EVz9Yel9UFzhh27Z5HZw\nTOX6bQfDkXWV69pa2mHja5srYfDjcOBHiF9enkxQUn6IxpBgIcT5cPY2Ttw7sxPWzTpXfmKLMezT\nulvl+u2GQFEOnLDuWRl1SrcEC58IIx27f2f4+XlJUy6aRIKFEOer03XQbYIxd3Fym1F2cgsERBrH\ntlYUfhUoU4OHoqwiLcEIXB4hxjBUx5GQsh9vs53xsfQsRCNIsBCiMUb+yxiO+upeY3nqya3Vh6AA\nnL2MeYxDqy9c29IOG2d12FnWr3iFQWkxXiUpxscSLEQjSLAQojFcfGDMvyF5L3x5t7EBLqiGYAHG\nvMWJzZU3ydny/O/0w8YQVBmvMAAcshLxdHaQOQvRKBIshGis9sOMDLV7vzHe19SzAGPeQpfAkd+M\n95vmwWsdIGmf9duktdGz8K4YLCzZbs8ew8fVUVZDiUaxabBQSo1QSsUrpQ4qpR6ro96NSimtlIqt\nUPa45bp4pdS1tmynEI12zXPGD2Z7Z2MiuSYhvcDBBRJWwx//ge8ehpxkiHvf+u3JTYOCTPBpe67M\n05L+/OwxvF0cJOWHaBSbbcqznKE9B7gGSAQ2KaWWaa33VKnnDjwAbKhQFolxZncXIAj4SSl1hda6\nxFbtFaJRnNxg8hJj6Meulv9O9k7Qpr+R8rwoFzqPApSRzvyameBgxQ1yFVdClXEwg1srOHsUH9er\nOXE2r9IlWmveWnWQxVuOM+eWGLqHeFmvPeKyYcueRW/goNY6QWtdCCwExtRQ7zngX0B+hbIxwEKt\ndYHW+jBw0HI/IS4+vu2MIam6tBtiBIquN8L4D4zhq/yzsO9b67YlzZK51rtKJlqvMDh7HB9XB5Iy\n80nNNlKW5BWWcP+Crbzx035SsgqZ8t5G9pzMtG6bxGXBlsEiGDhe4X2ipaycUioGCNVaf3e+1wpx\nSYm9EybMh7HvGstZIwaBZxhs/di6zynbkOfdpnK5VxicPUbH1h6k5hTS+8VVTHlvAze98zvLd57i\n8ZGdWPHgQFwc7Zj83gbiT5/noU/istdsE9xKKRMwC3i4CfeYppSKU0rFJScnW69xQlibg9nIYls2\nVGUyQfQkY/9F+lHrPSctATyCq+/38AqDjETu6BfK8hlXcc/AthxNzeVYai7vTonlnkHtCPN1YcHd\nfbE3KSbN28DOxIyanyFaJFsGixNAaIX3IZayMu5AV2CNUuoI0BdYZpnkru9aALTW72qtY7XWsf7+\n/lZuvhA2FjUJULDtU+vdM/1w9SEosOy1KEJlnyEyyINHR3Til0cGs+2p4VwT2aq8WrifK5/d3Rcn\nexM3vfM73+88Zb22iUuaLYPFJqCDUipCKeWIMWG9rOxDrXWG1tpPax2utQ4H1gOjtdZxlno3K6Wc\nlFIRQAdgow3bKsSF5xVqzGVs/RRKrbR2I+0w+ITX8CxjrwVnj5UXKaUwmVS1qu0D3Fh6X386B3ow\n/dMtvL3qAAXFsrakpbNZsNBaFwP3AyuAvcAirfVupdRMpdToeq7dDSwC9gA/APfJSihxWYqeApmJ\n8PX9xrLXpijIgpykystmy3hWDxZ18Xd3YsHdfbkhKojXV+4nZuZK7vtsC99sP0lJqQ03FIqLlk3P\ns9BaLweWVyl7qpa6g6u8fwF4wWaNE+JiEHkD9N8Ov78NB1caaUS6jDNSotekIAs+Gg2B3eG61ysv\n100/YnytcRjKMqqb0bBgAWB2sOONP0dxQ3QwK3afZuWeM3y34xQ7T2TwxHW17CkRly3ZwS1EczKZ\n4JpnYdpqY2J68R2w5M7aj2/97m9GHqrNH8KXd0FJ0bnP0mrYY1HGwRlcAxrcsyijlGJwxwBeGted\nDU8MY2LvMN79NYGf9505r/uIS58ECyEuBoE94K5Vxgl8u78yzs8+vatyne0LYcdCGPyYsXN891ew\n6LZzx7zWtseijGX5bGPZmRRPj4qkc6AHDy3azskqm/vE5U2ChRAXCzt7GPgI3PYtFGTDvKFGapAj\nv0HKAfj2IWMn+MBHoP8MuO41iP8O5vSG1S/C8Y3g7GNkuq1JE4MFGENTc26Jpqi4lL8s2EpRiZzn\n3VJIsBDiYhPeH+5dB51HGyulPrwO5vQBe0cYNxdMxrkU9L4bbl5gBIFfXjECR01DUGUsu7gpbdoP\n+Lb+brw4rhubj6Yz4Z0/Ku34zi0s5vudpy6rXse+05lsPtrExQeXAZtOcAshGsnNH26ca/Qw9v8A\n8d9D9GTwrJLIoNN1xivzFOz5Glp1qf2elr0WZJ8Gj6AmNW9MVDClWvP8t3sZ9e91TO4TRk5hCd/v\nPEVOYQmBnmY+n9aPMF+XJj3nYvD8t3s5lJzNH48Pbe6mNCsJFkJczJzcoNt441UXj0Doe2/ddcr3\nWhxvcrAAGBsdwpCOAbyyIp7564/i6mjPn7oH0a+dL898s5uJc9ezcFpfQn0u7YCx/0wWSVkFnDyb\nR5CXc/0XXKYkWAjRUlTcmBfWxzq3dHHkxbHdeGBoBzzMDjg7GkNk7QPcuGXuem6Zt54Xx3bD390J\nbxdH/N2cqm0EXLI5kW93nMTsYIfZwY72AW7cOSACs4OdVdrYFJn5RSRlGQsINh9Nl2AhhGgBPC17\nLc5aMReVRSuPymnWuwZ78sldfZg0bwNT3juXfCE6zIt5t8bi6+YEwLLtJ3n4i+2E+jhjtrcjr6iE\nr7ae4Mstibw+IYqo0OZNl34wKbv8+81H0xnVo+k9skuVBAshWgpHF+Pc8CauiGqo7iFe/PzwYA6c\nySI9t4jj6bm8sXI/4//3B/Pv6M2xtFweXrSN3hE+zL+jd3lP4tf9yfx9yQ7G/ec3/nJ1Bx4c1gFV\n2yZFGysLFkGeZrYeS2+WNlwsJFgI0ZJYYfns+fB3d8Lf3an8fa9wb+74MI6x//md/KIS2vq5MffW\n2EpDTgOv8GfFXwfy1NJdvLnqAG39XRkT1TwnFBxKysbRzsSoqCDeW3uYvMKS8qG2lkaWzgrRklzg\nYFFVzzY+LJneD0c7haezAx/d0RtPZ4dq9TzMDrw+IYqYMC/+uXQXpzPya7ib7R1MyibCz5U+ET4U\nl2p2JJ6t95qjqTk89fWuy24PigQLIVoSvyuMHFLWPEPjPLUPcOenhwfxw4NX0dqz9iNl7UyK1ydE\nUVSieWTxdrS+8AkMDyZn0z7AjehQbwA2N2AoasnmROb/cZQdl9l5IBIshGhJYm4zNvWtm9WszXBx\ntMfdXL1HUVWEnytPXN+ZtQdS+GTDhe0R5ReVcCwtl3YBbni7OtLW35UtR+sPFluPG72PhvRCLiUS\nLIRoSTyDjbToWz819ltcAib3CeOqDn489+0e3li5n9zC4ibd74PfDrPvdP3njCck56A1dAhwA6Bn\nmDebj6bX2cMpLdVsO1YWLKRnIYS4lA34q/H1t9nnyk5uNfJLFRc2T5vqoJRi9p+jGB7ZijdXHWDw\nq2tYFHe8znM1tNakZhdUK//9UArPfrOHx5bsrHdY62CysRKqfVmwaONNem4Rh1Nyar3mUHI2WQXF\nONqZ2C49CyHEJc0r1Dj/e8t8yDgB2z+H90fAL/+CuPer189v/t+Qfd2c+PctMSyZ3o8gL2ceXbyD\n699ayy/7k9Fao7XmcEoOizYd56HPt9H/5Z/p+fxPzFubUH4PrTWzftyPnUmx7fhZVscn1fnMg0nZ\nmJQxFAZGsABjv0Vttlp6Fdd3DyQhOYfM/KJa656vjLwi8oua7ww4CRZCtEQDHgJdCvNHw1fTIDgW\n2gyANS9VPrFv2wJ4OczIfltc/Tf1C61nGx+++r8reXtiNLmFJdz2/kZG/XsdvV5YxZDX1vDokh38\nsj+Z6DBvYtt488qK+PK9EmsPpBB3NJ0nr+9MqI8zs1bur7N3cSgpm1Afl/Jlve383fAw27Oljknu\nrcfT8XR2YHSUsXlvl5WGolbtPcOVL63imWW7rXK/xrBpsFBKjVBKxSulDiqlHqvh83uVUjuVUtuU\nUuuUUpGW8nClVJ6lfJtS6n+2bKcQLY53G+gxEVIPQu9pcOtSuO4VKMiEX1816pzeCd8+aOz83jQP\nPhh5UcxzKKUY1SOInx4axNOjIjEpxcAOfrw0rhsr/zqQuCeHMWdSDP+ZHIOLox0Pf7Gd4pJSXl+5\nn2AvZyb1acOMqzuw60QmP+6p/RCng0nZtPd3K39vMili2nizISGt1iCz9dhZosO86BFi7DzfcaJp\nwUJrzdxfE7hrfhy5RSWsiU9ullVhYMNgoZSyA+YAI4FIYGJZMKjgM611N611FPAKUHGJxiGtdZTl\nVU+GNCHEeRv5Ctz5E1z3Ktg5GBlro6fAxnfhxGb4fAo4e8Pdq2HCx8aZGu8MhOT45m45AI72Jm7v\nH8Gy+wcw689RTOwdRodW7uW7vQPczcwc05Xtx89y1/w4th8/y4yh7XG0NzE2OpgIP1feWLmf0hrm\nPopLSjmcklM+X1Hm2i6tSUjJYX1C9ZTl2QXFxJ/JIjrUGx9XR0J9nJu8IurZb/bwwvK9jOzamidG\nduZ0Zj7H0nKbdM/GsmXPojdwUGudoLUuBBYCYypW0FpXXJLgCshJ8EJcKI4uENqrctmQf4C9Gd4f\nCRnH4aaPjHTpkaNh2hooLYbVLzRHaxtlVPdAruvWmjXxyYT5uDAuJgQAezsTDwztwL7TWXy19US1\n646l5VJYUkq7KsFibHQwPq6OvLfucLVrdhw/i9ZG/isw0p1sP974nsWpjDw+/P0IE3uH8e+JMQzq\n6A/AhsPNc7aGLYNFMFCxz5poKatEKXWfUuoQRs9iRoWPIpRSW5VSvyilrqrpAUqpaUqpOKVUXHJy\nsjXbLkTL5N7KWC1VUgDDn6+cnda3HfS5B/Ysg6R9zdfG86CU4rkxXekV7s3ToyJxsDv3I29UjyB6\nhHjy6JIdzP/jSKXhnbJ5jg5VgoXZwY7JfcJYte9MtVVRZfsreliSH/YI8eTE2bwaV2U1xE+WIbI7\nB0RgMik6BLjh4+rIhhp6NRdCs09wa63naK3bAX8HnrQUnwLCtNbRwEPAZ0opjxqufVdrHau1jvX3\n979wjRbicjbgIbjnV+hTw+hvn+ng4AJrX7vw7WokXzcnvrj3SoZ2blWp3M6k+PTuvgzp6M9TX+/m\nH0vPpegoWzZbtWcBMLlfGxxMJj74rXLvYuuxdNoHuJWnL+kWbJm3qDDJnZHX8NVRK3afoa2/a/lQ\nmFKK3uE+bDic2uB7WJMtg8UJILTC+xBLWW0WAjcAaK0LtNaplu83A4eAK2zUTiFERSYTBPaAmjK9\nuvpCrzth1xJIPVT986R98Plk+HgcfPMgrJ0FadWHbC4Wbk72vDMllumD2/HZhmP0eXEVd320iW+2\nn6KVhxMeNewyD3A3MzoqiC/iEjmba+xL0Vobk9sVUqp3C/FEqXPBYt7aBKJm/sgPu05Xu2fVeZOM\n3CLWJ6RybZfWlcp7R/iQmJ7HiQrH1mYXFNc472JttgwWm4AOSqkIpZQjcDOwrGIFpVSHCm+vBw5Y\nyv0tE+QopdoCHYAEhBDN78q/gJ0TrH39XFlBNvz4T/hffzj8K+Smwt5lsOpZ+PB6yK57T0NzsjMp\n/j6iE+/dFsvVnQJISMlh76nMOs/SuHNABHlFJXy20UhBcjwtj9ScQqLDvMvruDnZ087fjR2JZ/ly\nSyLPf7cXk1K8uHwvBcXn9kscTMqiz0urWLw5sbxsdXwSxaWa4ZGVe0N92voAsLFC7+LvS3Zwy7z1\nNl8lZbMU5VrrYqXU/cAKwA54X2u9Wyk1E4jTWi8D7ldKDQOKgHTgNsvlA4GZSqkioBS4V2stJ6YL\ncTFwC4CeU41VU2D0MJL2QkEGRE2Ga54FVz/js5PbjA1/i26FW5eBvWOzNbs+Qzu3Kh+qysgtwuxY\n++/SnQM96N/elzdW7ueLuETsLaf/lU1ul+ke4skPu07zy/5krmznyx39I7hrfhzzfz/K3QPbUlhc\nygMLt5GcVcDz3+1haKcAvF0dWbH7NAHuTuVLcMt0au2Bh9mejYfTGBsdwvqEVL7bceqCnPlh0zkL\nrfVyrfUVWut2WusXLGVPWQIFWusHtNZdLMtjh2itd1vKl1Qoj9Faf2PLdgohzlP/GWD2hIM/GYkJ\nu4yBO36EG+acCxQAQVEw+m049gesePxceUH2RbHJrzaeLg442dd9bsW/buzOlL7hRAZ54O3iyLDO\nAVzRyr1SnR4hXuQWltAp0J13pvRkWGQrBnf0562fD5CWU8islfvZfTKTR67tSFZ+Ma/+GE9+UQm/\n7E9meJdW1Y6gtTMpekf4sCEhjZJSzbPf7CHYy5l7Braz+t9BVXL4kRDi/HkEwaMJNc9rVNX9Jji9\nHX5/25jTOHsMMo5B2JVwx/e2b6uNhHi78NSoqlvHKhvRtTXxZ7J46JoryrPsPnFdZ0bM/pX7Pt3C\n+sOpTOwdyn1D2pOaXcgHvx+mlbuZ3MIShke2rvGevSN8+GlvEm+tOsDeU5n8+5boC3IgU7OvhhJC\nXKLOZ9hj6DMQNQny0iG0N0SOgWO/Q2KczZp3MWjlYebFsd3wczt3WuAVrdyZ2DuMPxJSCfd15Z9/\nMgLOg9d0wNfVkTd+2o+72Z6+bX1rvGefCKP8zVUH6BPhw/XdAm3/B0F6FkKIC8HOHm74z7n3BVlw\n8GfY8A6ExDZfu5rJQ9dcwdncIqYPboeLo/Fj2MPswKMjOvHo4h1c3SkAR/uaf5fvEuSBq6MdeUUl\nPD2qywU7n1yChRDiwnNyh+jJRs6p4c+Be81DLpcrXzcn5kyKqVY+PiaExLRcRnStvbdgb2fijgER\nONmbiAyqtv3MZlRzJaWyttjYWB0Xd3l3aYW4rKQegrd7wqBHYcgTNX++dxm06gbBMeDic+Hb2AIo\npTZrrevt3knPQgjRPHzbQYfhxhkaVz0M9ufG9dEalk6H4xvOlYX0glu/BkfXC99WIRPcQohm1Pde\nyEmG3V9VLt/9pREoRr4Ct31rpCBJ3AR7vm6edgoJFkKIZtR2CPh1NM7QyLYkAy3Kh5XPGMNPve6C\niKtg6FPg29443U80C8qz9LQAAA89SURBVAkWQojmoxRc/5pxvOsHIyAjEdb/x9iHce0Lxoa/snrR\nU4zNfcn7m7fN1pCbBu8MMs4+v0RIsBBCNK+IgcZJfdlJRmqQtbOg43XQdlDlej0mgsketlboXaQf\nhW8fMjb6XQjJ8XDgp6bfJzEOTm2D9f/9//buPbqq8szj+PdHSCCCchFIEy6CFdRQIFC0qB11RCqU\nDjpjp6LYIsVSKVYcpxc701VXndapzpRWKu2SUdRpVbzgtNQitkWkQx1QGAPIrSJShEYDXlAE5ZJn\n/nj3mewkJxxC2MnJOc9nrbPO2e/Ze593r5eVh/fe/Hu1EA8WzrnW128UXPMkHNwPh/bDmH9peM6J\nJTBobNgX/NAB+OBdeOgKWHVv2Kwp3Sq4x9uSW+GxyVBT07z7VG8I7xsWhudoAzxYOOeyQ+mwsBvf\nNYugx2npzxkxGfbthk1PwoKpsPtPMPb2EGDmjYU31iebx6o1cGBv2Lu8OXZtgnaFId/1O/ezlAcL\n51z26Nq37u589Z02Gk4sg19dDy//NuwfPuq6EGDaFYTl0JOqYbz/ZthqFkLQaI7qDaHjvscgqHyo\n+XlrAR4snHNtR7uCMPP74Ptw9pfDRkwAvc6AKYtC81SmPcI/2BPmcTRVVWX6z01Vczh00vcqh4qr\n4LUVsLuZNZUW4MHCOde2fPJG+Lt74JLb6qZ3PxU+MQ1eegLe2JD+2uqNMKscfn9L0383VZs4eWDY\np+NYvb0tND/1PAOGTgS1gzXZX7vwYOGca1uKOoVlzwvSLEBx7g1Q1Bmeva3hdwf2wWPXhD6H5+8J\nK+A2RVUldOsfRm9VrTn2Tu5dm8J7r3I4qRQ+Ojp02tccPvJ1rSzRYCFprKTNkrZIujnN99dJWiep\nUtJySeWx774VXbdZ0iVJ5tM5lyNO6A7nfAU2/rphv8JT3whDXz/1vdCMtWpe0+5dtSZ0wpdVwIH3\n4K1GdnquqTlyv0lqJFTP08P78Enw3l9g67NNy08LSyxYRHtozwHGAeXAlfFgEHnIzIaYWQVwBzAr\nuracsGf3YGAs8NPUntzOOXdEo74SdvFbGqtdrH0UXvw5/NVNYQ/xj44Oy6Mf7W59+98OzUelFeEF\njfdbPH833DUybPSUTvUm6NoPOnQOx4PGhfkj25YfXV5aSZI1i7OBLWa21cwOAPOBS+MnmFl8gHEn\nINXrdCkw38w+NLNXgS3R/Zxz7siKu4aA8KfFMHsE3NYHnvhS2Jnvwmh123O/CnvfgLWPHN09q9aG\n99Jh0OtMKOiQPliYhWXXrQYqH0x/r+qN0PPM2uPCjuGezek0bwFJrjrbG3gtdrwDaDAmTtIM4Cag\nCLgodu2Ketf2Tiabzrmc84nptXMuOpeEbWCHf762n+PUC+EjQ+C5u6AiGl31ytIwlLXXGQ3vl2rS\nKq2AgkIoGZy+k3vb8jAHo2OXEIhG31K3b+XwwTA3ZOCYuteVVsCm34Rg00KbGTVVqy9RbmZzgDmS\nrgK+DUw+2mslTQOmAfTr1y+ZDDrn2p4OneHv72/8ewnOnQlPXAv3jQtrNB3+MASW65ZD5151z6+q\nhC59oVO01WlZBaxb0PCP++r7QqD49A/DvV95BgZ9qvb7t7ZCzcFQk4grqwjNZHteC01UWSjJZqid\nQN/YcZ8orTHzgcuacq2ZzTWzkWY2smfPns3MrnMurwy+LKx4++5fwnyNy+8NS28smNpwZFKqczul\ndBh8uKduJ/f7u0PH+rArwx7jxd0bDolNdW7XDxalw8N7c4bkJizJYPECMFDSAElFhA7rhfETJA2M\nHY4HXo4+LwQmSuogaQAwEHg+wbw65/JNQSHMWAk3roWx/wpDPhtmhL/6B1h2e+15H7wbmpZSHduQ\nvpO78iE4fAA+fg20L4KhnwtNS/EhutWbwryKHoPq5qVkcOjkzuJVaBMLFmZ2CLgeeBrYCDxqZusl\n3SppQnTa9ZLWS6ok9FtMjq5dDzwKbAAWAzPMLLsHITvn2h6pbjPS8KtDzWDZHbD5qZD2+rrwHq9Z\n9CqHgqLavgwzWH0/9B1VW2uouCoEj5cW1F5XvQG6DYDC4rr5KOwYOr2zuJM70T4LM1sELKqX9p3Y\n55lHuPb7QIZ5+845dxxJMP6HIQg8PBEGXAAnRWNrymI1i/ZFIWBsXwlbfg/b/ghvvQLnf732nI8M\nhZKPhRrHWdeGtF2bGjZBpZQNg02LsraT22dwO+dcXFEn+OLTMObW8Md9zUNwYmnDTu+y4WFdp19c\nDstnQdmI0A+SIoXaxc7VMH9SqKm8+UrjwaK0Ava/VbtYYZZp9dFQzjmXdTqeBOfNDIsVrnsMOvVo\neM75X4PSoWGtqJLBYfZ4fSOnhvkcLz4YllWHsCZUOmWxTu4sHBHlwcI55xpT2BFGfD79d136wMgv\nZr5+zK3w19+Gzb8J8zDqz7FISXVyV1VC+YT057QiDxbOOZe09kUw+G/DqzGFxaGTO93wWbPQUd5j\nYN2O9hbkfRbOOZctyoaFmkV8v439b8MjV4f5Hwu+1PwtXY+RBwvnnMsWpRWw703YsyMEjO0r4e4L\nwjpXZ/4N7N4cdghsBd4M5Zxz2SLVyf3rG8JEwHe2w0l9YMriMHT3zgr4451w+tgWz5rXLJxzLluU\nDA5rS21fGeZojJ8F05dD37PCjPNzZsD252DHqhbPmtcsnHMuWxQWw8w1UHgCtO/Q8PsRX4BlPwi1\niyt+3qJZ85qFc85lk+Ju6QMFhNV0z7o2LFh4pN34EuA1C+eca0vO/jI89xOYd0mogdQcCk1Wkx5N\n9Gc9WDjnXFtyYklYHXfrstCP0a4QTj418Z/1YOGcc23Nx68JrxbkfRbOOecy8mDhnHMuIw8Wzjnn\nMvJg4ZxzLqNEg4WksZI2S9oi6eY0398kaYOktZKWSDol9t1hSZXRa2H9a51zzrWcxEZDSSoA5gBj\ngB3AC5IWmtmG2GkvAiPNbJ+k6cAdwBXRd/vNrALnnHOtLsmaxdnAFjPbamYHgPnApfETzGypme2L\nDlcAfRLMj3POuWOUZLDoDcQ3k90RpTVmKvBU7LijpFWSVki6LN0FkqZF56zatWtX83PsnHMurayY\nlCfpamAkcEEs+RQz2ynpVOAZSevMrM5iKGY2F5gb3WOXpD83Ixs9gN3NuL4tysdnhvx87nx8ZsjP\n527qM5+S+ZRkg8VOoG/suE+UVoeki4F/Bi4wsw9T6Wa2M3rfKulZYDjQ6MpZZtazOZmVtMrMRjbn\nHm1NPj4z5Odz5+MzQ34+d1LPnGQz1AvAQEkDJBUBE4E6o5okDQfuBiaYWXUsvZukDtHnHsB5QLxj\n3DnnXAtKrGZhZockXQ88DRQA88xsvaRbgVVmthD4N6Az8JgkgO1mNgE4E7hbUg0hoP2g3igq55xz\nLSjRPgszWwQsqpf2ndjnixu57jlgSJJ5S2NuC/9eNsjHZ4b8fO58fGbIz+dO5JllZknc1znnXA7x\n5T6cc85llPfBItOSJLlCUl9JS6PlVdZLmhmld5f0O0kvR+/dWjuvx5ukAkkvSnoyOh4gaWVU5o9E\nAzByiqSukh6XtEnSRknn5HpZS/qH6N/2S5IeltQxF8ta0jxJ1ZJeiqWlLVsFs6PnXytpxLH+bl4H\ni9iSJOOAcuBKSeWtm6vEHAL+0czKgVHAjOhZbwaWmNlAYEl0nGtmAhtjx7cDPzKz04C3CRNCc82d\nwGIzOwMYRnj+nC1rSb2BGwjLB32MMKhmIrlZ1vcDY+ulNVa244CB0Wsa8LNj/dG8DhYcxZIkucLM\nqszsf6PP7xH+ePQmPO8D0WkPAGlny7dVkvoA44F7omMBFwGPR6fk4jN3Ac4H7gUwswNm9g45XtaE\nATvFktoDJwBV5GBZm9kfgLfqJTdWtpcC/2nBCqCrpNJj+d18DxZNXZIkJ0jqT5jkuBIoMbOq6KvX\ngZJWylZSfgx8A6iJjk8G3jGzQ9FxLpb5AGAXcF/U/HaPpE7kcFlHk3j/HdhOCBJ7gNXkflmnNFa2\nx+1vXL4Hi7wjqTOwALjRzN6Nf2dhaFzODI+T9Bmg2sxWt3ZeWlh7YATwMzMbDrxPvSanHCzrboT/\nRQ8AyoBONGyqyQtJlW2+B4ujWpIkV0gqJASKB83siSj5jVS1NHqvbuz6Nug8YIKkbYQmxosIbfld\no6YKyM0y3wHsMLOV0fHjhOCRy2V9MfCqme0ys4PAE4Tyz/WyTmmsbI/b37h8DxYZlyTJFVFb/b3A\nRjObFftqITA5+jwZ+FVL5y0pZvYtM+tjZv0JZfuMmU0ClgKfjU7LqWcGMLPXgdcknR4ljSYsl5Oz\nZU1ofhol6YTo33rqmXO6rGMaK9uFwBeiUVGjgD2x5qomyftJeZI+TWjXTi1J8v1WzlIiJH0S+G9g\nHbXt9/9E6Ld4FOgH/Bn4nJnV7zxr8yRdCHzNzD4TrWQ8H+hO2IDr6vgilrlAUgWhU78I2ApMIfzn\nMGfLWtJ3CZunHSKU67WE9vmcKmtJDwMXElaXfQO4Bfglaco2Cpx3EZrk9gFTzGzVMf1uvgcL55xz\nmeV7M5Rzzrmj4MHCOedcRh4snHPOZeTBwjnnXEYeLJxzzmXkwcK5DCQdllQZex23Bfgk9Y+vHupc\ntkp0pzzncsR+M6to7Uw415q8ZuHcMZK0TdIdktZJel7SaVF6f0nPRPsHLJHUL0ovkfRfktZEr3Oj\nWxVI+o9oL4bfSiqOzr9BYf+RtZLmt9JjOgd4sHDuaBTXa4a6IvbdHjMbQpgl++Mo7SfAA2Y2FHgQ\nmB2lzwaWmdkwwlpN66P0gcAcMxsMvANcHqXfDAyP7nNdUg/n3NHwGdzOZSBpr5l1TpO+DbjIzLZG\nizS+bmYnS9oNlJrZwSi9ysx6SNoF9IkvNxEtF/+7aNMaJH0TKDSz70laDOwlLOXwSzPbm/CjOtco\nr1k41zzWyOemiK9VdJjavsTxhJ0cRwAvxFZPda7FebBwrnmuiL3/T/T5OcIqtwCTCAs4Qtjucjr8\n/77gXRq7qaR2QF8zWwp8E+gCNKjdONdS/H8qzmVWLKkydrzYzFLDZ7tJWkuoHVwZpX2VsEvd1wk7\n1k2J0mcCcyVNJdQgphN2dUunAPhFFFAEzI62RnWuVXifhXPHKOqzGGlmu1s7L84lzZuhnHPOZeQ1\nC+eccxl5zcI551xGHiycc85l5MHCOedcRh4snHPOZeTBwjnnXEYeLJxzzmX0fxbRzCQ1jYY9AAAA\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4fb45a80f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('./%s/hist_0.json' % path_info['model_info']['model_dir'], 'r') as f:\n",
    "    history = json.load(f)\n",
    "    \n",
    "plt.plot(history['val_loss'], label='Validation')\n",
    "plt.plot(history['loss'], label='Training')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test evauation and train evauation is the numpy array of the shape (number of fold, number of evaluation measures)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32086846, 0.84799999, 0.86390531, 0.81481481, 0.83900112,\n",
       "        0.92848271],\n",
       "       [0.37076306, 0.824     , 0.84482759, 0.77631581, 0.80984753,\n",
       "        0.88210827],\n",
       "       [0.33463037, 0.852     , 0.83146065, 0.90277779, 0.86638576,\n",
       "        0.93028247]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.28811851, 0.88266665, 0.88030887, 0.88793105, 0.88411176,\n",
       "        0.94885004],\n",
       "       [0.31326854, 0.84933335, 0.84249085, 0.86764705, 0.85497642,\n",
       "        0.90064549],\n",
       "       [0.19089425, 0.93333334, 0.92172211, 0.95815897, 0.93976396,\n",
       "        0.9826659 ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load the pretrained network for training\n",
    "\n",
    "If you have pre-trianed model, you can use the pre-trained weight for next training. For using pre-trained weights, you have to use `warm_start` option in `training_inro` with addding the file path of the pre-trained weights in the `warm_start_model` option. Below is the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network_info = {\n",
    "    'architecture_info': {\n",
    "        'batch_normalization': 'False',\n",
    "        'drop_out': '0',\n",
    "        'weight_initial': 'glorot_uniform',\n",
    "        'weight_l1_penalty':'0.01',\n",
    "        'weight_decay': 'phylogenetic_tree',\n",
    "    },\n",
    "    'model_info': {\n",
    "        'decay': '0.001',\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'lr': '0.01',\n",
    "        'metrics': 'binary_accuracy, sensitivity, specificity, gmeasure, auc',\n",
    "        'network_class': 'DeepBiomeNetwork',\n",
    "        'normalizer': 'normalize_minmax',\n",
    "        'optimizer': 'adam',\n",
    "        'reader_class': 'MicroBiomeClassificationReader',\n",
    "        'texa_selection_metrics': 'accuracy, sensitivity, specificity, gmeasure'\n",
    "    },\n",
    "    'training_info': {\n",
    "        'warm_start':'True',\n",
    "        'warm_start_model':'./example_result/weight.h5',\n",
    "        'batch_size': '200',\n",
    "        'epochs': '100'\n",
    "    },\n",
    "    'validation_info': {\n",
    "        'batch_size': 'None', \n",
    "        'validation_size': '0.2'\n",
    "    },\n",
    "    'test_info': {\n",
    "        'batch_size': 'None'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|deepbiome.py:100] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:137] -------1 simulation start!----------------------------------\n",
      "[root    |INFO|readers.py:58] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:59] Construct Dataset\n",
      "[root    |INFO|readers.py:60] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:61] Load data\n",
      "[root    |INFO|deepbiome.py:147] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:148] Build network for 1 simulation\n",
      "[root    |INFO|build_network.py:513] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:514] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv\n",
      "[root    |INFO|build_network.py:518] Phylogenetic tree level list: ['Genus', 'Family', 'Order', 'Class', 'Phylum']\n",
      "[root    |INFO|build_network.py:519] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:526]      Genus: 48\n",
      "[root    |INFO|build_network.py:526]     Family: 40\n",
      "[root    |INFO|build_network.py:526]      Order: 23\n",
      "[root    |INFO|build_network.py:526]      Class: 17\n",
      "[root    |INFO|build_network.py:526]     Phylum: 9\n",
      "[root    |INFO|build_network.py:529] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:530] Phylogenetic_tree_dict info: ['Genus', 'Number', 'Order', 'Class', 'Phylum', 'Family']\n",
      "[root    |INFO|build_network.py:531] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [ Genus, Family]\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [Family,  Order]\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [ Order,  Class]\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [ Class, Phylum]\n",
      "[root    |INFO|build_network.py:554] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:570] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:571] Build network based on phylogenetic tree information\n",
      "[root    |INFO|build_network.py:572] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:648] ------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "l1_dense (Dense_with_tree)   (None, 40)                1960      \n",
      "_________________________________________________________________\n",
      "l1_activation (Activation)   (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "l2_dense (Dense_with_tree)   (None, 23)                943       \n",
      "_________________________________________________________________\n",
      "l2_activation (Activation)   (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "l3_dense (Dense_with_tree)   (None, 17)                408       \n",
      "_________________________________________________________________\n",
      "l3_activation (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "l4_dense (Dense_with_tree)   (None, 9)                 162       \n",
      "_________________________________________________________________\n",
      "l4_activation (Activation)   (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "last_dense_h (Dense)         (None, 1)                 10        \n",
      "_________________________________________________________________\n",
      "p_hat (Activation)           (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,483\n",
      "Trainable params: 3,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:57] Build Network\n",
      "[root    |INFO|build_network.py:58] Optimizer = adam\n",
      "[root    |INFO|build_network.py:59] Loss = binary_crossentropy\n",
      "[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc\n",
      "[root    |INFO|build_network.py:83] Load trained model weight at ./example_result/weight_0.h5 \n",
      "[root    |INFO|deepbiome.py:157] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:158] 1 fold computing start!----------------------------------\n",
      "[root    |INFO|build_network.py:141] Training start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 150 samples\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 1s 890us/step - loss: 0.2947 - binary_accuracy: 0.8783 - sensitivity: 0.9535 - specificity: 0.6973 - gmeasure: 0.8042 - auc: 0.9525 - val_loss: 0.3781 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.7905 - val_specificity: 0.9556 - val_gmeasure: 0.8691 - val_auc: 0.9179\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.2898 - binary_accuracy: 0.8800 - sensitivity: 0.8689 - specificity: 0.9034 - gmeasure: 0.8838 - auc: 0.9554 - val_loss: 0.3385 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.9333 - val_specificity: 0.6000 - val_gmeasure: 0.7483 - val_auc: 0.9189\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.2723 - binary_accuracy: 0.8800 - sensitivity: 0.9597 - specificity: 0.7100 - gmeasure: 0.8240 - auc: 0.9572 - val_loss: 0.3262 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8667 - val_specificity: 0.8444 - val_gmeasure: 0.8555 - val_auc: 0.9183\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.2589 - binary_accuracy: 0.8900 - sensitivity: 0.9235 - specificity: 0.8214 - gmeasure: 0.8695 - auc: 0.9561 - val_loss: 0.3290 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8571 - val_specificity: 0.8667 - val_gmeasure: 0.8619 - val_auc: 0.9192\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.2574 - binary_accuracy: 0.8917 - sensitivity: 0.9194 - specificity: 0.8275 - gmeasure: 0.8720 - auc: 0.9556 - val_loss: 0.3257 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.8667 - val_specificity: 0.8000 - val_gmeasure: 0.8327 - val_auc: 0.9192\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.2590 - binary_accuracy: 0.8900 - sensitivity: 0.9396 - specificity: 0.7837 - gmeasure: 0.8578 - auc: 0.9567 - val_loss: 0.3251 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.8667 - val_specificity: 0.8222 - val_gmeasure: 0.8442 - val_auc: 0.9200\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 0s 58us/step - loss: 0.2620 - binary_accuracy: 0.8933 - sensitivity: 0.9143 - specificity: 0.8529 - gmeasure: 0.8825 - auc: 0.9595 - val_loss: 0.3245 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8667 - val_specificity: 0.8667 - val_gmeasure: 0.8667 - val_auc: 0.9208\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 0s 58us/step - loss: 0.2560 - binary_accuracy: 0.8850 - sensitivity: 0.9344 - specificity: 0.7757 - gmeasure: 0.8506 - auc: 0.9565 - val_loss: 0.3236 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.8952 - val_specificity: 0.7333 - val_gmeasure: 0.8103 - val_auc: 0.9208\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.2623 - binary_accuracy: 0.8900 - sensitivity: 0.9107 - specificity: 0.8452 - gmeasure: 0.8756 - auc: 0.9585 - val_loss: 0.3237 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8571 - val_specificity: 0.8667 - val_gmeasure: 0.8619 - val_auc: 0.9219\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 0s 59us/step - loss: 0.2645 - binary_accuracy: 0.8967 - sensitivity: 0.9440 - specificity: 0.8021 - gmeasure: 0.8693 - auc: 0.9581 - val_loss: 0.3224 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.8952 - val_specificity: 0.7333 - val_gmeasure: 0.8103 - val_auc: 0.9221\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.2553 - binary_accuracy: 0.8900 - sensitivity: 0.9188 - specificity: 0.8281 - gmeasure: 0.8722 - auc: 0.9564 - val_loss: 0.3275 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8571 - val_specificity: 0.8889 - val_gmeasure: 0.8729 - val_auc: 0.9232\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 0s 59us/step - loss: 0.2537 - binary_accuracy: 0.8950 - sensitivity: 0.9304 - specificity: 0.8190 - gmeasure: 0.8728 - auc: 0.9574 - val_loss: 0.3235 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.9143 - val_specificity: 0.7111 - val_gmeasure: 0.8063 - val_auc: 0.9219\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.2554 - binary_accuracy: 0.8867 - sensitivity: 0.9339 - specificity: 0.7710 - gmeasure: 0.8455 - auc: 0.9575 - val_loss: 0.3226 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8571 - val_specificity: 0.8667 - val_gmeasure: 0.8619 - val_auc: 0.9234\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2489 - binary_accuracy: 0.8983 - sensitivity: 0.9184 - specificity: 0.8593 - gmeasure: 0.8879 - auc: 0.9619 - val_loss: 0.3192 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.8857 - val_specificity: 0.7778 - val_gmeasure: 0.8300 - val_auc: 0.9223\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 0s 69us/step - loss: 0.2565 - binary_accuracy: 0.8817 - sensitivity: 0.9493 - specificity: 0.7331 - gmeasure: 0.8341 - auc: 0.9579 - val_loss: 0.3177 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.8667 - val_specificity: 0.8222 - val_gmeasure: 0.8442 - val_auc: 0.9228\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 0s 58us/step - loss: 0.2518 - binary_accuracy: 0.8850 - sensitivity: 0.9018 - specificity: 0.8424 - gmeasure: 0.8706 - auc: 0.9602 - val_loss: 0.3224 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8571 - val_specificity: 0.8889 - val_gmeasure: 0.8729 - val_auc: 0.9236\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.2477 - binary_accuracy: 0.8950 - sensitivity: 0.9366 - specificity: 0.8002 - gmeasure: 0.8644 - auc: 0.9587 - val_loss: 0.3201 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.9143 - val_specificity: 0.7556 - val_gmeasure: 0.8311 - val_auc: 0.9228\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.2439 - binary_accuracy: 0.9000 - sensitivity: 0.9495 - specificity: 0.7890 - gmeasure: 0.8655 - auc: 0.9621 - val_loss: 0.3266 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8571 - val_specificity: 0.8889 - val_gmeasure: 0.8729 - val_auc: 0.9244\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.2499 - binary_accuracy: 0.8950 - sensitivity: 0.9055 - specificity: 0.8746 - gmeasure: 0.8896 - auc: 0.9608 - val_loss: 0.3169 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.8857 - val_specificity: 0.7778 - val_gmeasure: 0.8300 - val_auc: 0.9238\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.2530 - binary_accuracy: 0.8850 - sensitivity: 0.9520 - specificity: 0.7410 - gmeasure: 0.8384 - auc: 0.9618 - val_loss: 0.3162 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8667 - val_specificity: 0.8444 - val_gmeasure: 0.8555 - val_auc: 0.9242\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.2446 - binary_accuracy: 0.8983 - sensitivity: 0.9128 - specificity: 0.8665 - gmeasure: 0.8893 - auc: 0.9619 - val_loss: 0.3169 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8571 - val_specificity: 0.8667 - val_gmeasure: 0.8619 - val_auc: 0.9255\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 0s 60us/step - loss: 0.2419 - binary_accuracy: 0.8917 - sensitivity: 0.9348 - specificity: 0.7949 - gmeasure: 0.8616 - auc: 0.9610 - val_loss: 0.3161 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.9048 - val_specificity: 0.7556 - val_gmeasure: 0.8268 - val_auc: 0.9247\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.2427 - binary_accuracy: 0.8983 - sensitivity: 0.9274 - specificity: 0.8346 - gmeasure: 0.8789 - auc: 0.9624 - val_loss: 0.3185 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8571 - val_specificity: 0.8889 - val_gmeasure: 0.8729 - val_auc: 0.9259\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.2368 - binary_accuracy: 0.9100 - sensitivity: 0.9375 - specificity: 0.8442 - gmeasure: 0.8893 - auc: 0.9609 - val_loss: 0.3163 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.9048 - val_specificity: 0.7556 - val_gmeasure: 0.8268 - val_auc: 0.9253\n",
      "Epoch 25/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.2376 - binary_accuracy: 0.9083 - sensitivity: 0.9490 - specificity: 0.8152 - gmeasure: 0.8789 - auc: 0.9631 - val_loss: 0.3156 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8571 - val_specificity: 0.8667 - val_gmeasure: 0.8619 - val_auc: 0.9263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.2352 - binary_accuracy: 0.9033 - sensitivity: 0.9322 - specificity: 0.8394 - gmeasure: 0.8839 - auc: 0.9641 - val_loss: 0.3120 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8857 - val_specificity: 0.8444 - val_gmeasure: 0.8648 - val_auc: 0.9266\n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.2328 - binary_accuracy: 0.9083 - sensitivity: 0.9308 - specificity: 0.8506 - gmeasure: 0.8891 - auc: 0.9626 - val_loss: 0.3119 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8762 - val_specificity: 0.8667 - val_gmeasure: 0.8714 - val_auc: 0.9266\n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2346 - binary_accuracy: 0.9017 - sensitivity: 0.9468 - specificity: 0.8001 - gmeasure: 0.8695 - auc: 0.9637 - val_loss: 0.3129 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8667 - val_specificity: 0.8667 - val_gmeasure: 0.8667 - val_auc: 0.9266\n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.2322 - binary_accuracy: 0.9050 - sensitivity: 0.9226 - specificity: 0.8670 - gmeasure: 0.8943 - auc: 0.9640 - val_loss: 0.3117 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8857 - val_specificity: 0.8667 - val_gmeasure: 0.8761 - val_auc: 0.9263\n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.2296 - binary_accuracy: 0.9067 - sensitivity: 0.9445 - specificity: 0.8228 - gmeasure: 0.8814 - auc: 0.9640 - val_loss: 0.3102 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8857 - val_specificity: 0.8667 - val_gmeasure: 0.8761 - val_auc: 0.9263\n",
      "Epoch 31/100\n",
      "600/600 [==============================] - 0s 74us/step - loss: 0.2278 - binary_accuracy: 0.9117 - sensitivity: 0.9417 - specificity: 0.8456 - gmeasure: 0.8921 - auc: 0.9647 - val_loss: 0.3104 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8762 - val_specificity: 0.8667 - val_gmeasure: 0.8714 - val_auc: 0.9266\n",
      "Epoch 32/100\n",
      "600/600 [==============================] - 0s 78us/step - loss: 0.2300 - binary_accuracy: 0.9100 - sensitivity: 0.9439 - specificity: 0.8362 - gmeasure: 0.8883 - auc: 0.9642 - val_loss: 0.3085 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8857 - val_specificity: 0.8667 - val_gmeasure: 0.8761 - val_auc: 0.9276\n",
      "Epoch 33/100\n",
      "600/600 [==============================] - 0s 76us/step - loss: 0.2310 - binary_accuracy: 0.9100 - sensitivity: 0.9275 - specificity: 0.8715 - gmeasure: 0.8986 - auc: 0.9675 - val_loss: 0.3081 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8857 - val_specificity: 0.8667 - val_gmeasure: 0.8761 - val_auc: 0.9276\n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.2280 - binary_accuracy: 0.9000 - sensitivity: 0.9447 - specificity: 0.8051 - gmeasure: 0.8717 - auc: 0.9659 - val_loss: 0.3089 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8762 - val_specificity: 0.8667 - val_gmeasure: 0.8714 - val_auc: 0.9278\n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.2268 - binary_accuracy: 0.9083 - sensitivity: 0.9278 - specificity: 0.8672 - gmeasure: 0.8967 - auc: 0.9672 - val_loss: 0.3111 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8952 - val_specificity: 0.7778 - val_gmeasure: 0.8344 - val_auc: 0.9272\n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.2341 - binary_accuracy: 0.8917 - sensitivity: 0.9504 - specificity: 0.7715 - gmeasure: 0.8546 - auc: 0.9646 - val_loss: 0.3250 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8476 - val_specificity: 0.9333 - val_gmeasure: 0.8894 - val_auc: 0.9285\n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.2307 - binary_accuracy: 0.9067 - sensitivity: 0.9028 - specificity: 0.9139 - gmeasure: 0.9081 - auc: 0.9688 - val_loss: 0.3207 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.9333 - val_specificity: 0.7111 - val_gmeasure: 0.8147 - val_auc: 0.9276\n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 0s 60us/step - loss: 0.2348 - binary_accuracy: 0.8933 - sensitivity: 0.9591 - specificity: 0.7493 - gmeasure: 0.8462 - auc: 0.9669 - val_loss: 0.3156 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8571 - val_specificity: 0.9111 - val_gmeasure: 0.8837 - val_auc: 0.9287\n",
      "Epoch 39/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.2244 - binary_accuracy: 0.9083 - sensitivity: 0.9179 - specificity: 0.8888 - gmeasure: 0.9030 - auc: 0.9689 - val_loss: 0.3058 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8857 - val_specificity: 0.8667 - val_gmeasure: 0.8761 - val_auc: 0.9287\n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.2229 - binary_accuracy: 0.9067 - sensitivity: 0.9527 - specificity: 0.8005 - gmeasure: 0.8730 - auc: 0.9667 - val_loss: 0.3087 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8667 - val_specificity: 0.8667 - val_gmeasure: 0.8667 - val_auc: 0.9285\n",
      "Epoch 41/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.2188 - binary_accuracy: 0.9083 - sensitivity: 0.9267 - specificity: 0.8663 - gmeasure: 0.8958 - auc: 0.9692 - val_loss: 0.3043 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8857 - val_specificity: 0.8667 - val_gmeasure: 0.8761 - val_auc: 0.9297\n",
      "Epoch 42/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.2208 - binary_accuracy: 0.9117 - sensitivity: 0.9428 - specificity: 0.8553 - gmeasure: 0.8968 - auc: 0.9703 - val_loss: 0.3057 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8857 - val_specificity: 0.8667 - val_gmeasure: 0.8761 - val_auc: 0.9293\n",
      "Epoch 43/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.2170 - binary_accuracy: 0.9083 - sensitivity: 0.9517 - specificity: 0.8134 - gmeasure: 0.8794 - auc: 0.9698 - val_loss: 0.3051 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8667 - val_specificity: 0.8667 - val_gmeasure: 0.8667 - val_auc: 0.9299\n",
      "Epoch 44/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.2167 - binary_accuracy: 0.9183 - sensitivity: 0.9418 - specificity: 0.8734 - gmeasure: 0.9065 - auc: 0.9716 - val_loss: 0.3045 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9048 - val_specificity: 0.8444 - val_gmeasure: 0.8741 - val_auc: 0.9310\n",
      "Epoch 45/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.2139 - binary_accuracy: 0.9150 - sensitivity: 0.9568 - specificity: 0.8269 - gmeasure: 0.8889 - auc: 0.9701 - val_loss: 0.3047 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8667 - val_specificity: 0.8667 - val_gmeasure: 0.8667 - val_auc: 0.9306\n",
      "Epoch 46/100\n",
      "600/600 [==============================] - 0s 69us/step - loss: 0.2158 - binary_accuracy: 0.9133 - sensitivity: 0.9203 - specificity: 0.8995 - gmeasure: 0.9095 - auc: 0.9700 - val_loss: 0.3051 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9048 - val_specificity: 0.8444 - val_gmeasure: 0.8741 - val_auc: 0.9310\n",
      "Epoch 47/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2169 - binary_accuracy: 0.9050 - sensitivity: 0.9615 - specificity: 0.7785 - gmeasure: 0.8648 - auc: 0.9703 - val_loss: 0.3052 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8667 - val_specificity: 0.8667 - val_gmeasure: 0.8667 - val_auc: 0.9314\n",
      "Epoch 48/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.2262 - binary_accuracy: 0.9083 - sensitivity: 0.9093 - specificity: 0.9129 - gmeasure: 0.9104 - auc: 0.9717 - val_loss: 0.3140 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.9238 - val_specificity: 0.7333 - val_gmeasure: 0.8231 - val_auc: 0.9302\n",
      "Epoch 49/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2235 - binary_accuracy: 0.9000 - sensitivity: 0.9657 - specificity: 0.7514 - gmeasure: 0.8516 - auc: 0.9708 - val_loss: 0.3068 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8667 - val_specificity: 0.8889 - val_gmeasure: 0.8777 - val_auc: 0.9331\n",
      "Epoch 50/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.2266 - binary_accuracy: 0.9017 - sensitivity: 0.8961 - specificity: 0.9152 - gmeasure: 0.9049 - auc: 0.9719 - val_loss: 0.3039 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9048 - val_specificity: 0.8444 - val_gmeasure: 0.8741 - val_auc: 0.9316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100\n",
      "600/600 [==============================] - 0s 77us/step - loss: 0.2225 - binary_accuracy: 0.9033 - sensitivity: 0.9663 - specificity: 0.7647 - gmeasure: 0.8596 - auc: 0.9707 - val_loss: 0.3004 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8667 - val_specificity: 0.8667 - val_gmeasure: 0.8667 - val_auc: 0.9333\n",
      "Epoch 52/100\n",
      "600/600 [==============================] - 0s 69us/step - loss: 0.2177 - binary_accuracy: 0.9083 - sensitivity: 0.9085 - specificity: 0.9019 - gmeasure: 0.9044 - auc: 0.9737 - val_loss: 0.3005 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8952 - val_specificity: 0.8444 - val_gmeasure: 0.8695 - val_auc: 0.9316\n",
      "Epoch 53/100\n",
      "600/600 [==============================] - 0s 70us/step - loss: 0.2176 - binary_accuracy: 0.9083 - sensitivity: 0.9622 - specificity: 0.7960 - gmeasure: 0.8746 - auc: 0.9722 - val_loss: 0.3014 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8667 - val_specificity: 0.8889 - val_gmeasure: 0.8777 - val_auc: 0.9331\n",
      "Epoch 54/100\n",
      "600/600 [==============================] - 0s 77us/step - loss: 0.2125 - binary_accuracy: 0.9150 - sensitivity: 0.9127 - specificity: 0.9207 - gmeasure: 0.9161 - auc: 0.9742 - val_loss: 0.3005 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9048 - val_specificity: 0.8444 - val_gmeasure: 0.8741 - val_auc: 0.9323\n",
      "Epoch 55/100\n",
      "600/600 [==============================] - 0s 71us/step - loss: 0.2195 - binary_accuracy: 0.9000 - sensitivity: 0.9637 - specificity: 0.7587 - gmeasure: 0.8546 - auc: 0.9728 - val_loss: 0.2982 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8667 - val_specificity: 0.8889 - val_gmeasure: 0.8777 - val_auc: 0.9346\n",
      "Epoch 56/100\n",
      "600/600 [==============================] - 0s 70us/step - loss: 0.2248 - binary_accuracy: 0.9100 - sensitivity: 0.8980 - specificity: 0.9377 - gmeasure: 0.9174 - auc: 0.9733 - val_loss: 0.2980 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8952 - val_specificity: 0.8444 - val_gmeasure: 0.8695 - val_auc: 0.9321\n",
      "Epoch 57/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2231 - binary_accuracy: 0.8967 - sensitivity: 0.9634 - specificity: 0.7436 - gmeasure: 0.8433 - auc: 0.9735 - val_loss: 0.2943 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8952 - val_specificity: 0.8667 - val_gmeasure: 0.8808 - val_auc: 0.9333\n",
      "Epoch 58/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.2052 - binary_accuracy: 0.9217 - sensitivity: 0.9222 - specificity: 0.9170 - gmeasure: 0.9192 - auc: 0.9747 - val_loss: 0.2943 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8857 - val_specificity: 0.8667 - val_gmeasure: 0.8761 - val_auc: 0.9333\n",
      "Epoch 59/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.2018 - binary_accuracy: 0.9150 - sensitivity: 0.9532 - specificity: 0.8242 - gmeasure: 0.8859 - auc: 0.9741 - val_loss: 0.2947 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8952 - val_specificity: 0.8667 - val_gmeasure: 0.8808 - val_auc: 0.9329\n",
      "Epoch 60/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.2093 - binary_accuracy: 0.9200 - sensitivity: 0.9300 - specificity: 0.8996 - gmeasure: 0.9136 - auc: 0.9748 - val_loss: 0.2952 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8952 - val_specificity: 0.8667 - val_gmeasure: 0.8808 - val_auc: 0.9344\n",
      "Epoch 61/100\n",
      "600/600 [==============================] - 0s 60us/step - loss: 0.2041 - binary_accuracy: 0.9083 - sensitivity: 0.9542 - specificity: 0.8092 - gmeasure: 0.8780 - auc: 0.9747 - val_loss: 0.2943 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8952 - val_specificity: 0.8667 - val_gmeasure: 0.8808 - val_auc: 0.9344\n",
      "Epoch 62/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.1964 - binary_accuracy: 0.9333 - sensitivity: 0.9472 - specificity: 0.9020 - gmeasure: 0.9243 - auc: 0.9740 - val_loss: 0.2927 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8952 - val_specificity: 0.8667 - val_gmeasure: 0.8808 - val_auc: 0.9344\n",
      "Epoch 63/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.1939 - binary_accuracy: 0.9267 - sensitivity: 0.9487 - specificity: 0.8759 - gmeasure: 0.9114 - auc: 0.9754 - val_loss: 0.2924 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8952 - val_specificity: 0.8667 - val_gmeasure: 0.8808 - val_auc: 0.9340\n",
      "Epoch 64/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1948 - binary_accuracy: 0.9217 - sensitivity: 0.9538 - specificity: 0.8532 - gmeasure: 0.9019 - auc: 0.9754 - val_loss: 0.2923 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8762 - val_specificity: 0.8667 - val_gmeasure: 0.8714 - val_auc: 0.9354\n",
      "Epoch 65/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.1927 - binary_accuracy: 0.9300 - sensitivity: 0.9514 - specificity: 0.8828 - gmeasure: 0.9163 - auc: 0.9753 - val_loss: 0.2931 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8952 - val_specificity: 0.8667 - val_gmeasure: 0.8808 - val_auc: 0.9340\n",
      "Epoch 66/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.1941 - binary_accuracy: 0.9183 - sensitivity: 0.9475 - specificity: 0.8526 - gmeasure: 0.8983 - auc: 0.9766 - val_loss: 0.2902 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8952 - val_specificity: 0.8667 - val_gmeasure: 0.8808 - val_auc: 0.9350\n",
      "Epoch 67/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.1912 - binary_accuracy: 0.9250 - sensitivity: 0.9445 - specificity: 0.8854 - gmeasure: 0.9144 - auc: 0.9771 - val_loss: 0.2889 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8952 - val_specificity: 0.8667 - val_gmeasure: 0.8808 - val_auc: 0.9352\n",
      "Epoch 68/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.2157 - binary_accuracy: 0.9050 - sensitivity: 0.9650 - specificity: 0.7924 - gmeasure: 0.8693 - auc: 0.9790 - val_loss: 0.2970 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8667 - val_specificity: 0.9111 - val_gmeasure: 0.8886 - val_auc: 0.9382\n",
      "Epoch 69/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2153 - binary_accuracy: 0.9167 - sensitivity: 0.8931 - specificity: 0.9671 - gmeasure: 0.9293 - auc: 0.9777 - val_loss: 0.2936 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9048 - val_specificity: 0.8444 - val_gmeasure: 0.8741 - val_auc: 0.9342\n",
      "Epoch 70/100\n",
      "600/600 [==============================] - 0s 70us/step - loss: 0.2031 - binary_accuracy: 0.9033 - sensitivity: 0.9688 - specificity: 0.7647 - gmeasure: 0.8592 - auc: 0.9761 - val_loss: 0.2877 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8857 - val_specificity: 0.8889 - val_gmeasure: 0.8873 - val_auc: 0.9376\n",
      "Epoch 71/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.1876 - binary_accuracy: 0.9317 - sensitivity: 0.9415 - specificity: 0.9099 - gmeasure: 0.9254 - auc: 0.9787 - val_loss: 0.2870 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8857 - val_specificity: 0.8889 - val_gmeasure: 0.8873 - val_auc: 0.9380\n",
      "Epoch 72/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1861 - binary_accuracy: 0.9267 - sensitivity: 0.9491 - specificity: 0.8780 - gmeasure: 0.9127 - auc: 0.9779 - val_loss: 0.2885 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8952 - val_specificity: 0.8667 - val_gmeasure: 0.8808 - val_auc: 0.9365\n",
      "Epoch 73/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.1863 - binary_accuracy: 0.9217 - sensitivity: 0.9586 - specificity: 0.8415 - gmeasure: 0.8976 - auc: 0.9771 - val_loss: 0.2881 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8762 - val_specificity: 0.8889 - val_gmeasure: 0.8825 - val_auc: 0.9382\n",
      "Epoch 74/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.1883 - binary_accuracy: 0.9317 - sensitivity: 0.9371 - specificity: 0.9193 - gmeasure: 0.9280 - auc: 0.9793 - val_loss: 0.2910 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9048 - val_specificity: 0.8444 - val_gmeasure: 0.8741 - val_auc: 0.9352\n",
      "Epoch 75/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.1874 - binary_accuracy: 0.9200 - sensitivity: 0.9636 - specificity: 0.8235 - gmeasure: 0.8908 - auc: 0.9788 - val_loss: 0.2944 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8667 - val_specificity: 0.9111 - val_gmeasure: 0.8886 - val_auc: 0.9395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.1898 - binary_accuracy: 0.9217 - sensitivity: 0.9193 - specificity: 0.9242 - gmeasure: 0.9213 - auc: 0.9793 - val_loss: 0.2913 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.9048 - val_specificity: 0.8222 - val_gmeasure: 0.8625 - val_auc: 0.9363\n",
      "Epoch 77/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.1972 - binary_accuracy: 0.9217 - sensitivity: 0.9592 - specificity: 0.8442 - gmeasure: 0.8975 - auc: 0.9798 - val_loss: 0.2886 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8762 - val_specificity: 0.8889 - val_gmeasure: 0.8825 - val_auc: 0.9401\n",
      "Epoch 78/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.1866 - binary_accuracy: 0.9250 - sensitivity: 0.9440 - specificity: 0.8777 - gmeasure: 0.9085 - auc: 0.9786 - val_loss: 0.2887 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9048 - val_specificity: 0.8444 - val_gmeasure: 0.8741 - val_auc: 0.9357\n",
      "Epoch 79/100\n",
      "600/600 [==============================] - 0s 52us/step - loss: 0.1839 - binary_accuracy: 0.9283 - sensitivity: 0.9485 - specificity: 0.8857 - gmeasure: 0.9161 - auc: 0.9789 - val_loss: 0.2825 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8952 - val_specificity: 0.8667 - val_gmeasure: 0.8808 - val_auc: 0.9388\n",
      "Epoch 80/100\n",
      "600/600 [==============================] - 0s 57us/step - loss: 0.1791 - binary_accuracy: 0.9300 - sensitivity: 0.9566 - specificity: 0.8700 - gmeasure: 0.9120 - auc: 0.9798 - val_loss: 0.2821 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.9048 - val_specificity: 0.8667 - val_gmeasure: 0.8855 - val_auc: 0.9384\n",
      "Epoch 81/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.1774 - binary_accuracy: 0.9383 - sensitivity: 0.9546 - specificity: 0.9055 - gmeasure: 0.9293 - auc: 0.9805 - val_loss: 0.2819 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.9048 - val_specificity: 0.8667 - val_gmeasure: 0.8855 - val_auc: 0.9382\n",
      "Epoch 82/100\n",
      "600/600 [==============================] - 0s 58us/step - loss: 0.1793 - binary_accuracy: 0.9217 - sensitivity: 0.9595 - specificity: 0.8335 - gmeasure: 0.8934 - auc: 0.9798 - val_loss: 0.2829 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8857 - val_specificity: 0.8889 - val_gmeasure: 0.8873 - val_auc: 0.9399\n",
      "Epoch 83/100\n",
      "600/600 [==============================] - 0s 56us/step - loss: 0.1793 - binary_accuracy: 0.9350 - sensitivity: 0.9373 - specificity: 0.9292 - gmeasure: 0.9332 - auc: 0.9812 - val_loss: 0.2886 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9143 - val_specificity: 0.8222 - val_gmeasure: 0.8670 - val_auc: 0.9361\n",
      "Epoch 84/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.1833 - binary_accuracy: 0.9217 - sensitivity: 0.9541 - specificity: 0.8594 - gmeasure: 0.9043 - auc: 0.9808 - val_loss: 0.2826 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.8952 - val_specificity: 0.8889 - val_gmeasure: 0.8921 - val_auc: 0.9395\n",
      "Epoch 85/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.1753 - binary_accuracy: 0.9400 - sensitivity: 0.9539 - specificity: 0.9088 - gmeasure: 0.9309 - auc: 0.9811 - val_loss: 0.2840 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9143 - val_specificity: 0.8222 - val_gmeasure: 0.8670 - val_auc: 0.9376\n",
      "Epoch 86/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.1738 - binary_accuracy: 0.9267 - sensitivity: 0.9666 - specificity: 0.8416 - gmeasure: 0.9014 - auc: 0.9830 - val_loss: 0.3002 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8571 - val_specificity: 0.9556 - val_gmeasure: 0.9050 - val_auc: 0.9418\n",
      "Epoch 87/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.1915 - binary_accuracy: 0.9250 - sensitivity: 0.9209 - specificity: 0.9383 - gmeasure: 0.9286 - auc: 0.9827 - val_loss: 0.3095 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.9429 - val_specificity: 0.7778 - val_gmeasure: 0.8563 - val_auc: 0.9363\n",
      "Epoch 88/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.1848 - binary_accuracy: 0.9150 - sensitivity: 0.9685 - specificity: 0.7962 - gmeasure: 0.8772 - auc: 0.9814 - val_loss: 0.2971 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8571 - val_specificity: 0.9556 - val_gmeasure: 0.9050 - val_auc: 0.9422\n",
      "Epoch 89/100\n",
      "600/600 [==============================] - 0s 69us/step - loss: 0.1954 - binary_accuracy: 0.9217 - sensitivity: 0.9193 - specificity: 0.9194 - gmeasure: 0.9172 - auc: 0.9810 - val_loss: 0.3221 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.9429 - val_specificity: 0.7111 - val_gmeasure: 0.8188 - val_auc: 0.9344\n",
      "Epoch 90/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.1900 - binary_accuracy: 0.9183 - sensitivity: 0.9669 - specificity: 0.8154 - gmeasure: 0.8864 - auc: 0.9807 - val_loss: 0.3091 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8286 - val_specificity: 0.9556 - val_gmeasure: 0.8898 - val_auc: 0.9439\n",
      "Epoch 91/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.1917 - binary_accuracy: 0.9250 - sensitivity: 0.9176 - specificity: 0.9404 - gmeasure: 0.9280 - auc: 0.9826 - val_loss: 0.3204 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.9429 - val_specificity: 0.7333 - val_gmeasure: 0.8315 - val_auc: 0.9365\n",
      "Epoch 92/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.1938 - binary_accuracy: 0.9167 - sensitivity: 0.9686 - specificity: 0.8049 - gmeasure: 0.8826 - auc: 0.9812 - val_loss: 0.3101 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8286 - val_specificity: 0.9556 - val_gmeasure: 0.8898 - val_auc: 0.9429\n",
      "Epoch 93/100\n",
      "600/600 [==============================] - 0s 69us/step - loss: 0.1966 - binary_accuracy: 0.9133 - sensitivity: 0.9099 - specificity: 0.9292 - gmeasure: 0.9185 - auc: 0.9833 - val_loss: 0.3116 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9333 - val_specificity: 0.7778 - val_gmeasure: 0.8520 - val_auc: 0.9357\n",
      "Epoch 94/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.1826 - binary_accuracy: 0.9233 - sensitivity: 0.9710 - specificity: 0.8196 - gmeasure: 0.8911 - auc: 0.9812 - val_loss: 0.2926 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8476 - val_specificity: 0.9333 - val_gmeasure: 0.8894 - val_auc: 0.9418\n",
      "Epoch 95/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.1815 - binary_accuracy: 0.9283 - sensitivity: 0.9128 - specificity: 0.9629 - gmeasure: 0.9374 - auc: 0.9830 - val_loss: 0.2984 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9238 - val_specificity: 0.8000 - val_gmeasure: 0.8597 - val_auc: 0.9369\n",
      "Epoch 96/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.1816 - binary_accuracy: 0.9183 - sensitivity: 0.9709 - specificity: 0.8024 - gmeasure: 0.8825 - auc: 0.9821 - val_loss: 0.2853 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8571 - val_specificity: 0.9111 - val_gmeasure: 0.8837 - val_auc: 0.9422\n",
      "Epoch 97/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.1889 - binary_accuracy: 0.9217 - sensitivity: 0.9060 - specificity: 0.9562 - gmeasure: 0.9305 - auc: 0.9843 - val_loss: 0.2935 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9238 - val_specificity: 0.8000 - val_gmeasure: 0.8597 - val_auc: 0.9386\n",
      "Epoch 98/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.1956 - binary_accuracy: 0.9117 - sensitivity: 0.9755 - specificity: 0.7681 - gmeasure: 0.8652 - auc: 0.9822 - val_loss: 0.2737 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.8952 - val_specificity: 0.8889 - val_gmeasure: 0.8921 - val_auc: 0.9424\n",
      "Epoch 99/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.1880 - binary_accuracy: 0.9300 - sensitivity: 0.9167 - specificity: 0.9642 - gmeasure: 0.9396 - auc: 0.9839 - val_loss: 0.2804 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.9048 - val_specificity: 0.8222 - val_gmeasure: 0.8625 - val_auc: 0.9390\n",
      "Epoch 100/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.1839 - binary_accuracy: 0.9200 - sensitivity: 0.9711 - specificity: 0.8083 - gmeasure: 0.8858 - auc: 0.9832 - val_loss: 0.2771 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.9048 - val_specificity: 0.8667 - val_gmeasure: 0.8855 - val_auc: 0.9407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:151] Training end with time 6.148927211761475!\n",
      "[root    |INFO|build_network.py:79] Saved trained model weight at ./example_result/weight_0.h5 \n",
      "[root    |DEBUG|deepbiome.py:166] Save weight at ./example_result/weight_0.h5\n",
      "[root    |DEBUG|deepbiome.py:169] Save history at ./example_result/hist_0.json\n",
      "[root    |INFO|build_network.py:177] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "750/750 [==============================] - 0s 8us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:182] Evaluation end with time 0.014052629470825195!\n",
      "[root    |INFO|build_network.py:183] Evaluation: [0.1827818602323532, 0.9306666851043701, 0.9517374634742737, 0.8836206793785095, 0.917046844959259, 0.9758936762809753]\n",
      "[root    |INFO|build_network.py:177] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "250/250 [==============================] - 0s 22us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:182] Evaluation end with time 0.013617753982543945!\n",
      "[root    |INFO|build_network.py:183] Evaluation: [0.282605916261673, 0.8960000276565552, 0.9349112510681152, 0.8148148059844971, 0.872799813747406, 0.9505442380905151]\n",
      "[root    |INFO|deepbiome.py:179] Compute time : 7.753520727157593\n",
      "[root    |INFO|deepbiome.py:180] 1 fold computing end!---------------------------------------------\n",
      "[root    |INFO|deepbiome.py:137] -------2 simulation start!----------------------------------\n",
      "[root    |INFO|readers.py:58] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:59] Construct Dataset\n",
      "[root    |INFO|readers.py:60] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:61] Load data\n",
      "[root    |INFO|deepbiome.py:147] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:148] Build network for 2 simulation\n",
      "[root    |INFO|build_network.py:513] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:514] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv\n",
      "[root    |INFO|build_network.py:518] Phylogenetic tree level list: ['Genus', 'Family', 'Order', 'Class', 'Phylum']\n",
      "[root    |INFO|build_network.py:519] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:526]      Genus: 48\n",
      "[root    |INFO|build_network.py:526]     Family: 40\n",
      "[root    |INFO|build_network.py:526]      Order: 23\n",
      "[root    |INFO|build_network.py:526]      Class: 17\n",
      "[root    |INFO|build_network.py:526]     Phylum: 9\n",
      "[root    |INFO|build_network.py:529] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:530] Phylogenetic_tree_dict info: ['Genus', 'Number', 'Order', 'Class', 'Phylum', 'Family']\n",
      "[root    |INFO|build_network.py:531] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [ Genus, Family]\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [Family,  Order]\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [ Order,  Class]\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [ Class, Phylum]\n",
      "[root    |INFO|build_network.py:554] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:570] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:571] Build network based on phylogenetic tree information\n",
      "[root    |INFO|build_network.py:572] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:648] ------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "l1_dense (Dense_with_tree)   (None, 40)                1960      \n",
      "_________________________________________________________________\n",
      "l1_activation (Activation)   (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "l2_dense (Dense_with_tree)   (None, 23)                943       \n",
      "_________________________________________________________________\n",
      "l2_activation (Activation)   (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "l3_dense (Dense_with_tree)   (None, 17)                408       \n",
      "_________________________________________________________________\n",
      "l3_activation (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "l4_dense (Dense_with_tree)   (None, 9)                 162       \n",
      "_________________________________________________________________\n",
      "l4_activation (Activation)   (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "last_dense_h (Dense)         (None, 1)                 10        \n",
      "_________________________________________________________________\n",
      "p_hat (Activation)           (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,483\n",
      "Trainable params: 3,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:57] Build Network\n",
      "[root    |INFO|build_network.py:58] Optimizer = adam\n",
      "[root    |INFO|build_network.py:59] Loss = binary_crossentropy\n",
      "[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc\n",
      "[root    |INFO|build_network.py:83] Load trained model weight at ./example_result/weight_1.h5 \n",
      "[root    |INFO|deepbiome.py:157] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:158] 2 fold computing start!----------------------------------\n",
      "[root    |INFO|build_network.py:141] Training start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 150 samples\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 1s 850us/step - loss: 0.3123 - binary_accuracy: 0.8533 - sensitivity: 0.8351 - specificity: 0.8991 - gmeasure: 0.8661 - auc: 0.9036 - val_loss: 0.3839 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8532 - val_specificity: 0.7317 - val_gmeasure: 0.7901 - val_auc: 0.8867\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.3246 - binary_accuracy: 0.8283 - sensitivity: 0.8603 - specificity: 0.7430 - gmeasure: 0.7993 - auc: 0.9008 - val_loss: 0.3616 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8866\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 0s 69us/step - loss: 0.3121 - binary_accuracy: 0.8517 - sensitivity: 0.8337 - specificity: 0.9067 - gmeasure: 0.8679 - auc: 0.9088 - val_loss: 0.3452 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8165 - val_specificity: 0.8293 - val_gmeasure: 0.8229 - val_auc: 0.8834\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.3089 - binary_accuracy: 0.8483 - sensitivity: 0.8219 - specificity: 0.9186 - gmeasure: 0.8687 - auc: 0.9062 - val_loss: 0.3602 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8870\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 0s 72us/step - loss: 0.3020 - binary_accuracy: 0.8450 - sensitivity: 0.8533 - specificity: 0.8233 - gmeasure: 0.8382 - auc: 0.9048 - val_loss: 0.3728 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8440 - val_specificity: 0.7805 - val_gmeasure: 0.8116 - val_auc: 0.8904\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 0s 69us/step - loss: 0.3043 - binary_accuracy: 0.8383 - sensitivity: 0.8512 - specificity: 0.8044 - gmeasure: 0.8266 - auc: 0.9048 - val_loss: 0.3533 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8874\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.3009 - binary_accuracy: 0.8567 - sensitivity: 0.8399 - specificity: 0.9012 - gmeasure: 0.8700 - auc: 0.9067 - val_loss: 0.3480 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8825\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.2997 - binary_accuracy: 0.8583 - sensitivity: 0.8423 - specificity: 0.9011 - gmeasure: 0.8712 - auc: 0.9072 - val_loss: 0.3593 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8892\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2995 - binary_accuracy: 0.8467 - sensitivity: 0.8493 - specificity: 0.8415 - gmeasure: 0.8446 - auc: 0.9059 - val_loss: 0.3611 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8879\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 0s 70us/step - loss: 0.2992 - binary_accuracy: 0.8517 - sensitivity: 0.8431 - specificity: 0.8797 - gmeasure: 0.8605 - auc: 0.9084 - val_loss: 0.3484 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8845\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.2971 - binary_accuracy: 0.8600 - sensitivity: 0.8397 - specificity: 0.9141 - gmeasure: 0.8760 - auc: 0.9115 - val_loss: 0.3563 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8864\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2993 - binary_accuracy: 0.8517 - sensitivity: 0.8566 - specificity: 0.8450 - gmeasure: 0.8502 - auc: 0.9062 - val_loss: 0.3645 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8866\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.2935 - binary_accuracy: 0.8600 - sensitivity: 0.8539 - specificity: 0.8788 - gmeasure: 0.8661 - auc: 0.9076 - val_loss: 0.3493 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8836\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.2947 - binary_accuracy: 0.8617 - sensitivity: 0.8408 - specificity: 0.9154 - gmeasure: 0.8772 - auc: 0.9097 - val_loss: 0.3498 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8854\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.2924 - binary_accuracy: 0.8600 - sensitivity: 0.8466 - specificity: 0.8932 - gmeasure: 0.8695 - auc: 0.9102 - val_loss: 0.3607 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8904\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.2924 - binary_accuracy: 0.8550 - sensitivity: 0.8534 - specificity: 0.8590 - gmeasure: 0.8562 - auc: 0.9077 - val_loss: 0.3571 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8891\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.2912 - binary_accuracy: 0.8600 - sensitivity: 0.8443 - specificity: 0.9049 - gmeasure: 0.8739 - auc: 0.9095 - val_loss: 0.3502 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8257 - val_specificity: 0.7805 - val_gmeasure: 0.8028 - val_auc: 0.8854\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.2903 - binary_accuracy: 0.8633 - sensitivity: 0.8451 - specificity: 0.9152 - gmeasure: 0.8793 - auc: 0.9122 - val_loss: 0.3567 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8906\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 0s 58us/step - loss: 0.2899 - binary_accuracy: 0.8617 - sensitivity: 0.8538 - specificity: 0.8845 - gmeasure: 0.8689 - auc: 0.9087 - val_loss: 0.3598 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8889\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 0s 60us/step - loss: 0.2880 - binary_accuracy: 0.8600 - sensitivity: 0.8515 - specificity: 0.8806 - gmeasure: 0.8658 - auc: 0.9096 - val_loss: 0.3508 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8257 - val_specificity: 0.7805 - val_gmeasure: 0.8028 - val_auc: 0.8890\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2882 - binary_accuracy: 0.8633 - sensitivity: 0.8493 - specificity: 0.9011 - gmeasure: 0.8748 - auc: 0.9112 - val_loss: 0.3569 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8906\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 0s 72us/step - loss: 0.2882 - binary_accuracy: 0.8600 - sensitivity: 0.8513 - specificity: 0.8828 - gmeasure: 0.8666 - auc: 0.9096 - val_loss: 0.3561 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8906\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2862 - binary_accuracy: 0.8617 - sensitivity: 0.8396 - specificity: 0.9199 - gmeasure: 0.8787 - auc: 0.9126 - val_loss: 0.3488 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8257 - val_specificity: 0.7805 - val_gmeasure: 0.8028 - val_auc: 0.8890\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.2859 - binary_accuracy: 0.8650 - sensitivity: 0.8435 - specificity: 0.9216 - gmeasure: 0.8815 - auc: 0.9115 - val_loss: 0.3646 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8940\n",
      "Epoch 25/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2872 - binary_accuracy: 0.8533 - sensitivity: 0.8533 - specificity: 0.8551 - gmeasure: 0.8541 - auc: 0.9073 - val_loss: 0.3546 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100\n",
      "600/600 [==============================] - 0s 69us/step - loss: 0.2860 - binary_accuracy: 0.8617 - sensitivity: 0.8332 - specificity: 0.9406 - gmeasure: 0.8852 - auc: 0.9130 - val_loss: 0.3426 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8257 - val_specificity: 0.8049 - val_gmeasure: 0.8152 - val_auc: 0.8857\n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 0s 69us/step - loss: 0.2839 - binary_accuracy: 0.8667 - sensitivity: 0.8422 - specificity: 0.9319 - gmeasure: 0.8859 - auc: 0.9142 - val_loss: 0.3687 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8440 - val_specificity: 0.7805 - val_gmeasure: 0.8116 - val_auc: 0.8925\n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.2921 - binary_accuracy: 0.8483 - sensitivity: 0.8519 - specificity: 0.8372 - gmeasure: 0.8445 - auc: 0.9054 - val_loss: 0.3606 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8908\n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.2818 - binary_accuracy: 0.8667 - sensitivity: 0.8374 - specificity: 0.9439 - gmeasure: 0.8890 - auc: 0.9131 - val_loss: 0.3405 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8257 - val_specificity: 0.8049 - val_gmeasure: 0.8152 - val_auc: 0.8877\n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.2857 - binary_accuracy: 0.8583 - sensitivity: 0.8350 - specificity: 0.9194 - gmeasure: 0.8753 - auc: 0.9138 - val_loss: 0.3647 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8440 - val_specificity: 0.7805 - val_gmeasure: 0.8116 - val_auc: 0.8929\n",
      "Epoch 31/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.2823 - binary_accuracy: 0.8567 - sensitivity: 0.8540 - specificity: 0.8672 - gmeasure: 0.8603 - auc: 0.9109 - val_loss: 0.3580 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8862\n",
      "Epoch 32/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.2798 - binary_accuracy: 0.8667 - sensitivity: 0.8444 - specificity: 0.9274 - gmeasure: 0.8848 - auc: 0.9130 - val_loss: 0.3451 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8257 - val_specificity: 0.8049 - val_gmeasure: 0.8152 - val_auc: 0.8880\n",
      "Epoch 33/100\n",
      "600/600 [==============================] - 0s 59us/step - loss: 0.2835 - binary_accuracy: 0.8700 - sensitivity: 0.8464 - specificity: 0.9378 - gmeasure: 0.8907 - auc: 0.9108 - val_loss: 0.3606 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8440 - val_specificity: 0.7805 - val_gmeasure: 0.8116 - val_auc: 0.8898\n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2775 - binary_accuracy: 0.8667 - sensitivity: 0.8491 - specificity: 0.9115 - gmeasure: 0.8796 - auc: 0.9139 - val_loss: 0.3514 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8257 - val_specificity: 0.7805 - val_gmeasure: 0.8028 - val_auc: 0.8880\n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.2781 - binary_accuracy: 0.8700 - sensitivity: 0.8463 - specificity: 0.9283 - gmeasure: 0.8858 - auc: 0.9150 - val_loss: 0.3570 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8257 - val_specificity: 0.7805 - val_gmeasure: 0.8028 - val_auc: 0.8896\n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.2768 - binary_accuracy: 0.8617 - sensitivity: 0.8496 - specificity: 0.8957 - gmeasure: 0.8723 - auc: 0.9108 - val_loss: 0.3619 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8440 - val_specificity: 0.7805 - val_gmeasure: 0.8116 - val_auc: 0.8917\n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.2757 - binary_accuracy: 0.8650 - sensitivity: 0.8490 - specificity: 0.9077 - gmeasure: 0.8778 - auc: 0.9125 - val_loss: 0.3488 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8257 - val_specificity: 0.7805 - val_gmeasure: 0.8028 - val_auc: 0.8885\n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2767 - binary_accuracy: 0.8683 - sensitivity: 0.8400 - specificity: 0.9442 - gmeasure: 0.8904 - auc: 0.9154 - val_loss: 0.3556 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8257 - val_specificity: 0.7805 - val_gmeasure: 0.8028 - val_auc: 0.8919\n",
      "Epoch 39/100\n",
      "600/600 [==============================] - 0s 70us/step - loss: 0.2745 - binary_accuracy: 0.8683 - sensitivity: 0.8482 - specificity: 0.9210 - gmeasure: 0.8838 - auc: 0.9115 - val_loss: 0.3596 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8440 - val_specificity: 0.7805 - val_gmeasure: 0.8116 - val_auc: 0.8902\n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 0s 70us/step - loss: 0.2743 - binary_accuracy: 0.8683 - sensitivity: 0.8467 - specificity: 0.9255 - gmeasure: 0.8851 - auc: 0.9120 - val_loss: 0.3531 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8257 - val_specificity: 0.7805 - val_gmeasure: 0.8028 - val_auc: 0.8885\n",
      "Epoch 41/100\n",
      "600/600 [==============================] - 0s 72us/step - loss: 0.2732 - binary_accuracy: 0.8700 - sensitivity: 0.8467 - specificity: 0.9335 - gmeasure: 0.8889 - auc: 0.9138 - val_loss: 0.3591 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8905\n",
      "Epoch 42/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2728 - binary_accuracy: 0.8683 - sensitivity: 0.8478 - specificity: 0.9183 - gmeasure: 0.8815 - auc: 0.9108 - val_loss: 0.3570 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8902\n",
      "Epoch 43/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.2720 - binary_accuracy: 0.8700 - sensitivity: 0.8484 - specificity: 0.9256 - gmeasure: 0.8861 - auc: 0.9121 - val_loss: 0.3534 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8917\n",
      "Epoch 44/100\n",
      "600/600 [==============================] - 0s 71us/step - loss: 0.2735 - binary_accuracy: 0.8733 - sensitivity: 0.8436 - specificity: 0.9471 - gmeasure: 0.8934 - auc: 0.9152 - val_loss: 0.3551 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8917\n",
      "Epoch 45/100\n",
      "600/600 [==============================] - 0s 74us/step - loss: 0.2743 - binary_accuracy: 0.8683 - sensitivity: 0.8520 - specificity: 0.9157 - gmeasure: 0.8828 - auc: 0.9109 - val_loss: 0.3611 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8440 - val_specificity: 0.7805 - val_gmeasure: 0.8116 - val_auc: 0.8900\n",
      "Epoch 46/100\n",
      "600/600 [==============================] - 0s 70us/step - loss: 0.2764 - binary_accuracy: 0.8717 - sensitivity: 0.8424 - specificity: 0.9523 - gmeasure: 0.8952 - auc: 0.9163 - val_loss: 0.3459 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8165 - val_specificity: 0.8049 - val_gmeasure: 0.8107 - val_auc: 0.8885\n",
      "Epoch 47/100\n",
      "600/600 [==============================] - 0s 70us/step - loss: 0.2715 - binary_accuracy: 0.8733 - sensitivity: 0.8490 - specificity: 0.9382 - gmeasure: 0.8924 - auc: 0.9123 - val_loss: 0.3696 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8440 - val_specificity: 0.7561 - val_gmeasure: 0.7989 - val_auc: 0.8902\n",
      "Epoch 48/100\n",
      "600/600 [==============================] - 0s 74us/step - loss: 0.2719 - binary_accuracy: 0.8667 - sensitivity: 0.8498 - specificity: 0.9143 - gmeasure: 0.8813 - auc: 0.9128 - val_loss: 0.3496 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8165 - val_specificity: 0.8049 - val_gmeasure: 0.8107 - val_auc: 0.8885\n",
      "Epoch 49/100\n",
      "600/600 [==============================] - 0s 69us/step - loss: 0.2693 - binary_accuracy: 0.8750 - sensitivity: 0.8490 - specificity: 0.9445 - gmeasure: 0.8953 - auc: 0.9136 - val_loss: 0.3598 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8902\n",
      "Epoch 50/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.2688 - binary_accuracy: 0.8750 - sensitivity: 0.8529 - specificity: 0.9400 - gmeasure: 0.8947 - auc: 0.9174 - val_loss: 0.3552 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.2674 - binary_accuracy: 0.8717 - sensitivity: 0.8490 - specificity: 0.9311 - gmeasure: 0.8889 - auc: 0.9130 - val_loss: 0.3571 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8924\n",
      "Epoch 52/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2682 - binary_accuracy: 0.8750 - sensitivity: 0.8491 - specificity: 0.9546 - gmeasure: 0.8995 - auc: 0.9198 - val_loss: 0.3485 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8257 - val_specificity: 0.8049 - val_gmeasure: 0.8152 - val_auc: 0.8873\n",
      "Epoch 53/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.2703 - binary_accuracy: 0.8700 - sensitivity: 0.8510 - specificity: 0.9215 - gmeasure: 0.8853 - auc: 0.9140 - val_loss: 0.3650 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8349 - val_specificity: 0.7561 - val_gmeasure: 0.7945 - val_auc: 0.8909\n",
      "Epoch 54/100\n",
      "600/600 [==============================] - 0s 70us/step - loss: 0.2682 - binary_accuracy: 0.8750 - sensitivity: 0.8493 - specificity: 0.9460 - gmeasure: 0.8962 - auc: 0.9145 - val_loss: 0.3419 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8165 - val_specificity: 0.8537 - val_gmeasure: 0.8349 - val_auc: 0.8863\n",
      "Epoch 55/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2674 - binary_accuracy: 0.8750 - sensitivity: 0.8446 - specificity: 0.9587 - gmeasure: 0.8997 - auc: 0.9169 - val_loss: 0.3677 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8440 - val_specificity: 0.7561 - val_gmeasure: 0.7989 - val_auc: 0.8911\n",
      "Epoch 56/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2670 - binary_accuracy: 0.8733 - sensitivity: 0.8492 - specificity: 0.9365 - gmeasure: 0.8916 - auc: 0.9147 - val_loss: 0.3541 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8257 - val_specificity: 0.8049 - val_gmeasure: 0.8152 - val_auc: 0.8928\n",
      "Epoch 57/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.2675 - binary_accuracy: 0.8717 - sensitivity: 0.8514 - specificity: 0.9276 - gmeasure: 0.8884 - auc: 0.9133 - val_loss: 0.3544 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8257 - val_specificity: 0.8049 - val_gmeasure: 0.8152 - val_auc: 0.8928\n",
      "Epoch 58/100\n",
      "600/600 [==============================] - 0s 59us/step - loss: 0.2684 - binary_accuracy: 0.8767 - sensitivity: 0.8444 - specificity: 0.9642 - gmeasure: 0.9023 - auc: 0.9195 - val_loss: 0.3508 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8257 - val_specificity: 0.8293 - val_gmeasure: 0.8275 - val_auc: 0.8930\n",
      "Epoch 59/100\n",
      "600/600 [==============================] - 0s 69us/step - loss: 0.2624 - binary_accuracy: 0.8733 - sensitivity: 0.8513 - specificity: 0.9308 - gmeasure: 0.8900 - auc: 0.9150 - val_loss: 0.3729 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8532 - val_specificity: 0.7561 - val_gmeasure: 0.8032 - val_auc: 0.8928\n",
      "Epoch 60/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.2637 - binary_accuracy: 0.8750 - sensitivity: 0.8535 - specificity: 0.9282 - gmeasure: 0.8897 - auc: 0.9136 - val_loss: 0.3442 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8257 - val_specificity: 0.8537 - val_gmeasure: 0.8396 - val_auc: 0.8844\n",
      "Epoch 61/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.2644 - binary_accuracy: 0.8767 - sensitivity: 0.8438 - specificity: 0.9640 - gmeasure: 0.9019 - auc: 0.9195 - val_loss: 0.3658 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8349 - val_specificity: 0.7561 - val_gmeasure: 0.7945 - val_auc: 0.8930\n",
      "Epoch 62/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.2655 - binary_accuracy: 0.8683 - sensitivity: 0.8512 - specificity: 0.9142 - gmeasure: 0.8818 - auc: 0.9126 - val_loss: 0.3587 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8257 - val_specificity: 0.8049 - val_gmeasure: 0.8152 - val_auc: 0.8928\n",
      "Epoch 63/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.2645 - binary_accuracy: 0.8733 - sensitivity: 0.8432 - specificity: 0.9554 - gmeasure: 0.8975 - auc: 0.9194 - val_loss: 0.3491 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8257 - val_specificity: 0.8537 - val_gmeasure: 0.8396 - val_auc: 0.8893\n",
      "Epoch 64/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.2632 - binary_accuracy: 0.8767 - sensitivity: 0.8520 - specificity: 0.9444 - gmeasure: 0.8968 - auc: 0.9166 - val_loss: 0.3785 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8532 - val_specificity: 0.7561 - val_gmeasure: 0.8032 - val_auc: 0.8963\n",
      "Epoch 65/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.2665 - binary_accuracy: 0.8717 - sensitivity: 0.8462 - specificity: 0.9415 - gmeasure: 0.8925 - auc: 0.9136 - val_loss: 0.3436 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8257 - val_specificity: 0.8537 - val_gmeasure: 0.8396 - val_auc: 0.8843\n",
      "Epoch 66/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2616 - binary_accuracy: 0.8800 - sensitivity: 0.8494 - specificity: 0.9607 - gmeasure: 0.9033 - auc: 0.9174 - val_loss: 0.3678 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8257 - val_specificity: 0.7805 - val_gmeasure: 0.8028 - val_auc: 0.8963\n",
      "Epoch 67/100\n",
      "600/600 [==============================] - 0s 60us/step - loss: 0.2615 - binary_accuracy: 0.8800 - sensitivity: 0.8537 - specificity: 0.9526 - gmeasure: 0.9017 - auc: 0.9170 - val_loss: 0.3608 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8257 - val_specificity: 0.7805 - val_gmeasure: 0.8028 - val_auc: 0.8947\n",
      "Epoch 68/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.2590 - binary_accuracy: 0.8767 - sensitivity: 0.8514 - specificity: 0.9452 - gmeasure: 0.8970 - auc: 0.9156 - val_loss: 0.3579 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8257 - val_specificity: 0.7805 - val_gmeasure: 0.8028 - val_auc: 0.8963\n",
      "Epoch 69/100\n",
      "600/600 [==============================] - 0s 69us/step - loss: 0.2585 - binary_accuracy: 0.8800 - sensitivity: 0.8514 - specificity: 0.9554 - gmeasure: 0.9017 - auc: 0.9181 - val_loss: 0.3586 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8257 - val_specificity: 0.7805 - val_gmeasure: 0.8028 - val_auc: 0.8963\n",
      "Epoch 70/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.2582 - binary_accuracy: 0.8817 - sensitivity: 0.8516 - specificity: 0.9635 - gmeasure: 0.9056 - auc: 0.9185 - val_loss: 0.3570 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8257 - val_specificity: 0.8049 - val_gmeasure: 0.8152 - val_auc: 0.8963\n",
      "Epoch 71/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.2578 - binary_accuracy: 0.8817 - sensitivity: 0.8519 - specificity: 0.9614 - gmeasure: 0.9050 - auc: 0.9178 - val_loss: 0.3627 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8257 - val_specificity: 0.7805 - val_gmeasure: 0.8028 - val_auc: 0.8965\n",
      "Epoch 72/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.2575 - binary_accuracy: 0.8783 - sensitivity: 0.8539 - specificity: 0.9449 - gmeasure: 0.8981 - auc: 0.9170 - val_loss: 0.3619 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8257 - val_specificity: 0.7805 - val_gmeasure: 0.8028 - val_auc: 0.8947\n",
      "Epoch 73/100\n",
      "600/600 [==============================] - 0s 55us/step - loss: 0.2580 - binary_accuracy: 0.8767 - sensitivity: 0.8474 - specificity: 0.9582 - gmeasure: 0.9008 - auc: 0.9190 - val_loss: 0.3589 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8257 - val_specificity: 0.8049 - val_gmeasure: 0.8152 - val_auc: 0.8965\n",
      "Epoch 74/100\n",
      "600/600 [==============================] - 0s 54us/step - loss: 0.2565 - binary_accuracy: 0.8833 - sensitivity: 0.8534 - specificity: 0.9630 - gmeasure: 0.9064 - auc: 0.9180 - val_loss: 0.3642 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8967\n",
      "Epoch 75/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.2583 - binary_accuracy: 0.8783 - sensitivity: 0.8539 - specificity: 0.9471 - gmeasure: 0.8988 - auc: 0.9170 - val_loss: 0.3544 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8257 - val_specificity: 0.8537 - val_gmeasure: 0.8396 - val_auc: 0.8898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100\n",
      "600/600 [==============================] - 0s 59us/step - loss: 0.2561 - binary_accuracy: 0.8817 - sensitivity: 0.8491 - specificity: 0.9692 - gmeasure: 0.9070 - auc: 0.9195 - val_loss: 0.3649 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8972\n",
      "Epoch 77/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.2593 - binary_accuracy: 0.8750 - sensitivity: 0.8527 - specificity: 0.9352 - gmeasure: 0.8925 - auc: 0.9149 - val_loss: 0.3504 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8257 - val_specificity: 0.8537 - val_gmeasure: 0.8396 - val_auc: 0.8921\n",
      "Epoch 78/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.2640 - binary_accuracy: 0.8767 - sensitivity: 0.8401 - specificity: 0.9759 - gmeasure: 0.9053 - auc: 0.9227 - val_loss: 0.3589 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8349 - val_specificity: 0.8293 - val_gmeasure: 0.8321 - val_auc: 0.8972\n",
      "Epoch 79/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.2640 - binary_accuracy: 0.8683 - sensitivity: 0.8530 - specificity: 0.9064 - gmeasure: 0.8790 - auc: 0.9144 - val_loss: 0.3760 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8440 - val_specificity: 0.7805 - val_gmeasure: 0.8116 - val_auc: 0.8972\n",
      "Epoch 80/100\n",
      "600/600 [==============================] - 0s 58us/step - loss: 0.2583 - binary_accuracy: 0.8800 - sensitivity: 0.8519 - specificity: 0.9564 - gmeasure: 0.9024 - auc: 0.9202 - val_loss: 0.3399 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8165 - val_specificity: 0.8537 - val_gmeasure: 0.8349 - val_auc: 0.8926\n",
      "Epoch 81/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.2579 - binary_accuracy: 0.8817 - sensitivity: 0.8467 - specificity: 0.9758 - gmeasure: 0.9089 - auc: 0.9207 - val_loss: 0.3875 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8532 - val_specificity: 0.7561 - val_gmeasure: 0.8032 - val_auc: 0.8990\n",
      "Epoch 82/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.2636 - binary_accuracy: 0.8667 - sensitivity: 0.8533 - specificity: 0.9028 - gmeasure: 0.8777 - auc: 0.9115 - val_loss: 0.3533 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8349 - val_specificity: 0.8537 - val_gmeasure: 0.8442 - val_auc: 0.8921\n",
      "Epoch 83/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.2607 - binary_accuracy: 0.8783 - sensitivity: 0.8426 - specificity: 0.9747 - gmeasure: 0.9062 - auc: 0.9211 - val_loss: 0.3403 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8257 - val_specificity: 0.8537 - val_gmeasure: 0.8396 - val_auc: 0.8944\n",
      "Epoch 84/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.2610 - binary_accuracy: 0.8700 - sensitivity: 0.8440 - specificity: 0.9365 - gmeasure: 0.8888 - auc: 0.9179 - val_loss: 0.3951 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8624 - val_specificity: 0.7561 - val_gmeasure: 0.8075 - val_auc: 0.8974\n",
      "Epoch 85/100\n",
      "600/600 [==============================] - 0s 59us/step - loss: 0.2579 - binary_accuracy: 0.8683 - sensitivity: 0.8529 - specificity: 0.9002 - gmeasure: 0.8755 - auc: 0.9153 - val_loss: 0.3460 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8349 - val_specificity: 0.8537 - val_gmeasure: 0.8442 - val_auc: 0.8886\n",
      "Epoch 86/100\n",
      "600/600 [==============================] - 0s 59us/step - loss: 0.2662 - binary_accuracy: 0.8750 - sensitivity: 0.8378 - specificity: 0.9761 - gmeasure: 0.9042 - auc: 0.9206 - val_loss: 0.3515 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8349 - val_specificity: 0.8537 - val_gmeasure: 0.8442 - val_auc: 0.8924\n",
      "Epoch 87/100\n",
      "600/600 [==============================] - 0s 60us/step - loss: 0.2575 - binary_accuracy: 0.8750 - sensitivity: 0.8515 - specificity: 0.9390 - gmeasure: 0.8941 - auc: 0.9173 - val_loss: 0.3941 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8532 - val_specificity: 0.7561 - val_gmeasure: 0.8032 - val_auc: 0.8992\n",
      "Epoch 88/100\n",
      "600/600 [==============================] - 0s 56us/step - loss: 0.2586 - binary_accuracy: 0.8767 - sensitivity: 0.8517 - specificity: 0.9485 - gmeasure: 0.8984 - auc: 0.9186 - val_loss: 0.3451 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8349 - val_specificity: 0.8537 - val_gmeasure: 0.8442 - val_auc: 0.8906\n",
      "Epoch 89/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.2561 - binary_accuracy: 0.8817 - sensitivity: 0.8464 - specificity: 0.9759 - gmeasure: 0.9088 - auc: 0.9227 - val_loss: 0.3774 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8440 - val_specificity: 0.7805 - val_gmeasure: 0.8116 - val_auc: 0.8952\n",
      "Epoch 90/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.2627 - binary_accuracy: 0.8683 - sensitivity: 0.8538 - specificity: 0.9124 - gmeasure: 0.8821 - auc: 0.9152 - val_loss: 0.3695 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8349 - val_specificity: 0.8293 - val_gmeasure: 0.8321 - val_auc: 0.8974\n",
      "Epoch 91/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.2544 - binary_accuracy: 0.8833 - sensitivity: 0.8516 - specificity: 0.9693 - gmeasure: 0.9083 - auc: 0.9211 - val_loss: 0.3441 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8349 - val_specificity: 0.8537 - val_gmeasure: 0.8442 - val_auc: 0.8946\n",
      "Epoch 92/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.2545 - binary_accuracy: 0.8850 - sensitivity: 0.8509 - specificity: 0.9748 - gmeasure: 0.9106 - auc: 0.9207 - val_loss: 0.3852 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8440 - val_specificity: 0.7561 - val_gmeasure: 0.7989 - val_auc: 0.8956\n",
      "Epoch 93/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.2547 - binary_accuracy: 0.8783 - sensitivity: 0.8537 - specificity: 0.9443 - gmeasure: 0.8978 - auc: 0.9160 - val_loss: 0.3631 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8349 - val_specificity: 0.8293 - val_gmeasure: 0.8321 - val_auc: 0.8942\n",
      "Epoch 94/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.2558 - binary_accuracy: 0.8817 - sensitivity: 0.8494 - specificity: 0.9680 - gmeasure: 0.9067 - auc: 0.9211 - val_loss: 0.3526 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8349 - val_specificity: 0.8537 - val_gmeasure: 0.8442 - val_auc: 0.8911\n",
      "Epoch 95/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.2538 - binary_accuracy: 0.8817 - sensitivity: 0.8504 - specificity: 0.9667 - gmeasure: 0.9064 - auc: 0.9179 - val_loss: 0.3800 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8440 - val_specificity: 0.7805 - val_gmeasure: 0.8116 - val_auc: 0.8961\n",
      "Epoch 96/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.2514 - binary_accuracy: 0.8800 - sensitivity: 0.8525 - specificity: 0.9568 - gmeasure: 0.9028 - auc: 0.9187 - val_loss: 0.3548 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8349 - val_specificity: 0.8293 - val_gmeasure: 0.8321 - val_auc: 0.8896\n",
      "Epoch 97/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.2525 - binary_accuracy: 0.8833 - sensitivity: 0.8494 - specificity: 0.9759 - gmeasure: 0.9103 - auc: 0.9220 - val_loss: 0.3692 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8349 - val_specificity: 0.8293 - val_gmeasure: 0.8321 - val_auc: 0.8929\n",
      "Epoch 98/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.2566 - binary_accuracy: 0.8750 - sensitivity: 0.8535 - specificity: 0.9346 - gmeasure: 0.8924 - auc: 0.9175 - val_loss: 0.3695 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8349 - val_specificity: 0.8293 - val_gmeasure: 0.8321 - val_auc: 0.8948\n",
      "Epoch 99/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.2521 - binary_accuracy: 0.8850 - sensitivity: 0.8537 - specificity: 0.9695 - gmeasure: 0.9096 - auc: 0.9215 - val_loss: 0.3446 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8349 - val_specificity: 0.8537 - val_gmeasure: 0.8442 - val_auc: 0.8919\n",
      "Epoch 100/100\n",
      "600/600 [==============================] - 0s 70us/step - loss: 0.2579 - binary_accuracy: 0.8783 - sensitivity: 0.8516 - specificity: 0.9546 - gmeasure: 0.9013 - auc: 0.9209 - val_loss: 0.3848 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8440 - val_specificity: 0.7805 - val_gmeasure: 0.8116 - val_auc: 0.8987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:151] Training end with time 6.21663236618042!\n",
      "[root    |INFO|build_network.py:79] Saved trained model weight at ./example_result/weight_1.h5 \n",
      "[root    |DEBUG|deepbiome.py:166] Save weight at ./example_result/weight_1.h5\n",
      "[root    |DEBUG|deepbiome.py:169] Save history at ./example_result/hist_1.json\n",
      "[root    |INFO|build_network.py:177] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "750/750 [==============================] - 0s 7us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:182] Evaluation end with time 0.011637210845947266!\n",
      "[root    |INFO|build_network.py:183] Evaluation: [0.27894675731658936, 0.8679999709129333, 0.8516483306884766, 0.9117646813392639, 0.881193995475769, 0.9125816822052002]\n",
      "[root    |INFO|build_network.py:177] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "250/250 [==============================] - 0s 17us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:182] Evaluation end with time 0.011641740798950195!\n",
      "[root    |INFO|build_network.py:183] Evaluation: [0.4448338449001312, 0.828000009059906, 0.8505747318267822, 0.7763158082962036, 0.8125974535942078, 0.8998034000396729]\n",
      "[root    |INFO|deepbiome.py:179] Compute time : 8.03881287574768\n",
      "[root    |INFO|deepbiome.py:180] 2 fold computing end!---------------------------------------------\n",
      "[root    |INFO|deepbiome.py:137] -------3 simulation start!----------------------------------\n",
      "[root    |INFO|readers.py:58] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:59] Construct Dataset\n",
      "[root    |INFO|readers.py:60] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:61] Load data\n",
      "[root    |INFO|deepbiome.py:147] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:148] Build network for 3 simulation\n",
      "[root    |INFO|build_network.py:513] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:514] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv\n",
      "[root    |INFO|build_network.py:518] Phylogenetic tree level list: ['Genus', 'Family', 'Order', 'Class', 'Phylum']\n",
      "[root    |INFO|build_network.py:519] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:526]      Genus: 48\n",
      "[root    |INFO|build_network.py:526]     Family: 40\n",
      "[root    |INFO|build_network.py:526]      Order: 23\n",
      "[root    |INFO|build_network.py:526]      Class: 17\n",
      "[root    |INFO|build_network.py:526]     Phylum: 9\n",
      "[root    |INFO|build_network.py:529] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:530] Phylogenetic_tree_dict info: ['Genus', 'Number', 'Order', 'Class', 'Phylum', 'Family']\n",
      "[root    |INFO|build_network.py:531] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [ Genus, Family]\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [Family,  Order]\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [ Order,  Class]\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [ Class, Phylum]\n",
      "[root    |INFO|build_network.py:554] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:570] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:571] Build network based on phylogenetic tree information\n",
      "[root    |INFO|build_network.py:572] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:648] ------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "l1_dense (Dense_with_tree)   (None, 40)                1960      \n",
      "_________________________________________________________________\n",
      "l1_activation (Activation)   (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "l2_dense (Dense_with_tree)   (None, 23)                943       \n",
      "_________________________________________________________________\n",
      "l2_activation (Activation)   (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "l3_dense (Dense_with_tree)   (None, 17)                408       \n",
      "_________________________________________________________________\n",
      "l3_activation (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "l4_dense (Dense_with_tree)   (None, 9)                 162       \n",
      "_________________________________________________________________\n",
      "l4_activation (Activation)   (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "last_dense_h (Dense)         (None, 1)                 10        \n",
      "_________________________________________________________________\n",
      "p_hat (Activation)           (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,483\n",
      "Trainable params: 3,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:57] Build Network\n",
      "[root    |INFO|build_network.py:58] Optimizer = adam\n",
      "[root    |INFO|build_network.py:59] Loss = binary_crossentropy\n",
      "[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc\n",
      "[root    |INFO|build_network.py:83] Load trained model weight at ./example_result/weight_2.h5 \n",
      "[root    |INFO|deepbiome.py:157] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:158] 3 fold computing start!----------------------------------\n",
      "[root    |INFO|build_network.py:141] Training start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 150 samples\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 1s 834us/step - loss: 0.1879 - binary_accuracy: 0.9250 - sensitivity: 0.9544 - specificity: 0.8668 - gmeasure: 0.9075 - auc: 0.9862 - val_loss: 0.2923 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.9796 - val_specificity: 0.7308 - val_gmeasure: 0.8461 - val_auc: 0.9662\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - ETA: 0s - loss: 0.1461 - binary_accuracy: 0.9550 - sensitivity: 0.9845 - specificity: 0.9014 - gmeasure: 0.9420 - auc: 0.99 - 0s 80us/step - loss: 0.1519 - binary_accuracy: 0.9483 - sensitivity: 0.9643 - specificity: 0.9154 - gmeasure: 0.9393 - auc: 0.9873 - val_loss: 0.2505 - val_binary_accuracy: 0.9067 - val_sensitivity: 0.9082 - val_specificity: 0.9038 - val_gmeasure: 0.9060 - val_auc: 0.9609\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.1656 - binary_accuracy: 0.9433 - sensitivity: 0.9377 - specificity: 0.9592 - gmeasure: 0.9481 - auc: 0.9877 - val_loss: 0.2538 - val_binary_accuracy: 0.9133 - val_sensitivity: 0.9694 - val_specificity: 0.8077 - val_gmeasure: 0.8849 - val_auc: 0.9672\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.1482 - binary_accuracy: 0.9450 - sensitivity: 0.9733 - specificity: 0.8812 - gmeasure: 0.9259 - auc: 0.9868 - val_loss: 0.2775 - val_binary_accuracy: 0.9000 - val_sensitivity: 0.9796 - val_specificity: 0.7500 - val_gmeasure: 0.8571 - val_auc: 0.9670\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1450 - binary_accuracy: 0.9500 - sensitivity: 0.9733 - specificity: 0.8987 - gmeasure: 0.9353 - auc: 0.9878 - val_loss: 0.2310 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9666\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.1410 - binary_accuracy: 0.9583 - sensitivity: 0.9612 - specificity: 0.9511 - gmeasure: 0.9561 - auc: 0.9883 - val_loss: 0.2307 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9664\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 0s 69us/step - loss: 0.1382 - binary_accuracy: 0.9550 - sensitivity: 0.9712 - specificity: 0.9200 - gmeasure: 0.9451 - auc: 0.9884 - val_loss: 0.2489 - val_binary_accuracy: 0.9133 - val_sensitivity: 0.9694 - val_specificity: 0.8077 - val_gmeasure: 0.8849 - val_auc: 0.9676\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 0s 70us/step - loss: 0.1369 - binary_accuracy: 0.9500 - sensitivity: 0.9731 - specificity: 0.8966 - gmeasure: 0.9338 - auc: 0.9875 - val_loss: 0.2441 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9694 - val_specificity: 0.8269 - val_gmeasure: 0.8953 - val_auc: 0.9674\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.1343 - binary_accuracy: 0.9500 - sensitivity: 0.9703 - specificity: 0.9017 - gmeasure: 0.9352 - auc: 0.9876 - val_loss: 0.2330 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9674\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1350 - binary_accuracy: 0.9617 - sensitivity: 0.9714 - specificity: 0.9408 - gmeasure: 0.9560 - auc: 0.9883 - val_loss: 0.2319 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9670\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1332 - binary_accuracy: 0.9567 - sensitivity: 0.9703 - specificity: 0.9255 - gmeasure: 0.9474 - auc: 0.9880 - val_loss: 0.2404 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9694 - val_specificity: 0.8269 - val_gmeasure: 0.8953 - val_auc: 0.9680\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 0s 59us/step - loss: 0.1329 - binary_accuracy: 0.9517 - sensitivity: 0.9706 - specificity: 0.9076 - gmeasure: 0.9378 - auc: 0.9882 - val_loss: 0.2370 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9592 - val_specificity: 0.8462 - val_gmeasure: 0.9009 - val_auc: 0.9686\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1322 - binary_accuracy: 0.9517 - sensitivity: 0.9715 - specificity: 0.9110 - gmeasure: 0.9407 - auc: 0.9887 - val_loss: 0.2333 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9688\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.1319 - binary_accuracy: 0.9567 - sensitivity: 0.9707 - specificity: 0.9268 - gmeasure: 0.9485 - auc: 0.9884 - val_loss: 0.2331 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9686\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 0s 70us/step - loss: 0.1304 - binary_accuracy: 0.9567 - sensitivity: 0.9711 - specificity: 0.9243 - gmeasure: 0.9473 - auc: 0.9890 - val_loss: 0.2385 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9694 - val_specificity: 0.8269 - val_gmeasure: 0.8953 - val_auc: 0.9692\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1301 - binary_accuracy: 0.9517 - sensitivity: 0.9708 - specificity: 0.9109 - gmeasure: 0.9403 - auc: 0.9885 - val_loss: 0.2313 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9684\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.1294 - binary_accuracy: 0.9533 - sensitivity: 0.9713 - specificity: 0.9154 - gmeasure: 0.9429 - auc: 0.9892 - val_loss: 0.2346 - val_binary_accuracy: 0.9267 - val_sensitivity: 0.9592 - val_specificity: 0.8654 - val_gmeasure: 0.9111 - val_auc: 0.9694\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1283 - binary_accuracy: 0.9533 - sensitivity: 0.9710 - specificity: 0.9164 - gmeasure: 0.9432 - auc: 0.9894 - val_loss: 0.2329 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9690\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1276 - binary_accuracy: 0.9600 - sensitivity: 0.9710 - specificity: 0.9362 - gmeasure: 0.9534 - auc: 0.9894 - val_loss: 0.2283 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9692\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.1277 - binary_accuracy: 0.9633 - sensitivity: 0.9709 - specificity: 0.9460 - gmeasure: 0.9582 - auc: 0.9897 - val_loss: 0.2328 - val_binary_accuracy: 0.9267 - val_sensitivity: 0.9592 - val_specificity: 0.8654 - val_gmeasure: 0.9111 - val_auc: 0.9696\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 0s 56us/step - loss: 0.1263 - binary_accuracy: 0.9533 - sensitivity: 0.9710 - specificity: 0.9134 - gmeasure: 0.9417 - auc: 0.9895 - val_loss: 0.2377 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9694 - val_specificity: 0.8269 - val_gmeasure: 0.8953 - val_auc: 0.9706\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 0s 59us/step - loss: 0.1266 - binary_accuracy: 0.9533 - sensitivity: 0.9731 - specificity: 0.9060 - gmeasure: 0.9386 - auc: 0.9895 - val_loss: 0.2287 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9696\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 0s 54us/step - loss: 0.1248 - binary_accuracy: 0.9667 - sensitivity: 0.9710 - specificity: 0.9564 - gmeasure: 0.9635 - auc: 0.9893 - val_loss: 0.2244 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9698\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.1246 - binary_accuracy: 0.9583 - sensitivity: 0.9707 - specificity: 0.9308 - gmeasure: 0.9505 - auc: 0.9891 - val_loss: 0.2326 - val_binary_accuracy: 0.9333 - val_sensitivity: 0.9694 - val_specificity: 0.8654 - val_gmeasure: 0.9159 - val_auc: 0.9702\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 0s 68us/step - loss: 0.1233 - binary_accuracy: 0.9550 - sensitivity: 0.9710 - specificity: 0.9199 - gmeasure: 0.9450 - auc: 0.9899 - val_loss: 0.2269 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9704\n",
      "Epoch 26/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.1227 - binary_accuracy: 0.9633 - sensitivity: 0.9708 - specificity: 0.9471 - gmeasure: 0.9588 - auc: 0.9889 - val_loss: 0.2242 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9708\n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.1253 - binary_accuracy: 0.9550 - sensitivity: 0.9708 - specificity: 0.9212 - gmeasure: 0.9451 - auc: 0.9896 - val_loss: 0.2300 - val_binary_accuracy: 0.9267 - val_sensitivity: 0.9592 - val_specificity: 0.8654 - val_gmeasure: 0.9111 - val_auc: 0.9714\n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.1212 - binary_accuracy: 0.9650 - sensitivity: 0.9707 - specificity: 0.9510 - gmeasure: 0.9607 - auc: 0.9891 - val_loss: 0.2212 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9712\n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.1217 - binary_accuracy: 0.9583 - sensitivity: 0.9714 - specificity: 0.9315 - gmeasure: 0.9509 - auc: 0.9897 - val_loss: 0.2315 - val_binary_accuracy: 0.9333 - val_sensitivity: 0.9694 - val_specificity: 0.8654 - val_gmeasure: 0.9159 - val_auc: 0.9715\n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.1222 - binary_accuracy: 0.9583 - sensitivity: 0.9715 - specificity: 0.9335 - gmeasure: 0.9520 - auc: 0.9905 - val_loss: 0.2209 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9710\n",
      "Epoch 31/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1185 - binary_accuracy: 0.9617 - sensitivity: 0.9705 - specificity: 0.9386 - gmeasure: 0.9542 - auc: 0.9892 - val_loss: 0.2304 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9796 - val_specificity: 0.8654 - val_gmeasure: 0.9207 - val_auc: 0.9715\n",
      "Epoch 32/100\n",
      "600/600 [==============================] - 0s 72us/step - loss: 0.1223 - binary_accuracy: 0.9533 - sensitivity: 0.9754 - specificity: 0.9088 - gmeasure: 0.9409 - auc: 0.9895 - val_loss: 0.2182 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9687\n",
      "Epoch 33/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.1177 - binary_accuracy: 0.9667 - sensitivity: 0.9709 - specificity: 0.9569 - gmeasure: 0.9638 - auc: 0.9906 - val_loss: 0.2128 - val_binary_accuracy: 0.9267 - val_sensitivity: 0.9490 - val_specificity: 0.8846 - val_gmeasure: 0.9162 - val_auc: 0.9697\n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.1183 - binary_accuracy: 0.9650 - sensitivity: 0.9703 - specificity: 0.9497 - gmeasure: 0.9596 - auc: 0.9897 - val_loss: 0.2282 - val_binary_accuracy: 0.9333 - val_sensitivity: 0.9796 - val_specificity: 0.8462 - val_gmeasure: 0.9104 - val_auc: 0.9698\n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.1176 - binary_accuracy: 0.9600 - sensitivity: 0.9709 - specificity: 0.9365 - gmeasure: 0.9532 - auc: 0.9908 - val_loss: 0.2154 - val_binary_accuracy: 0.9267 - val_sensitivity: 0.9592 - val_specificity: 0.8654 - val_gmeasure: 0.9111 - val_auc: 0.9698\n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.1155 - binary_accuracy: 0.9617 - sensitivity: 0.9704 - specificity: 0.9378 - gmeasure: 0.9536 - auc: 0.9895 - val_loss: 0.2132 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9592 - val_specificity: 0.8462 - val_gmeasure: 0.9009 - val_auc: 0.9702\n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 0s 58us/step - loss: 0.1187 - binary_accuracy: 0.9583 - sensitivity: 0.9733 - specificity: 0.9281 - gmeasure: 0.9502 - auc: 0.9896 - val_loss: 0.2161 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9592 - val_specificity: 0.8462 - val_gmeasure: 0.9009 - val_auc: 0.9704\n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 0s 56us/step - loss: 0.1184 - binary_accuracy: 0.9600 - sensitivity: 0.9637 - specificity: 0.9518 - gmeasure: 0.9576 - auc: 0.9906 - val_loss: 0.2034 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9490 - val_specificity: 0.9231 - val_gmeasure: 0.9359 - val_auc: 0.9709\n",
      "Epoch 39/100\n",
      "600/600 [==============================] - 0s 60us/step - loss: 0.1147 - binary_accuracy: 0.9617 - sensitivity: 0.9668 - specificity: 0.9520 - gmeasure: 0.9592 - auc: 0.9914 - val_loss: 0.2295 - val_binary_accuracy: 0.9133 - val_sensitivity: 0.9796 - val_specificity: 0.7885 - val_gmeasure: 0.8788 - val_auc: 0.9714\n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.1160 - binary_accuracy: 0.9567 - sensitivity: 0.9758 - specificity: 0.9147 - gmeasure: 0.9447 - auc: 0.9901 - val_loss: 0.2060 - val_binary_accuracy: 0.9333 - val_sensitivity: 0.9490 - val_specificity: 0.9038 - val_gmeasure: 0.9261 - val_auc: 0.9715\n",
      "Epoch 41/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.1118 - binary_accuracy: 0.9683 - sensitivity: 0.9708 - specificity: 0.9606 - gmeasure: 0.9655 - auc: 0.9904 - val_loss: 0.2039 - val_binary_accuracy: 0.9333 - val_sensitivity: 0.9490 - val_specificity: 0.9038 - val_gmeasure: 0.9261 - val_auc: 0.9716\n",
      "Epoch 42/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.1122 - binary_accuracy: 0.9667 - sensitivity: 0.9732 - specificity: 0.9499 - gmeasure: 0.9612 - auc: 0.9905 - val_loss: 0.2099 - val_binary_accuracy: 0.9333 - val_sensitivity: 0.9694 - val_specificity: 0.8654 - val_gmeasure: 0.9159 - val_auc: 0.9719\n",
      "Epoch 43/100\n",
      "600/600 [==============================] - 0s 57us/step - loss: 0.1133 - binary_accuracy: 0.9633 - sensitivity: 0.9685 - specificity: 0.9513 - gmeasure: 0.9597 - auc: 0.9909 - val_loss: 0.2065 - val_binary_accuracy: 0.9333 - val_sensitivity: 0.9592 - val_specificity: 0.8846 - val_gmeasure: 0.9211 - val_auc: 0.9721\n",
      "Epoch 44/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.1096 - binary_accuracy: 0.9617 - sensitivity: 0.9731 - specificity: 0.9349 - gmeasure: 0.9538 - auc: 0.9908 - val_loss: 0.2145 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9796 - val_specificity: 0.8654 - val_gmeasure: 0.9207 - val_auc: 0.9719\n",
      "Epoch 45/100\n",
      "600/600 [==============================] - 0s 57us/step - loss: 0.1101 - binary_accuracy: 0.9667 - sensitivity: 0.9758 - specificity: 0.9450 - gmeasure: 0.9602 - auc: 0.9903 - val_loss: 0.2012 - val_binary_accuracy: 0.9267 - val_sensitivity: 0.9490 - val_specificity: 0.8846 - val_gmeasure: 0.9162 - val_auc: 0.9721\n",
      "Epoch 46/100\n",
      "600/600 [==============================] - 0s 53us/step - loss: 0.1084 - binary_accuracy: 0.9683 - sensitivity: 0.9708 - specificity: 0.9620 - gmeasure: 0.9664 - auc: 0.9907 - val_loss: 0.2081 - val_binary_accuracy: 0.9333 - val_sensitivity: 0.9694 - val_specificity: 0.8654 - val_gmeasure: 0.9159 - val_auc: 0.9721\n",
      "Epoch 47/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.1084 - binary_accuracy: 0.9600 - sensitivity: 0.9784 - specificity: 0.9191 - gmeasure: 0.9482 - auc: 0.9903 - val_loss: 0.2067 - val_binary_accuracy: 0.9333 - val_sensitivity: 0.9694 - val_specificity: 0.8654 - val_gmeasure: 0.9159 - val_auc: 0.9723\n",
      "Epoch 48/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.1103 - binary_accuracy: 0.9683 - sensitivity: 0.9713 - specificity: 0.9622 - gmeasure: 0.9665 - auc: 0.9912 - val_loss: 0.1982 - val_binary_accuracy: 0.9267 - val_sensitivity: 0.9490 - val_specificity: 0.8846 - val_gmeasure: 0.9162 - val_auc: 0.9721\n",
      "Epoch 49/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.1055 - binary_accuracy: 0.9700 - sensitivity: 0.9758 - specificity: 0.9571 - gmeasure: 0.9663 - auc: 0.9909 - val_loss: 0.2228 - val_binary_accuracy: 0.9333 - val_sensitivity: 0.9796 - val_specificity: 0.8462 - val_gmeasure: 0.9104 - val_auc: 0.9723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.1084 - binary_accuracy: 0.9600 - sensitivity: 0.9784 - specificity: 0.9193 - gmeasure: 0.9484 - auc: 0.9906 - val_loss: 0.2001 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9694 - val_specificity: 0.8846 - val_gmeasure: 0.9260 - val_auc: 0.9723\n",
      "Epoch 51/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.1085 - binary_accuracy: 0.9667 - sensitivity: 0.9693 - specificity: 0.9625 - gmeasure: 0.9657 - auc: 0.9919 - val_loss: 0.2050 - val_binary_accuracy: 0.9333 - val_sensitivity: 0.9694 - val_specificity: 0.8654 - val_gmeasure: 0.9159 - val_auc: 0.9725\n",
      "Epoch 52/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1044 - binary_accuracy: 0.9683 - sensitivity: 0.9780 - specificity: 0.9455 - gmeasure: 0.9615 - auc: 0.9906 - val_loss: 0.2079 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9796 - val_specificity: 0.8654 - val_gmeasure: 0.9207 - val_auc: 0.9727\n",
      "Epoch 53/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.1042 - binary_accuracy: 0.9700 - sensitivity: 0.9757 - specificity: 0.9585 - gmeasure: 0.9670 - auc: 0.9909 - val_loss: 0.1987 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9694 - val_specificity: 0.8846 - val_gmeasure: 0.9260 - val_auc: 0.9725\n",
      "Epoch 54/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.1045 - binary_accuracy: 0.9683 - sensitivity: 0.9757 - specificity: 0.9519 - gmeasure: 0.9637 - auc: 0.9906 - val_loss: 0.2028 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9694 - val_specificity: 0.8846 - val_gmeasure: 0.9260 - val_auc: 0.9725\n",
      "Epoch 55/100\n",
      "600/600 [==============================] - 0s 69us/step - loss: 0.1041 - binary_accuracy: 0.9683 - sensitivity: 0.9755 - specificity: 0.9535 - gmeasure: 0.9642 - auc: 0.9914 - val_loss: 0.1998 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9694 - val_specificity: 0.8846 - val_gmeasure: 0.9260 - val_auc: 0.9725\n",
      "Epoch 56/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.1085 - binary_accuracy: 0.9633 - sensitivity: 0.9613 - specificity: 0.9681 - gmeasure: 0.9646 - auc: 0.9910 - val_loss: 0.1980 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9694 - val_specificity: 0.8846 - val_gmeasure: 0.9260 - val_auc: 0.9725\n",
      "Epoch 57/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.1055 - binary_accuracy: 0.9683 - sensitivity: 0.9778 - specificity: 0.9434 - gmeasure: 0.9601 - auc: 0.9911 - val_loss: 0.2254 - val_binary_accuracy: 0.9333 - val_sensitivity: 0.9796 - val_specificity: 0.8462 - val_gmeasure: 0.9104 - val_auc: 0.9731\n",
      "Epoch 58/100\n",
      "600/600 [==============================] - 0s 59us/step - loss: 0.1016 - binary_accuracy: 0.9667 - sensitivity: 0.9758 - specificity: 0.9463 - gmeasure: 0.9609 - auc: 0.9910 - val_loss: 0.1909 - val_binary_accuracy: 0.9267 - val_sensitivity: 0.9388 - val_specificity: 0.9038 - val_gmeasure: 0.9211 - val_auc: 0.9729\n",
      "Epoch 59/100\n",
      "600/600 [==============================] - 0s 60us/step - loss: 0.1036 - binary_accuracy: 0.9683 - sensitivity: 0.9685 - specificity: 0.9678 - gmeasure: 0.9682 - auc: 0.9917 - val_loss: 0.2026 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9694 - val_specificity: 0.8846 - val_gmeasure: 0.9260 - val_auc: 0.9739\n",
      "Epoch 60/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.0999 - binary_accuracy: 0.9683 - sensitivity: 0.9784 - specificity: 0.9476 - gmeasure: 0.9628 - auc: 0.9913 - val_loss: 0.2041 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9796 - val_specificity: 0.8654 - val_gmeasure: 0.9207 - val_auc: 0.9741\n",
      "Epoch 61/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.1007 - binary_accuracy: 0.9667 - sensitivity: 0.9732 - specificity: 0.9515 - gmeasure: 0.9621 - auc: 0.9915 - val_loss: 0.1937 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9694 - val_specificity: 0.8846 - val_gmeasure: 0.9260 - val_auc: 0.9739\n",
      "Epoch 62/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.0992 - binary_accuracy: 0.9683 - sensitivity: 0.9758 - specificity: 0.9518 - gmeasure: 0.9636 - auc: 0.9913 - val_loss: 0.2018 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9796 - val_specificity: 0.8654 - val_gmeasure: 0.9207 - val_auc: 0.9743\n",
      "Epoch 63/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.0980 - binary_accuracy: 0.9717 - sensitivity: 0.9788 - specificity: 0.9575 - gmeasure: 0.9680 - auc: 0.9908 - val_loss: 0.1907 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9694 - val_specificity: 0.9038 - val_gmeasure: 0.9360 - val_auc: 0.9741\n",
      "Epoch 64/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.0984 - binary_accuracy: 0.9733 - sensitivity: 0.9733 - specificity: 0.9752 - gmeasure: 0.9742 - auc: 0.9917 - val_loss: 0.1972 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9694 - val_specificity: 0.8846 - val_gmeasure: 0.9260 - val_auc: 0.9753\n",
      "Epoch 65/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.1009 - binary_accuracy: 0.9617 - sensitivity: 0.9783 - specificity: 0.9268 - gmeasure: 0.9521 - auc: 0.9908 - val_loss: 0.2003 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9796 - val_specificity: 0.8654 - val_gmeasure: 0.9207 - val_auc: 0.9757\n",
      "Epoch 66/100\n",
      "600/600 [==============================] - 0s 67us/step - loss: 0.0972 - binary_accuracy: 0.9733 - sensitivity: 0.9758 - specificity: 0.9678 - gmeasure: 0.9718 - auc: 0.9920 - val_loss: 0.1852 - val_binary_accuracy: 0.9267 - val_sensitivity: 0.9388 - val_specificity: 0.9038 - val_gmeasure: 0.9211 - val_auc: 0.9755\n",
      "Epoch 67/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.0992 - binary_accuracy: 0.9650 - sensitivity: 0.9655 - specificity: 0.9623 - gmeasure: 0.9636 - auc: 0.9923 - val_loss: 0.2118 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9796 - val_specificity: 0.8654 - val_gmeasure: 0.9207 - val_auc: 0.9757\n",
      "Epoch 68/100\n",
      "600/600 [==============================] - 0s 56us/step - loss: 0.0971 - binary_accuracy: 0.9683 - sensitivity: 0.9782 - specificity: 0.9474 - gmeasure: 0.9626 - auc: 0.9915 - val_loss: 0.1889 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9694 - val_specificity: 0.9038 - val_gmeasure: 0.9360 - val_auc: 0.9761\n",
      "Epoch 69/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.0959 - binary_accuracy: 0.9750 - sensitivity: 0.9761 - specificity: 0.9721 - gmeasure: 0.9741 - auc: 0.9921 - val_loss: 0.1938 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9694 - val_specificity: 0.8846 - val_gmeasure: 0.9260 - val_auc: 0.9763\n",
      "Epoch 70/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.0952 - binary_accuracy: 0.9700 - sensitivity: 0.9783 - specificity: 0.9524 - gmeasure: 0.9651 - auc: 0.9919 - val_loss: 0.1994 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9796 - val_specificity: 0.8654 - val_gmeasure: 0.9207 - val_auc: 0.9761\n",
      "Epoch 71/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.0945 - binary_accuracy: 0.9733 - sensitivity: 0.9783 - specificity: 0.9623 - gmeasure: 0.9701 - auc: 0.9918 - val_loss: 0.1876 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9694 - val_specificity: 0.9038 - val_gmeasure: 0.9360 - val_auc: 0.9759\n",
      "Epoch 72/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.0963 - binary_accuracy: 0.9717 - sensitivity: 0.9758 - specificity: 0.9626 - gmeasure: 0.9690 - auc: 0.9922 - val_loss: 0.2040 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9796 - val_specificity: 0.8654 - val_gmeasure: 0.9207 - val_auc: 0.9765\n",
      "Epoch 73/100\n",
      "600/600 [==============================] - 0s 54us/step - loss: 0.0941 - binary_accuracy: 0.9733 - sensitivity: 0.9782 - specificity: 0.9628 - gmeasure: 0.9704 - auc: 0.9920 - val_loss: 0.1842 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9592 - val_specificity: 0.9038 - val_gmeasure: 0.9311 - val_auc: 0.9767\n",
      "Epoch 74/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.0931 - binary_accuracy: 0.9767 - sensitivity: 0.9783 - specificity: 0.9722 - gmeasure: 0.9752 - auc: 0.9919 - val_loss: 0.1995 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9796 - val_specificity: 0.8654 - val_gmeasure: 0.9207 - val_auc: 0.9766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100\n",
      "600/600 [==============================] - 0s 57us/step - loss: 0.0926 - binary_accuracy: 0.9700 - sensitivity: 0.9783 - specificity: 0.9522 - gmeasure: 0.9651 - auc: 0.9920 - val_loss: 0.1873 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9694 - val_specificity: 0.9038 - val_gmeasure: 0.9360 - val_auc: 0.9767\n",
      "Epoch 76/100\n",
      "600/600 [==============================] - 0s 58us/step - loss: 0.0921 - binary_accuracy: 0.9750 - sensitivity: 0.9784 - specificity: 0.9679 - gmeasure: 0.9731 - auc: 0.9926 - val_loss: 0.1861 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9694 - val_specificity: 0.9038 - val_gmeasure: 0.9360 - val_auc: 0.9766\n",
      "Epoch 77/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.0913 - binary_accuracy: 0.9783 - sensitivity: 0.9780 - specificity: 0.9784 - gmeasure: 0.9782 - auc: 0.9919 - val_loss: 0.1935 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9796 - val_specificity: 0.8846 - val_gmeasure: 0.9309 - val_auc: 0.9768\n",
      "Epoch 78/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.0906 - binary_accuracy: 0.9750 - sensitivity: 0.9788 - specificity: 0.9670 - gmeasure: 0.9729 - auc: 0.9920 - val_loss: 0.1912 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9796 - val_specificity: 0.8846 - val_gmeasure: 0.9309 - val_auc: 0.9769\n",
      "Epoch 79/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.0908 - binary_accuracy: 0.9767 - sensitivity: 0.9784 - specificity: 0.9730 - gmeasure: 0.9757 - auc: 0.9915 - val_loss: 0.1884 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9694 - val_specificity: 0.8846 - val_gmeasure: 0.9260 - val_auc: 0.9769\n",
      "Epoch 80/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.0900 - binary_accuracy: 0.9750 - sensitivity: 0.9782 - specificity: 0.9685 - gmeasure: 0.9733 - auc: 0.9920 - val_loss: 0.1919 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9796 - val_specificity: 0.8846 - val_gmeasure: 0.9309 - val_auc: 0.9768\n",
      "Epoch 81/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.0912 - binary_accuracy: 0.9750 - sensitivity: 0.9764 - specificity: 0.9734 - gmeasure: 0.9749 - auc: 0.9919 - val_loss: 0.1861 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9694 - val_specificity: 0.9038 - val_gmeasure: 0.9360 - val_auc: 0.9768\n",
      "Epoch 82/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.0893 - binary_accuracy: 0.9717 - sensitivity: 0.9785 - specificity: 0.9573 - gmeasure: 0.9678 - auc: 0.9916 - val_loss: 0.1963 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9796 - val_specificity: 0.8654 - val_gmeasure: 0.9207 - val_auc: 0.9770\n",
      "Epoch 83/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.0908 - binary_accuracy: 0.9783 - sensitivity: 0.9785 - specificity: 0.9787 - gmeasure: 0.9785 - auc: 0.9929 - val_loss: 0.1826 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9694 - val_specificity: 0.9038 - val_gmeasure: 0.9360 - val_auc: 0.9769\n",
      "Epoch 84/100\n",
      "600/600 [==============================] - 0s 58us/step - loss: 0.0879 - binary_accuracy: 0.9767 - sensitivity: 0.9784 - specificity: 0.9735 - gmeasure: 0.9759 - auc: 0.9925 - val_loss: 0.1972 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9796 - val_specificity: 0.8654 - val_gmeasure: 0.9207 - val_auc: 0.9770\n",
      "Epoch 85/100\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.0879 - binary_accuracy: 0.9750 - sensitivity: 0.9782 - specificity: 0.9679 - gmeasure: 0.9731 - auc: 0.9919 - val_loss: 0.1840 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9694 - val_specificity: 0.9038 - val_gmeasure: 0.9360 - val_auc: 0.9769\n",
      "Epoch 86/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.0880 - binary_accuracy: 0.9783 - sensitivity: 0.9774 - specificity: 0.9804 - gmeasure: 0.9789 - auc: 0.9919 - val_loss: 0.1826 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9694 - val_specificity: 0.9038 - val_gmeasure: 0.9360 - val_auc: 0.9771\n",
      "Epoch 87/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.0867 - binary_accuracy: 0.9767 - sensitivity: 0.9783 - specificity: 0.9734 - gmeasure: 0.9758 - auc: 0.9929 - val_loss: 0.1958 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9796 - val_specificity: 0.8654 - val_gmeasure: 0.9207 - val_auc: 0.9772\n",
      "Epoch 88/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.0876 - binary_accuracy: 0.9733 - sensitivity: 0.9778 - specificity: 0.9601 - gmeasure: 0.9687 - auc: 0.9927 - val_loss: 0.1793 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9694 - val_specificity: 0.9038 - val_gmeasure: 0.9360 - val_auc: 0.9774\n",
      "Epoch 89/100\n",
      "600/600 [==============================] - 0s 58us/step - loss: 0.0866 - binary_accuracy: 0.9767 - sensitivity: 0.9758 - specificity: 0.9788 - gmeasure: 0.9772 - auc: 0.9927 - val_loss: 0.1900 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9796 - val_specificity: 0.8846 - val_gmeasure: 0.9309 - val_auc: 0.9776\n",
      "Epoch 90/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.0858 - binary_accuracy: 0.9767 - sensitivity: 0.9784 - specificity: 0.9738 - gmeasure: 0.9760 - auc: 0.9930 - val_loss: 0.1821 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9694 - val_specificity: 0.9038 - val_gmeasure: 0.9360 - val_auc: 0.9772\n",
      "Epoch 91/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.0850 - binary_accuracy: 0.9783 - sensitivity: 0.9782 - specificity: 0.9785 - gmeasure: 0.9783 - auc: 0.9928 - val_loss: 0.1860 - val_binary_accuracy: 0.9533 - val_sensitivity: 0.9796 - val_specificity: 0.9038 - val_gmeasure: 0.9410 - val_auc: 0.9771\n",
      "Epoch 92/100\n",
      "600/600 [==============================] - 0s 57us/step - loss: 0.0846 - binary_accuracy: 0.9767 - sensitivity: 0.9781 - specificity: 0.9741 - gmeasure: 0.9761 - auc: 0.9928 - val_loss: 0.1906 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9796 - val_specificity: 0.8846 - val_gmeasure: 0.9309 - val_auc: 0.9772\n",
      "Epoch 93/100\n",
      "600/600 [==============================] - 0s 61us/step - loss: 0.0841 - binary_accuracy: 0.9783 - sensitivity: 0.9781 - specificity: 0.9778 - gmeasure: 0.9779 - auc: 0.9930 - val_loss: 0.1834 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9694 - val_specificity: 0.9038 - val_gmeasure: 0.9360 - val_auc: 0.9774\n",
      "Epoch 94/100\n",
      "600/600 [==============================] - 0s 70us/step - loss: 0.0839 - binary_accuracy: 0.9783 - sensitivity: 0.9788 - specificity: 0.9790 - gmeasure: 0.9789 - auc: 0.9927 - val_loss: 0.1847 - val_binary_accuracy: 0.9533 - val_sensitivity: 0.9796 - val_specificity: 0.9038 - val_gmeasure: 0.9410 - val_auc: 0.9774\n",
      "Epoch 95/100\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.0842 - binary_accuracy: 0.9767 - sensitivity: 0.9784 - specificity: 0.9735 - gmeasure: 0.9759 - auc: 0.9928 - val_loss: 0.1863 - val_binary_accuracy: 0.9533 - val_sensitivity: 0.9796 - val_specificity: 0.9038 - val_gmeasure: 0.9410 - val_auc: 0.9775\n",
      "Epoch 96/100\n",
      "600/600 [==============================] - 0s 64us/step - loss: 0.0835 - binary_accuracy: 0.9767 - sensitivity: 0.9781 - specificity: 0.9722 - gmeasure: 0.9751 - auc: 0.9927 - val_loss: 0.1785 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9694 - val_specificity: 0.9038 - val_gmeasure: 0.9360 - val_auc: 0.9780\n",
      "Epoch 97/100\n",
      "600/600 [==============================] - 0s 65us/step - loss: 0.0830 - binary_accuracy: 0.9783 - sensitivity: 0.9789 - specificity: 0.9789 - gmeasure: 0.9789 - auc: 0.9922 - val_loss: 0.1916 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9796 - val_specificity: 0.8654 - val_gmeasure: 0.9207 - val_auc: 0.9782\n",
      "Epoch 98/100\n",
      "600/600 [==============================] - 0s 59us/step - loss: 0.0845 - binary_accuracy: 0.9750 - sensitivity: 0.9789 - specificity: 0.9705 - gmeasure: 0.9745 - auc: 0.9933 - val_loss: 0.1787 - val_binary_accuracy: 0.9533 - val_sensitivity: 0.9796 - val_specificity: 0.9038 - val_gmeasure: 0.9410 - val_auc: 0.9780\n",
      "Epoch 99/100\n",
      "600/600 [==============================] - 0s 62us/step - loss: 0.0830 - binary_accuracy: 0.9767 - sensitivity: 0.9782 - specificity: 0.9712 - gmeasure: 0.9746 - auc: 0.9927 - val_loss: 0.1866 - val_binary_accuracy: 0.9533 - val_sensitivity: 0.9796 - val_specificity: 0.9038 - val_gmeasure: 0.9410 - val_auc: 0.9780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100\n",
      "600/600 [==============================] - 0s 66us/step - loss: 0.0818 - binary_accuracy: 0.9783 - sensitivity: 0.9781 - specificity: 0.9780 - gmeasure: 0.9780 - auc: 0.9928 - val_loss: 0.1796 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9694 - val_specificity: 0.9038 - val_gmeasure: 0.9360 - val_auc: 0.9779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:151] Training end with time 6.0595057010650635!\n",
      "[root    |INFO|build_network.py:79] Saved trained model weight at ./example_result/weight_2.h5 \n",
      "[root    |DEBUG|deepbiome.py:166] Save weight at ./example_result/weight_2.h5\n",
      "[root    |DEBUG|deepbiome.py:169] Save history at ./example_result/hist_2.json\n",
      "[root    |INFO|build_network.py:177] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "750/750 [==============================] - 0s 7us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:182] Evaluation end with time 0.013599395751953125!\n",
      "[root    |INFO|build_network.py:183] Evaluation: [0.10130026936531067, 0.972000002861023, 0.976516604423523, 0.9623430967330933, 0.9694039225578308, 0.989936888217926]\n",
      "[root    |INFO|build_network.py:177] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "250/250 [==============================] - 0s 19us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:182] Evaluation end with time 0.011395692825317383!\n",
      "[root    |INFO|build_network.py:183] Evaluation: [0.2500866949558258, 0.9120000004768372, 0.932584285736084, 0.8611111044883728, 0.8961353898048401, 0.958996593952179]\n",
      "[root    |INFO|deepbiome.py:179] Compute time : 7.9529359340667725\n",
      "[root    |INFO|deepbiome.py:180] 3 fold computing end!---------------------------------------------\n",
      "[root    |INFO|deepbiome.py:183] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:185] Train Evaluation : ['loss' 'binary_accuracy' 'sensitivity' 'specificity' 'gmeasure' 'auc']\n",
      "[root    |INFO|deepbiome.py:188]       mean : [0.1876763  0.92355555 0.92663413 0.91924282 0.92254825 0.95947075]\n",
      "[root    |INFO|deepbiome.py:189]        std : [0.07260641 0.04275455 0.05397934 0.0325704  0.03622105 0.0336476 ]\n",
      "[root    |INFO|deepbiome.py:190] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:192] Test Evaluation : ['loss' 'binary_accuracy' 'sensitivity' 'specificity' 'gmeasure' 'auc']\n",
      "[root    |INFO|deepbiome.py:195]       mean : [0.32584215 0.87866668 0.90602342 0.81741391 0.86051089 0.93644808]\n",
      "[root    |INFO|deepbiome.py:196]        std : [0.08518076 0.03641733 0.03921965 0.03466629 0.03519384 0.02614045]\n",
      "[root    |INFO|deepbiome.py:197] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:206] Total Computing Ended\n",
      "[root    |INFO|deepbiome.py:207] -----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_evaluation, train_evaluation, network = deepbiome.deepbiome_train(log, network_info, path_info, number_of_fold=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the history plot again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6zwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4fb4397c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('./%s/hist_0.json' % path_info['model_info']['model_dir'], 'r') as f:\n",
    "    history = json.load(f)\n",
    "    \n",
    "plt.plot(history['val_loss'], label='Validation')\n",
    "plt.plot(history['loss'], label='Training')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load the pretrained network for testing\n",
    "\n",
    "If you want to test the trained model, you can use the `deepbiome_test` function. If you use the index file, this function provide the evaluation using test index (index set not included in the index file) for each fold. If not, this function provide the evaluation using the whole samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|deepbiome.py:259] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:289] Test Evaluation : ['loss' 'binary_accuracy' 'sensitivity' 'specificity' 'gmeasure' 'auc']\n",
      "[root    |INFO|deepbiome.py:291] -------1 fold test start!----------------------------------\n",
      "[root    |INFO|readers.py:58] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:59] Construct Dataset\n",
      "[root    |INFO|readers.py:60] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:61] Load data\n",
      "[root    |INFO|deepbiome.py:301] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:302] Build network for 1 fold testing\n",
      "[root    |INFO|build_network.py:513] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:514] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv\n",
      "[root    |INFO|build_network.py:518] Phylogenetic tree level list: ['Genus', 'Family', 'Order', 'Class', 'Phylum']\n",
      "[root    |INFO|build_network.py:519] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:526]      Genus: 48\n",
      "[root    |INFO|build_network.py:526]     Family: 40\n",
      "[root    |INFO|build_network.py:526]      Order: 23\n",
      "[root    |INFO|build_network.py:526]      Class: 17\n",
      "[root    |INFO|build_network.py:526]     Phylum: 9\n",
      "[root    |INFO|build_network.py:529] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:530] Phylogenetic_tree_dict info: ['Genus', 'Number', 'Order', 'Class', 'Phylum', 'Family']\n",
      "[root    |INFO|build_network.py:531] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [ Genus, Family]\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [Family,  Order]\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [ Order,  Class]\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [ Class, Phylum]\n",
      "[root    |INFO|build_network.py:554] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:570] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:571] Build network based on phylogenetic tree information\n",
      "[root    |INFO|build_network.py:572] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:648] ------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "l1_dense (Dense_with_tree)   (None, 40)                1960      \n",
      "_________________________________________________________________\n",
      "l1_activation (Activation)   (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "l2_dense (Dense_with_tree)   (None, 23)                943       \n",
      "_________________________________________________________________\n",
      "l2_activation (Activation)   (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "l3_dense (Dense_with_tree)   (None, 17)                408       \n",
      "_________________________________________________________________\n",
      "l3_activation (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "l4_dense (Dense_with_tree)   (None, 9)                 162       \n",
      "_________________________________________________________________\n",
      "l4_activation (Activation)   (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "last_dense_h (Dense)         (None, 1)                 10        \n",
      "_________________________________________________________________\n",
      "p_hat (Activation)           (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,483\n",
      "Trainable params: 3,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:57] Build Network\n",
      "[root    |INFO|build_network.py:58] Optimizer = adam\n",
      "[root    |INFO|build_network.py:59] Loss = binary_crossentropy\n",
      "[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc\n",
      "[root    |INFO|deepbiome.py:311] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:312] 1 fold computing start!----------------------------------\n",
      "[root    |INFO|build_network.py:177] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "250/250 [==============================] - 0s 425us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:182] Evaluation end with time 0.2846240997314453!\n",
      "[root    |INFO|build_network.py:183] Evaluation: [0.282605916261673, 0.8960000276565552, 0.9349112510681152, 0.8148148059844971, 0.872799813747406, 0.9505442380905151]\n",
      "[root    |INFO|deepbiome.py:315] \n",
      "[root    |INFO|deepbiome.py:317] Compute time : 1.8910877704620361\n",
      "[root    |INFO|deepbiome.py:318] 1 fold computing end!---------------------------------------------\n",
      "[root    |INFO|deepbiome.py:291] -------2 fold test start!----------------------------------\n",
      "[root    |INFO|readers.py:58] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:59] Construct Dataset\n",
      "[root    |INFO|readers.py:60] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:61] Load data\n",
      "[root    |INFO|deepbiome.py:301] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:302] Build network for 2 fold testing\n",
      "[root    |INFO|build_network.py:513] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:514] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv\n",
      "[root    |INFO|build_network.py:518] Phylogenetic tree level list: ['Genus', 'Family', 'Order', 'Class', 'Phylum']\n",
      "[root    |INFO|build_network.py:519] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:526]      Genus: 48\n",
      "[root    |INFO|build_network.py:526]     Family: 40\n",
      "[root    |INFO|build_network.py:526]      Order: 23\n",
      "[root    |INFO|build_network.py:526]      Class: 17\n",
      "[root    |INFO|build_network.py:526]     Phylum: 9\n",
      "[root    |INFO|build_network.py:529] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:530] Phylogenetic_tree_dict info: ['Genus', 'Number', 'Order', 'Class', 'Phylum', 'Family']\n",
      "[root    |INFO|build_network.py:531] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [ Genus, Family]\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [Family,  Order]\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [ Order,  Class]\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [ Class, Phylum]\n",
      "[root    |INFO|build_network.py:554] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:570] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:571] Build network based on phylogenetic tree information\n",
      "[root    |INFO|build_network.py:572] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:648] ------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "l1_dense (Dense_with_tree)   (None, 40)                1960      \n",
      "_________________________________________________________________\n",
      "l1_activation (Activation)   (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "l2_dense (Dense_with_tree)   (None, 23)                943       \n",
      "_________________________________________________________________\n",
      "l2_activation (Activation)   (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "l3_dense (Dense_with_tree)   (None, 17)                408       \n",
      "_________________________________________________________________\n",
      "l3_activation (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "l4_dense (Dense_with_tree)   (None, 9)                 162       \n",
      "_________________________________________________________________\n",
      "l4_activation (Activation)   (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "last_dense_h (Dense)         (None, 1)                 10        \n",
      "_________________________________________________________________\n",
      "p_hat (Activation)           (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,483\n",
      "Trainable params: 3,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:57] Build Network\n",
      "[root    |INFO|build_network.py:58] Optimizer = adam\n",
      "[root    |INFO|build_network.py:59] Loss = binary_crossentropy\n",
      "[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc\n",
      "[root    |INFO|deepbiome.py:311] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:312] 2 fold computing start!----------------------------------\n",
      "[root    |INFO|build_network.py:177] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "250/250 [==============================] - 0s 456us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:182] Evaluation end with time 0.3126504421234131!\n",
      "[root    |INFO|build_network.py:183] Evaluation: [0.4448338449001312, 0.828000009059906, 0.8505747318267822, 0.7763158082962036, 0.8125974535942078, 0.8998034000396729]\n",
      "[root    |INFO|deepbiome.py:315] \n",
      "[root    |INFO|deepbiome.py:317] Compute time : 1.777827262878418\n",
      "[root    |INFO|deepbiome.py:318] 2 fold computing end!---------------------------------------------\n",
      "[root    |INFO|deepbiome.py:291] -------3 fold test start!----------------------------------\n",
      "[root    |INFO|readers.py:58] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:59] Construct Dataset\n",
      "[root    |INFO|readers.py:60] -----------------------------------------------------------------------\n",
      "[root    |INFO|readers.py:61] Load data\n",
      "[root    |INFO|deepbiome.py:301] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:302] Build network for 3 fold testing\n",
      "[root    |INFO|build_network.py:513] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:514] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv\n",
      "[root    |INFO|build_network.py:518] Phylogenetic tree level list: ['Genus', 'Family', 'Order', 'Class', 'Phylum']\n",
      "[root    |INFO|build_network.py:519] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:526]      Genus: 48\n",
      "[root    |INFO|build_network.py:526]     Family: 40\n",
      "[root    |INFO|build_network.py:526]      Order: 23\n",
      "[root    |INFO|build_network.py:526]      Class: 17\n",
      "[root    |INFO|build_network.py:526]     Phylum: 9\n",
      "[root    |INFO|build_network.py:529] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:530] Phylogenetic_tree_dict info: ['Genus', 'Number', 'Order', 'Class', 'Phylum', 'Family']\n",
      "[root    |INFO|build_network.py:531] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [ Genus, Family]\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [Family,  Order]\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [ Order,  Class]\n",
      "[root    |INFO|build_network.py:541] Build edge weights between [ Class, Phylum]\n",
      "[root    |INFO|build_network.py:554] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:570] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:571] Build network based on phylogenetic tree information\n",
      "[root    |INFO|build_network.py:572] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:648] ------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "l1_dense (Dense_with_tree)   (None, 40)                1960      \n",
      "_________________________________________________________________\n",
      "l1_activation (Activation)   (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "l2_dense (Dense_with_tree)   (None, 23)                943       \n",
      "_________________________________________________________________\n",
      "l2_activation (Activation)   (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "l3_dense (Dense_with_tree)   (None, 17)                408       \n",
      "_________________________________________________________________\n",
      "l3_activation (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "l4_dense (Dense_with_tree)   (None, 9)                 162       \n",
      "_________________________________________________________________\n",
      "l4_activation (Activation)   (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "last_dense_h (Dense)         (None, 1)                 10        \n",
      "_________________________________________________________________\n",
      "p_hat (Activation)           (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,483\n",
      "Trainable params: 3,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:57] Build Network\n",
      "[root    |INFO|build_network.py:58] Optimizer = adam\n",
      "[root    |INFO|build_network.py:59] Loss = binary_crossentropy\n",
      "[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc\n",
      "[root    |INFO|deepbiome.py:311] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:312] 3 fold computing start!----------------------------------\n",
      "[root    |INFO|build_network.py:177] Evaluation start!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "250/250 [==============================] - 0s 434us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:182] Evaluation end with time 0.2911825180053711!\n",
      "[root    |INFO|build_network.py:183] Evaluation: [0.2500866949558258, 0.9120000004768372, 0.932584285736084, 0.8611111044883728, 0.8961353898048401, 0.958996593952179]\n",
      "[root    |INFO|deepbiome.py:315] \n",
      "[root    |INFO|deepbiome.py:317] Compute time : 1.838158369064331\n",
      "[root    |INFO|deepbiome.py:318] 3 fold computing end!---------------------------------------------\n",
      "[root    |INFO|deepbiome.py:321] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:323] Test Evaluation : ['loss' 'binary_accuracy' 'sensitivity' 'specificity' 'gmeasure' 'auc']\n",
      "[root    |INFO|deepbiome.py:326]       mean : [0.32584215 0.87866668 0.90602342 0.81741391 0.86051089 0.93644808]\n",
      "[root    |INFO|deepbiome.py:327]        std : [0.08518076 0.03641733 0.03921965 0.03466629 0.03519384 0.02614045]\n",
      "[root    |INFO|deepbiome.py:328] -----------------------------------------------------------------\n",
      "[root    |INFO|deepbiome.py:331] Total Computing Ended\n",
      "[root    |INFO|deepbiome.py:332] -----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "evaluation = deepbiome.deepbiome_test(log, network_info, path_info, number_of_fold=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function provide the evaluation result as a numpy array of the shape (number of fold, number of evaluation measures)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  loss binary_accuracy     sensitivity     specificity        gmeasure             auc\n",
      "Mean:           0.3258          0.8787          0.9060          0.8174          0.8605          0.9364\n",
      "Std :           0.0852          0.0364          0.0392          0.0347          0.0352          0.0261\n"
     ]
    }
   ],
   "source": [
    "print('      %s' % ''.join(['%16s'%'loss']+ ['%16s'%s.strip() for s in network_info['model_info']['metrics'].split(',')]))\n",
    "print('Mean: %s' % ''.join(['%16.4f'%v for v in np.mean(evaluation, axis=0)]))\n",
    "print('Std : %s' % ''.join(['%16.4f'%v for v in np.std(evaluation, axis=0)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
