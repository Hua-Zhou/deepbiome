

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Example : k times repetition with the list of k input files &mdash; deepbiome 0.1.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script>
        <script type="text/javascript" src="_static/copybutton.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Release History" href="release-history.html" />
    <link rel="prev" title="Usage" href="usage.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> deepbiome
          

          
          </a>

          
            
            
              <div class="version">
                0.1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="prerequisites.html">Prerequisites</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage.html">Usage</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Example : k times repetition with the list of k input files</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#1.-Load-library">1. Load library</a></li>
<li class="toctree-l2"><a class="reference internal" href="#2.-Prepare-the-dataset">2. Prepare the dataset</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Example-of-the-tree-information">Example of the tree information</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Example-of-the-list-of-the-name-of-input-files">Example of the list of the name of input files</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Example-of-the-lists-of-the-input-files">Example of the lists of the input files</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Example-of-the-Y-(regression)">Example of the Y (regression)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Example-of-the-Y-(classification)">Example of the Y (classification)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Exmple-of-the-training-index-file-for-repetition">Exmple of the training index file for repetition</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#3.-Prepare-the-configuration">3. Prepare the configuration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#For-preparing-the-configuration-about-the-network-information-(network_info)">For preparing the configuration about the network information (<code class="docutils literal notranslate"><span class="pre">network_info</span></code>)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#For-preparing-the-configuration-about-the-path-information-(path_info)">For preparing the configuration about the path information (<code class="docutils literal notranslate"><span class="pre">path_info</span></code>)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#4.-Deepbiome-Training">4. Deepbiome Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="#5.-Load-the-pretrained-network-for-training">5. Load the pretrained network for training</a></li>
<li class="toctree-l2"><a class="reference internal" href="#6.-Load-the-pretrained-network-for-testing">6. Load the pretrained network for testing</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="release-history.html">Release History</a></li>
<li class="toctree-l1"><a class="reference internal" href="min_versions.html">Minimum Version of Python and NumPy</a></li>
<li class="toctree-l1"><a class="reference internal" href="authors.html">Credits</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">deepbiome</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Example : k times repetition with the list of k input files</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/example_with_the_list_of_inputs.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 5ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Example-:-k-times-repetition-with-the-list-of-k-input-files">
<h1>Example : k times repetition with the list of k input files<a class="headerlink" href="#Example-:-k-times-repetition-with-the-list-of-k-input-files" title="Permalink to this headline">¶</a></h1>
<p>Deepbiome packages takes microbiome abundance data as input and uses the phylogenetic taxonomy to guide the decision of the optimal number of layers and neurons in the deep learning architecture.</p>
<p>To use deepbiome, you can experiment (1) <strong>k times repeatition</strong> or (2) <strong>k fold cross-validation</strong>. For each experiment, we asuume that the dataset is given by - <strong>A list of k input files for k times repeatition.</strong> - <strong>One input file for k times cross validation.</strong></p>
<p>This notebook contains an example of (1) <strong>k times repeatition</strong> for the deep neural netowrk using deepbiome.</p>
<div class="section" id="1.-Load-library">
<h2>1. Load library<a class="headerlink" href="#1.-Load-library" title="Permalink to this headline">¶</a></h2>
<p>First, we have to load deepbiome package. The deepbiome package is build on the tensorflow and keras library</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">from</span> <span class="nn">pkg_resources</span> <span class="kn">import</span> <span class="n">resource_filename</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">deepbiome</span> <span class="kn">import</span> <span class="n">deepbiome</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Using TensorFlow backend.
</pre></div></div>
</div>
</div>
<div class="section" id="2.-Prepare-the-dataset">
<h2>2. Prepare the dataset<a class="headerlink" href="#2.-Prepare-the-dataset" title="Permalink to this headline">¶</a></h2>
<p>In this example, we assume that we have <strong>a list of k input files for k times repeatition.</strong></p>
<p>Deepbiome needs 4 data as follow: 1. <strong>the tree information</strong> 1. <strong>the lists of the input files</strong> (each file has all sample’s information for one repeatition) 1. <strong>the list of the name of input files</strong> 1. <strong>y</strong></p>
<p>In addition, we can set <strong>the training index for each repeatition</strong>. If we set the index file, deepbiome build the training set for each repeatition based on each fold index in the index file. If not, deepbiome will generate the index file locally.</p>
<p>Eath data should have the csv format. Below is the example of each file.</p>
<div class="section" id="Example-of-the-tree-information">
<h3>Example of the tree information<a class="headerlink" href="#Example-of-the-tree-information" title="Permalink to this headline">¶</a></h3>
<p>First we need a file about the phylogenetic tree information. This tree information file should have the format below:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">tree_information</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">resource_filename</span><span class="p">(</span><span class="s1">&#39;deepbiome&#39;</span><span class="p">,</span> <span class="s1">&#39;tests/data/genus48_dic.csv&#39;</span><span class="p">))</span>
<span class="n">tree_information</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Genus</th>
      <th>Family</th>
      <th>Order</th>
      <th>Class</th>
      <th>Phylum</th>
      <th>Domain</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Streptococcus</td>
      <td>Streptococcaceae</td>
      <td>Lactobacillales</td>
      <td>Bacilli</td>
      <td>Firmicutes</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Tropheryma</td>
      <td>Cellulomonadaceae</td>
      <td>Actinomycetales</td>
      <td>Actinobacteria</td>
      <td>Actinobacteria</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Veillonella</td>
      <td>Veillonellaceae</td>
      <td>Selenomonadales</td>
      <td>Negativicutes</td>
      <td>Firmicutes</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Actinomyces</td>
      <td>Actinomycetaceae</td>
      <td>Actinomycetales</td>
      <td>Actinobacteria</td>
      <td>Actinobacteria</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Flavobacterium</td>
      <td>Flavobacteriaceae</td>
      <td>Flavobacteriales</td>
      <td>Flavobacteria</td>
      <td>Bacteroidetes</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Prevotella</td>
      <td>Prevotellaceae</td>
      <td>Bacteroidales</td>
      <td>Bacteroidia</td>
      <td>Bacteroidetes</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Porphyromonas</td>
      <td>Porphyromonadaceae</td>
      <td>Bacteroidales</td>
      <td>Bacteroidia</td>
      <td>Bacteroidetes</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Parvimonas</td>
      <td>Clostridiales_Incertae_Sedis_XI</td>
      <td>Clostridiales</td>
      <td>Clostridia</td>
      <td>Firmicutes</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Fusobacterium</td>
      <td>Fusobacteriaceae</td>
      <td>Fusobacteriales</td>
      <td>Fusobacteria</td>
      <td>Fusobacteria</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Propionibacterium</td>
      <td>Propionibacteriaceae</td>
      <td>Actinomycetales</td>
      <td>Actinobacteria</td>
      <td>Actinobacteria</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Gemella</td>
      <td>Bacillales_Incertae_Sedis_XI</td>
      <td>Bacillales</td>
      <td>Bacilli</td>
      <td>Firmicutes</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Rothia</td>
      <td>Micrococcaceae</td>
      <td>Actinomycetales</td>
      <td>Actinobacteria</td>
      <td>Actinobacteria</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>12</th>
      <td>Granulicatella</td>
      <td>Carnobacteriaceae</td>
      <td>Lactobacillales</td>
      <td>Bacilli</td>
      <td>Firmicutes</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>13</th>
      <td>Neisseria</td>
      <td>Neisseriaceae</td>
      <td>Neisseriales</td>
      <td>Betaproteobacteria</td>
      <td>Proteobacteria</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>14</th>
      <td>Lactobacillus</td>
      <td>Lactobacillaceae</td>
      <td>Lactobacillales</td>
      <td>Bacilli</td>
      <td>Firmicutes</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>15</th>
      <td>Megasphaera</td>
      <td>Veillonellaceae</td>
      <td>Selenomonadales</td>
      <td>Negativicutes</td>
      <td>Firmicutes</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>16</th>
      <td>Catonella</td>
      <td>Lachnospiraceae</td>
      <td>Clostridiales</td>
      <td>Clostridia</td>
      <td>Firmicutes</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>17</th>
      <td>Atopobium</td>
      <td>Coriobacteriaceae</td>
      <td>Coriobacteriales</td>
      <td>Actinobacteria</td>
      <td>Actinobacteria</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>18</th>
      <td>Campylobacter</td>
      <td>Campylobacteraceae</td>
      <td>Campylobacterales</td>
      <td>Epsilonproteobacteria</td>
      <td>Proteobacteria</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>19</th>
      <td>Capnocytophaga</td>
      <td>Flavobacteriaceae</td>
      <td>Flavobacteriales</td>
      <td>Flavobacteria</td>
      <td>Bacteroidetes</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>20</th>
      <td>Solobacterium</td>
      <td>Erysipelotrichaceae</td>
      <td>Erysipelotrichales</td>
      <td>Erysipelotrichia</td>
      <td>Firmicutes</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>21</th>
      <td>Moryella</td>
      <td>Lachnospiraceae</td>
      <td>Clostridiales</td>
      <td>Clostridia</td>
      <td>Firmicutes</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>22</th>
      <td>TM7_genera_incertae_sedis</td>
      <td>TM7_genera_incertae_sedis</td>
      <td>TM7_genera_incertae_sedis</td>
      <td>TM7_genera_incertae_sedis</td>
      <td>TM7</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>23</th>
      <td>Staphylococcus</td>
      <td>Staphylococcaceae</td>
      <td>Bacillales</td>
      <td>Bacilli</td>
      <td>Firmicutes</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>24</th>
      <td>Filifactor</td>
      <td>Peptostreptococcaceae</td>
      <td>Clostridiales</td>
      <td>Clostridia</td>
      <td>Firmicutes</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>25</th>
      <td>Oribacterium</td>
      <td>Lachnospiraceae</td>
      <td>Clostridiales</td>
      <td>Clostridia</td>
      <td>Firmicutes</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>26</th>
      <td>Burkholderia</td>
      <td>Burkholderiaceae</td>
      <td>Burkholderiales</td>
      <td>Betaproteobacteria</td>
      <td>Proteobacteria</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>27</th>
      <td>Sneathia</td>
      <td>Leptotrichiaceae</td>
      <td>Fusobacteriales</td>
      <td>Fusobacteria</td>
      <td>Fusobacteria</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>28</th>
      <td>Treponema</td>
      <td>Spirochaetaceae</td>
      <td>Spirochaetales</td>
      <td>Spirochaetes</td>
      <td>Spirochaetes</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>29</th>
      <td>Moraxella</td>
      <td>Moraxellaceae</td>
      <td>Pseudomonadales</td>
      <td>Gammaproteobacteria</td>
      <td>Proteobacteria</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>30</th>
      <td>Haemophilus</td>
      <td>Pasteurellaceae</td>
      <td>Pasteurellales</td>
      <td>Gammaproteobacteria</td>
      <td>Proteobacteria</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>31</th>
      <td>Selenomonas</td>
      <td>Veillonellaceae</td>
      <td>Selenomonadales</td>
      <td>Negativicutes</td>
      <td>Firmicutes</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>32</th>
      <td>Corynebacterium</td>
      <td>Corynebacteriaceae</td>
      <td>Actinomycetales</td>
      <td>Actinobacteria</td>
      <td>Actinobacteria</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>33</th>
      <td>Rhizobium</td>
      <td>Rhizobiaceae</td>
      <td>Rhizobiales</td>
      <td>Alphaproteobacteria</td>
      <td>Proteobacteria</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>34</th>
      <td>Bradyrhizobium</td>
      <td>Bradyrhizobiaceae</td>
      <td>Rhizobiales</td>
      <td>Alphaproteobacteria</td>
      <td>Proteobacteria</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>35</th>
      <td>Methylobacterium</td>
      <td>Methylobacteriaceae</td>
      <td>Rhizobiales</td>
      <td>Alphaproteobacteria</td>
      <td>Proteobacteria</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>36</th>
      <td>OD1_genera_incertae_sedis</td>
      <td>OD1_genera_incertae_sedis</td>
      <td>OD1_genera_incertae_sedis</td>
      <td>OD1_genera_incertae_sedis</td>
      <td>OD1</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>37</th>
      <td>Finegoldia</td>
      <td>Clostridiales_Incertae_Sedis_XI</td>
      <td>Clostridiales</td>
      <td>Clostridia</td>
      <td>Firmicutes</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>38</th>
      <td>Microbacterium</td>
      <td>Microbacteriaceae</td>
      <td>Actinomycetales</td>
      <td>Actinobacteria</td>
      <td>Actinobacteria</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>39</th>
      <td>Sphingomonas</td>
      <td>Sphingomonadaceae</td>
      <td>Sphingomonadales</td>
      <td>Alphaproteobacteria</td>
      <td>Proteobacteria</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>40</th>
      <td>Chryseobacterium</td>
      <td>Flavobacteriaceae</td>
      <td>Flavobacteriales</td>
      <td>Flavobacteria</td>
      <td>Bacteroidetes</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>41</th>
      <td>Bacteroides</td>
      <td>Bacteroidaceae</td>
      <td>Bacteroidales</td>
      <td>Bacteroidia</td>
      <td>Bacteroidetes</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>42</th>
      <td>Bdellovibrio</td>
      <td>Bdellovibrionaceae</td>
      <td>Bdellovibrionales</td>
      <td>Deltaproteobacteria</td>
      <td>Proteobacteria</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>43</th>
      <td>Streptophyta</td>
      <td>Chloroplast</td>
      <td>Chloroplast</td>
      <td>Chloroplast</td>
      <td>Cyanobacteria_Chloroplast</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>44</th>
      <td>Lachnospiracea_incertae_sedis</td>
      <td>Lachnospiraceae</td>
      <td>Clostridiales</td>
      <td>Clostridia</td>
      <td>Firmicutes</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>45</th>
      <td>Paracoccus</td>
      <td>Rhodobacteraceae</td>
      <td>Rhodobacterales</td>
      <td>Alphaproteobacteria</td>
      <td>Proteobacteria</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>46</th>
      <td>Fastidiosipila</td>
      <td>Ruminococcaceae</td>
      <td>Clostridiales</td>
      <td>Clostridia</td>
      <td>Firmicutes</td>
      <td>Bacteria</td>
    </tr>
    <tr>
      <th>47</th>
      <td>Pseudonocardia</td>
      <td>Pseudonocardiaceae</td>
      <td>Actinomycetales</td>
      <td>Actinobacteria</td>
      <td>Actinobacteria</td>
      <td>Bacteria</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
</div>
<div class="section" id="Example-of-the-list-of-the-name-of-input-files">
<h3>Example of the list of the name of input files<a class="headerlink" href="#Example-of-the-list-of-the-name-of-input-files" title="Permalink to this headline">¶</a></h3>
<p>In this example. we assume that input is given by the lists of files. Each file has all sample’s information for one repeatition. If we want to use the list of the input files, we need the make a list of the name of each input file with the format below:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">list_of_input_files</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">resource_filename</span><span class="p">(</span><span class="s1">&#39;deepbiome&#39;</span><span class="p">,</span> <span class="s1">&#39;tests/data/gcount_list.csv&#39;</span><span class="p">),</span> <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">list_of_input_files</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>gcount_0001.csv</td>
    </tr>
    <tr>
      <th>1</th>
      <td>gcount_0002.csv</td>
    </tr>
    <tr>
      <th>2</th>
      <td>gcount_0003.csv</td>
    </tr>
    <tr>
      <th>3</th>
      <td>gcount_0004.csv</td>
    </tr>
    <tr>
      <th>4</th>
      <td>gcount_0005.csv</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">list_of_input_files</span><span class="o">.</span><span class="n">tail</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>995</th>
      <td>gcount_0996.csv</td>
    </tr>
    <tr>
      <th>996</th>
      <td>gcount_0997.csv</td>
    </tr>
    <tr>
      <th>997</th>
      <td>gcount_0998.csv</td>
    </tr>
    <tr>
      <th>998</th>
      <td>gcount_0999.csv</td>
    </tr>
    <tr>
      <th>999</th>
      <td>gcount_1000.csv</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
</div>
<div class="section" id="Example-of-the-lists-of-the-input-files">
<h3>Example of the lists of the input files<a class="headerlink" href="#Example-of-the-lists-of-the-input-files" title="Permalink to this headline">¶</a></h3>
<p>Below is an example of the each input file. This example has 1000 samples for each row, and abandunt of each microbiome for each column.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">x_1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">resource_filename</span><span class="p">(</span><span class="s1">&#39;deepbiome&#39;</span><span class="p">,</span> <span class="s1">&#39;tests/data/count/</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">list_of_input_files</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]))</span>
<span class="n">x_1</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Streptococcus</th>
      <th>Tropheryma</th>
      <th>Veillonella</th>
      <th>Actinomyces</th>
      <th>Flavobacterium</th>
      <th>Prevotella</th>
      <th>Porphyromonas</th>
      <th>Parvimonas</th>
      <th>Fusobacterium</th>
      <th>Propionibacterium</th>
      <th>...</th>
      <th>Microbacterium</th>
      <th>Sphingomonas</th>
      <th>Chryseobacterium</th>
      <th>Bacteroides</th>
      <th>Bdellovibrio</th>
      <th>Streptophyta</th>
      <th>Lachnospiracea_incertae_sedis</th>
      <th>Paracoccus</th>
      <th>Fastidiosipila</th>
      <th>Pseudonocardia</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>841</td>
      <td>0</td>
      <td>813</td>
      <td>505</td>
      <td>5</td>
      <td>3224</td>
      <td>0</td>
      <td>362</td>
      <td>11</td>
      <td>65</td>
      <td>...</td>
      <td>0</td>
      <td>87</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2133</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1445</td>
      <td>0</td>
      <td>1</td>
      <td>573</td>
      <td>0</td>
      <td>1278</td>
      <td>82</td>
      <td>85</td>
      <td>69</td>
      <td>154</td>
      <td>...</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3638</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1259</td>
      <td>0</td>
      <td>805</td>
      <td>650</td>
      <td>0</td>
      <td>1088</td>
      <td>0</td>
      <td>0</td>
      <td>74</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>2</td>
      <td>8</td>
      <td>1</td>
      <td>39</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3445</td>
    </tr>
    <tr>
      <th>3</th>
      <td>982</td>
      <td>0</td>
      <td>327</td>
      <td>594</td>
      <td>0</td>
      <td>960</td>
      <td>81</td>
      <td>19</td>
      <td>9</td>
      <td>0</td>
      <td>...</td>
      <td>157</td>
      <td>1</td>
      <td>0</td>
      <td>4</td>
      <td>60</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3507</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1162</td>
      <td>0</td>
      <td>130</td>
      <td>969</td>
      <td>163</td>
      <td>1515</td>
      <td>167</td>
      <td>4</td>
      <td>162</td>
      <td>3</td>
      <td>...</td>
      <td>0</td>
      <td>9</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>60</td>
      <td>0</td>
      <td>0</td>
      <td>3945</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 48 columns</p>
</div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">x_1</span><span class="o">.</span><span class="n">tail</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Streptococcus</th>
      <th>Tropheryma</th>
      <th>Veillonella</th>
      <th>Actinomyces</th>
      <th>Flavobacterium</th>
      <th>Prevotella</th>
      <th>Porphyromonas</th>
      <th>Parvimonas</th>
      <th>Fusobacterium</th>
      <th>Propionibacterium</th>
      <th>...</th>
      <th>Microbacterium</th>
      <th>Sphingomonas</th>
      <th>Chryseobacterium</th>
      <th>Bacteroides</th>
      <th>Bdellovibrio</th>
      <th>Streptophyta</th>
      <th>Lachnospiracea_incertae_sedis</th>
      <th>Paracoccus</th>
      <th>Fastidiosipila</th>
      <th>Pseudonocardia</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>995</th>
      <td>1401</td>
      <td>4</td>
      <td>30</td>
      <td>526</td>
      <td>0</td>
      <td>923</td>
      <td>25</td>
      <td>0</td>
      <td>127</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>7</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4470</td>
    </tr>
    <tr>
      <th>996</th>
      <td>2655</td>
      <td>6</td>
      <td>106</td>
      <td>74</td>
      <td>0</td>
      <td>952</td>
      <td>76</td>
      <td>13</td>
      <td>158</td>
      <td>125</td>
      <td>...</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2826</td>
    </tr>
    <tr>
      <th>997</th>
      <td>335</td>
      <td>0</td>
      <td>71</td>
      <td>259</td>
      <td>67</td>
      <td>718</td>
      <td>1</td>
      <td>4</td>
      <td>4</td>
      <td>167</td>
      <td>...</td>
      <td>0</td>
      <td>246</td>
      <td>0</td>
      <td>0</td>
      <td>6</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>6527</td>
    </tr>
    <tr>
      <th>998</th>
      <td>649</td>
      <td>69</td>
      <td>966</td>
      <td>1227</td>
      <td>0</td>
      <td>508</td>
      <td>2</td>
      <td>30</td>
      <td>550</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>6</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>4402</td>
    </tr>
    <tr>
      <th>999</th>
      <td>1258</td>
      <td>0</td>
      <td>0</td>
      <td>1119</td>
      <td>0</td>
      <td>2348</td>
      <td>25</td>
      <td>0</td>
      <td>137</td>
      <td>176</td>
      <td>...</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2585</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 48 columns</p>
</div></div>
</div>
</div>
<div class="section" id="Example-of-the-Y-(regression)">
<h3>Example of the Y (regression)<a class="headerlink" href="#Example-of-the-Y-(regression)" title="Permalink to this headline">¶</a></h3>
<p>This is an example of the output file for regression problem. Below example file has 1000 samples in row, 1000 repeatition in column.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">resource_filename</span><span class="p">(</span><span class="s1">&#39;deepbiome&#39;</span><span class="p">,</span> <span class="s1">&#39;tests/data/regression_y.csv&#39;</span><span class="p">))</span>
<span class="n">y</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>x1</th>
      <th>x2</th>
      <th>x3</th>
      <th>x4</th>
      <th>x5</th>
      <th>x6</th>
      <th>x7</th>
      <th>x8</th>
      <th>x9</th>
      <th>x10</th>
      <th>...</th>
      <th>x991</th>
      <th>x992</th>
      <th>x993</th>
      <th>x994</th>
      <th>x995</th>
      <th>x996</th>
      <th>x997</th>
      <th>x998</th>
      <th>x999</th>
      <th>x1000</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>4.997270</td>
      <td>5.492354</td>
      <td>5.473725</td>
      <td>1.759484</td>
      <td>5.313252</td>
      <td>1.500044</td>
      <td>4.949712</td>
      <td>5.493533</td>
      <td>3.743509</td>
      <td>5.492373</td>
      <td>...</td>
      <td>2.793883</td>
      <td>1.500004</td>
      <td>5.487526</td>
      <td>5.493518</td>
      <td>3.599047</td>
      <td>5.491461</td>
      <td>5.486244</td>
      <td>5.487390</td>
      <td>5.493492</td>
      <td>3.762523</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5.004092</td>
      <td>1.500002</td>
      <td>4.640348</td>
      <td>1.538071</td>
      <td>5.491065</td>
      <td>5.481009</td>
      <td>5.492323</td>
      <td>2.968531</td>
      <td>3.576358</td>
      <td>5.491456</td>
      <td>...</td>
      <td>1.500033</td>
      <td>3.369529</td>
      <td>1.500016</td>
      <td>3.103297</td>
      <td>5.493214</td>
      <td>3.831125</td>
      <td>5.492104</td>
      <td>5.474811</td>
      <td>5.492416</td>
      <td>3.268805</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5.485126</td>
      <td>4.187426</td>
      <td>5.491340</td>
      <td>5.469662</td>
      <td>5.490478</td>
      <td>1.953375</td>
      <td>5.494656</td>
      <td>3.741680</td>
      <td>4.862400</td>
      <td>5.490701</td>
      <td>...</td>
      <td>5.491728</td>
      <td>2.459981</td>
      <td>5.475697</td>
      <td>3.114158</td>
      <td>1.500004</td>
      <td>1.500019</td>
      <td>4.113815</td>
      <td>5.470539</td>
      <td>5.494373</td>
      <td>5.481754</td>
    </tr>
    <tr>
      <th>3</th>
      <td>5.489590</td>
      <td>4.863187</td>
      <td>1.500003</td>
      <td>5.484699</td>
      <td>5.492657</td>
      <td>5.491270</td>
      <td>4.091023</td>
      <td>5.495239</td>
      <td>5.492804</td>
      <td>1.500046</td>
      <td>...</td>
      <td>1.500034</td>
      <td>1.500012</td>
      <td>5.483070</td>
      <td>2.475049</td>
      <td>5.493846</td>
      <td>3.287076</td>
      <td>3.696412</td>
      <td>5.487583</td>
      <td>1.500044</td>
      <td>2.760404</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.500001</td>
      <td>5.480769</td>
      <td>5.489725</td>
      <td>1.500044</td>
      <td>2.695212</td>
      <td>5.492262</td>
      <td>3.381424</td>
      <td>4.805420</td>
      <td>1.500047</td>
      <td>5.474376</td>
      <td>...</td>
      <td>1.500046</td>
      <td>2.586990</td>
      <td>5.440610</td>
      <td>4.376103</td>
      <td>1.500030</td>
      <td>4.713223</td>
      <td>5.491059</td>
      <td>3.230658</td>
      <td>1.500045</td>
      <td>5.488727</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 1000 columns</p>
</div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">y</span><span class="o">.</span><span class="n">tail</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>x1</th>
      <th>x2</th>
      <th>x3</th>
      <th>x4</th>
      <th>x5</th>
      <th>x6</th>
      <th>x7</th>
      <th>x8</th>
      <th>x9</th>
      <th>x10</th>
      <th>...</th>
      <th>x991</th>
      <th>x992</th>
      <th>x993</th>
      <th>x994</th>
      <th>x995</th>
      <th>x996</th>
      <th>x997</th>
      <th>x998</th>
      <th>x999</th>
      <th>x1000</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>995</th>
      <td>2.609926</td>
      <td>5.491258</td>
      <td>3.318610</td>
      <td>5.444070</td>
      <td>2.884154</td>
      <td>5.486857</td>
      <td>5.496554</td>
      <td>1.500019</td>
      <td>5.482893</td>
      <td>1.824835</td>
      <td>...</td>
      <td>4.478641</td>
      <td>5.485122</td>
      <td>4.915985</td>
      <td>4.073239</td>
      <td>1.500019</td>
      <td>5.492295</td>
      <td>1.500005</td>
      <td>1.559586</td>
      <td>5.496415</td>
      <td>4.171127</td>
    </tr>
    <tr>
      <th>996</th>
      <td>5.488959</td>
      <td>3.739806</td>
      <td>5.489474</td>
      <td>1.500021</td>
      <td>5.492632</td>
      <td>1.500019</td>
      <td>5.484813</td>
      <td>5.467055</td>
      <td>5.491282</td>
      <td>1.874777</td>
      <td>...</td>
      <td>5.498820</td>
      <td>5.493926</td>
      <td>5.487404</td>
      <td>3.162812</td>
      <td>1.846298</td>
      <td>5.492417</td>
      <td>1.919107</td>
      <td>5.480324</td>
      <td>5.467765</td>
      <td>5.457627</td>
    </tr>
    <tr>
      <th>997</th>
      <td>3.498418</td>
      <td>4.250451</td>
      <td>5.488116</td>
      <td>4.162031</td>
      <td>5.494052</td>
      <td>5.472900</td>
      <td>1.500057</td>
      <td>5.491497</td>
      <td>5.491935</td>
      <td>1.500033</td>
      <td>...</td>
      <td>1.966474</td>
      <td>5.475258</td>
      <td>3.848034</td>
      <td>2.863883</td>
      <td>4.370685</td>
      <td>5.494647</td>
      <td>5.478855</td>
      <td>2.465739</td>
      <td>1.500018</td>
      <td>5.486403</td>
    </tr>
    <tr>
      <th>998</th>
      <td>5.486107</td>
      <td>1.917414</td>
      <td>5.414975</td>
      <td>5.492364</td>
      <td>2.027914</td>
      <td>5.491349</td>
      <td>5.494135</td>
      <td>5.491245</td>
      <td>1.500039</td>
      <td>1.500019</td>
      <td>...</td>
      <td>4.556995</td>
      <td>5.457072</td>
      <td>2.071106</td>
      <td>5.417333</td>
      <td>5.491818</td>
      <td>5.473390</td>
      <td>4.374154</td>
      <td>5.489109</td>
      <td>4.515340</td>
      <td>1.500020</td>
    </tr>
    <tr>
      <th>999</th>
      <td>5.319623</td>
      <td>5.482776</td>
      <td>1.500035</td>
      <td>5.485141</td>
      <td>5.491019</td>
      <td>3.733982</td>
      <td>5.494374</td>
      <td>3.077159</td>
      <td>5.493188</td>
      <td>1.500001</td>
      <td>...</td>
      <td>5.485356</td>
      <td>1.500059</td>
      <td>5.400762</td>
      <td>5.489606</td>
      <td>5.494583</td>
      <td>5.490943</td>
      <td>5.123794</td>
      <td>5.473465</td>
      <td>3.274979</td>
      <td>3.700653</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 1000 columns</p>
</div></div>
</div>
<p>For one repeatition, the deepbiome will use the one column.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>0    4.997270
1    5.004092
2    5.485126
3    5.489590
4    1.500001
Name: x1, dtype: float64
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tail</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>995    2.609926
996    5.488959
997    3.498418
998    5.486107
999    5.319623
Name: x1, dtype: float64
</pre></div>
</div>
</div>
</div>
<div class="section" id="Example-of-the-Y-(classification)">
<h3>Example of the Y (classification)<a class="headerlink" href="#Example-of-the-Y-(classification)" title="Permalink to this headline">¶</a></h3>
<p>This is an example of the output file for classification problem. Below example file has 1000 samples in row, 1000 repeatition in column.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">resource_filename</span><span class="p">(</span><span class="s1">&#39;deepbiome&#39;</span><span class="p">,</span> <span class="s1">&#39;tests/data/classification_y.csv&#39;</span><span class="p">))</span>
<span class="n">y</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>V1</th>
      <th>V2</th>
      <th>V3</th>
      <th>V4</th>
      <th>V5</th>
      <th>V6</th>
      <th>V7</th>
      <th>V8</th>
      <th>V9</th>
      <th>V10</th>
      <th>...</th>
      <th>V991</th>
      <th>V992</th>
      <th>V993</th>
      <th>V994</th>
      <th>V995</th>
      <th>V996</th>
      <th>V997</th>
      <th>V998</th>
      <th>V999</th>
      <th>V1000</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>...</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>...</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 1000 columns</p>
</div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">y</span><span class="o">.</span><span class="n">tail</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>V1</th>
      <th>V2</th>
      <th>V3</th>
      <th>V4</th>
      <th>V5</th>
      <th>V6</th>
      <th>V7</th>
      <th>V8</th>
      <th>V9</th>
      <th>V10</th>
      <th>...</th>
      <th>V991</th>
      <th>V992</th>
      <th>V993</th>
      <th>V994</th>
      <th>V995</th>
      <th>V996</th>
      <th>V997</th>
      <th>V998</th>
      <th>V999</th>
      <th>V1000</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>995</th>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>...</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>996</th>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>997</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>...</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>998</th>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>...</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>999</th>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 1000 columns</p>
</div></div>
</div>
<p>For one repeatition, the deepbiome will use the one column.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>0    1.0
1    1.0
2    0.0
3    0.0
4    1.0
Name: V1, dtype: float64
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">y</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tail</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>995    1.0
996    0.0
997    1.0
998    0.0
999    1.0
Name: V1, dtype: float64
</pre></div>
</div>
</div>
</div>
<div class="section" id="Exmple-of-the-training-index-file-for-repetition">
<h3>Exmple of the training index file for repetition<a class="headerlink" href="#Exmple-of-the-training-index-file-for-repetition" title="Permalink to this headline">¶</a></h3>
<p>For each repeatition, we have to set the training and test set. If the index file is given, the deepbiome library set the training set and test set based on the index file. Below is the example of the index file. Each column has the training indexs for each repeatition. The deepbiome will only use the samples in this index set for training.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">idxs</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">resource_filename</span><span class="p">(</span><span class="s1">&#39;deepbiome&#39;</span><span class="p">,</span> <span class="s1">&#39;tests/data/regression_idx.csv&#39;</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
<span class="n">idxs</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>V1</th>
      <th>V2</th>
      <th>V3</th>
      <th>V4</th>
      <th>V5</th>
      <th>V6</th>
      <th>V7</th>
      <th>V8</th>
      <th>V9</th>
      <th>V10</th>
      <th>...</th>
      <th>V991</th>
      <th>V992</th>
      <th>V993</th>
      <th>V994</th>
      <th>V995</th>
      <th>V996</th>
      <th>V997</th>
      <th>V998</th>
      <th>V999</th>
      <th>V1000</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>490</td>
      <td>690</td>
      <td>62</td>
      <td>703</td>
      <td>690</td>
      <td>845</td>
      <td>150</td>
      <td>268</td>
      <td>488</td>
      <td>179</td>
      <td>...</td>
      <td>675</td>
      <td>886</td>
      <td>225</td>
      <td>222</td>
      <td>781</td>
      <td>778</td>
      <td>603</td>
      <td>222</td>
      <td>254</td>
      <td>407</td>
    </tr>
    <tr>
      <th>1</th>
      <td>498</td>
      <td>968</td>
      <td>123</td>
      <td>913</td>
      <td>348</td>
      <td>262</td>
      <td>705</td>
      <td>239</td>
      <td>632</td>
      <td>44</td>
      <td>...</td>
      <td>636</td>
      <td>216</td>
      <td>495</td>
      <td>557</td>
      <td>196</td>
      <td>516</td>
      <td>23</td>
      <td>351</td>
      <td>472</td>
      <td>945</td>
    </tr>
    <tr>
      <th>2</th>
      <td>389</td>
      <td>999</td>
      <td>335</td>
      <td>947</td>
      <td>215</td>
      <td>696</td>
      <td>793</td>
      <td>349</td>
      <td>734</td>
      <td>624</td>
      <td>...</td>
      <td>626</td>
      <td>230</td>
      <td>26</td>
      <td>330</td>
      <td>470</td>
      <td>992</td>
      <td>329</td>
      <td>532</td>
      <td>655</td>
      <td>426</td>
    </tr>
    <tr>
      <th>3</th>
      <td>51</td>
      <td>139</td>
      <td>843</td>
      <td>491</td>
      <td>47</td>
      <td>421</td>
      <td>892</td>
      <td>32</td>
      <td>438</td>
      <td>996</td>
      <td>...</td>
      <td>956</td>
      <td>706</td>
      <td>836</td>
      <td>151</td>
      <td>80</td>
      <td>409</td>
      <td>671</td>
      <td>772</td>
      <td>882</td>
      <td>181</td>
    </tr>
    <tr>
      <th>4</th>
      <td>592</td>
      <td>83</td>
      <td>204</td>
      <td>810</td>
      <td>198</td>
      <td>955</td>
      <td>357</td>
      <td>125</td>
      <td>190</td>
      <td>162</td>
      <td>...</td>
      <td>542</td>
      <td>108</td>
      <td>959</td>
      <td>311</td>
      <td>771</td>
      <td>902</td>
      <td>986</td>
      <td>481</td>
      <td>922</td>
      <td>305</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 1000 columns</p>
</div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">idxs</span><span class="o">.</span><span class="n">tail</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>V1</th>
      <th>V2</th>
      <th>V3</th>
      <th>V4</th>
      <th>V5</th>
      <th>V6</th>
      <th>V7</th>
      <th>V8</th>
      <th>V9</th>
      <th>V10</th>
      <th>...</th>
      <th>V991</th>
      <th>V992</th>
      <th>V993</th>
      <th>V994</th>
      <th>V995</th>
      <th>V996</th>
      <th>V997</th>
      <th>V998</th>
      <th>V999</th>
      <th>V1000</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>745</th>
      <td>599</td>
      <td>824</td>
      <td>997</td>
      <td>216</td>
      <td>586</td>
      <td>796</td>
      <td>806</td>
      <td>39</td>
      <td>483</td>
      <td>518</td>
      <td>...</td>
      <td>573</td>
      <td>861</td>
      <td>366</td>
      <td>374</td>
      <td>585</td>
      <td>871</td>
      <td>140</td>
      <td>597</td>
      <td>795</td>
      <td>743</td>
    </tr>
    <tr>
      <th>746</th>
      <td>720</td>
      <td>633</td>
      <td>821</td>
      <td>149</td>
      <td>339</td>
      <td>461</td>
      <td>750</td>
      <td>194</td>
      <td>769</td>
      <td>699</td>
      <td>...</td>
      <td>913</td>
      <td>570</td>
      <td>670</td>
      <td>249</td>
      <td>840</td>
      <td>889</td>
      <td>242</td>
      <td>959</td>
      <td>791</td>
      <td>954</td>
    </tr>
    <tr>
      <th>747</th>
      <td>80</td>
      <td>268</td>
      <td>661</td>
      <td>187</td>
      <td>929</td>
      <td>469</td>
      <td>481</td>
      <td>332</td>
      <td>781</td>
      <td>615</td>
      <td>...</td>
      <td>985</td>
      <td>459</td>
      <td>965</td>
      <td>888</td>
      <td>461</td>
      <td>551</td>
      <td>465</td>
      <td>827</td>
      <td>557</td>
      <td>662</td>
    </tr>
    <tr>
      <th>748</th>
      <td>570</td>
      <td>32</td>
      <td>750</td>
      <td>332</td>
      <td>902</td>
      <td>107</td>
      <td>281</td>
      <td>667</td>
      <td>917</td>
      <td>793</td>
      <td>...</td>
      <td>924</td>
      <td>662</td>
      <td>975</td>
      <td>199</td>
      <td>32</td>
      <td>715</td>
      <td>668</td>
      <td>241</td>
      <td>299</td>
      <td>518</td>
    </tr>
    <tr>
      <th>749</th>
      <td>440</td>
      <td>589</td>
      <td>607</td>
      <td>597</td>
      <td>380</td>
      <td>961</td>
      <td>747</td>
      <td>396</td>
      <td>649</td>
      <td>974</td>
      <td>...</td>
      <td>867</td>
      <td>839</td>
      <td>234</td>
      <td>99</td>
      <td>901</td>
      <td>19</td>
      <td>821</td>
      <td>450</td>
      <td>780</td>
      <td>326</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 1000 columns</p>
</div></div>
</div>
<p>Below is the index set for 1st repeatition. From 1000 samples above, it uses 750 samples for training.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">idxs</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>0    490
1    498
2    389
3     51
4    592
Name: V1, dtype: int64
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">idxs</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tail</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>745    599
746    720
747     80
748    570
749    440
Name: V1, dtype: int64
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="3.-Prepare-the-configuration">
<h2>3. Prepare the configuration<a class="headerlink" href="#3.-Prepare-the-configuration" title="Permalink to this headline">¶</a></h2>
<p>For detailed configuration, we used python dictionary as inputs for the main training function. You can build the configuration information for the network training by: 1. the python dictionary format 1. the configufation file (.cfg).</p>
<p>In this notebook, we showed the dictionary python dictionary format configuration.</p>
<p>Please check the detailed information about each options in the <a class="reference external" href="https://young-won.github.io/deepbiome/prerequisites.html#configuration">documantation</a></p>
<div class="section" id="For-preparing-the-configuration-about-the-network-information-(network_info)">
<h3>For preparing the configuration about the network information (<code class="docutils literal notranslate"><span class="pre">network_info</span></code>)<a class="headerlink" href="#For-preparing-the-configuration-about-the-network-information-(network_info)" title="Permalink to this headline">¶</a></h3>
<p>For giving the information about the training hyper-parameter, you have to provide the dictionary for configuration to netowrk_info field. Your configuration for the network training should include the information about:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">network_info</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;architecture_info&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;batch_normalization&#39;</span><span class="p">:</span> <span class="s1">&#39;False&#39;</span><span class="p">,</span>
        <span class="s1">&#39;drop_out&#39;</span><span class="p">:</span> <span class="s1">&#39;0&#39;</span><span class="p">,</span>
        <span class="s1">&#39;weight_initial&#39;</span><span class="p">:</span> <span class="s1">&#39;glorot_uniform&#39;</span><span class="p">,</span>
        <span class="s1">&#39;weight_l1_penalty&#39;</span><span class="p">:</span><span class="s1">&#39;0.01&#39;</span><span class="p">,</span>
        <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="s1">&#39;phylogenetic_tree&#39;</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s1">&#39;model_info&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="s1">&#39;0.01&#39;</span><span class="p">,</span>
        <span class="s1">&#39;decay&#39;</span><span class="p">:</span> <span class="s1">&#39;0.001&#39;</span><span class="p">,</span>
        <span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span>
        <span class="s1">&#39;metrics&#39;</span><span class="p">:</span> <span class="s1">&#39;binary_accuracy, sensitivity, specificity, gmeasure, auc&#39;</span><span class="p">,</span>
        <span class="s1">&#39;texa_selection_metrics&#39;</span><span class="p">:</span> <span class="s1">&#39;accuracy, sensitivity, specificity, gmeasure&#39;</span><span class="p">,</span>
        <span class="s1">&#39;network_class&#39;</span><span class="p">:</span> <span class="s1">&#39;DeepBiomeNetwork&#39;</span><span class="p">,</span>
        <span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="s1">&#39;adam&#39;</span><span class="p">,</span>
        <span class="s1">&#39;reader_class&#39;</span><span class="p">:</span> <span class="s1">&#39;MicroBiomeClassificationReader&#39;</span><span class="p">,</span>
        <span class="s1">&#39;normalizer&#39;</span><span class="p">:</span> <span class="s1">&#39;normalize_minmax&#39;</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s1">&#39;training_info&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="s1">&#39;50&#39;</span><span class="p">,</span>
        <span class="s1">&#39;epochs&#39;</span><span class="p">:</span> <span class="s1">&#39;100&#39;</span>
    <span class="p">},</span>
    <span class="s1">&#39;validation_info&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="s1">&#39;None&#39;</span><span class="p">,</span>
        <span class="s1">&#39;validation_size&#39;</span><span class="p">:</span> <span class="s1">&#39;0.2&#39;</span>
    <span class="p">},</span>
    <span class="s1">&#39;test_info&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="s1">&#39;None&#39;</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="For-preparing-the-configuration-about-the-path-information-(path_info)">
<h3>For preparing the configuration about the path information (<code class="docutils literal notranslate"><span class="pre">path_info</span></code>)<a class="headerlink" href="#For-preparing-the-configuration-about-the-path-information-(path_info)" title="Permalink to this headline">¶</a></h3>
<p>For giving the information about the path of dataset, paths for saving the trained weight and the evaluation results, you have to provide the dictionary for configuration to path_info feild. Your configuration for the path information should include the information about:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">path_info</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;data_info&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;count_list_path&#39;</span><span class="p">:</span> <span class="n">resource_filename</span><span class="p">(</span><span class="s1">&#39;deepbiome&#39;</span><span class="p">,</span> <span class="s1">&#39;tests/data/gcount_list.csv&#39;</span><span class="p">),</span>
        <span class="s1">&#39;count_path&#39;</span><span class="p">:</span> <span class="n">resource_filename</span><span class="p">(</span><span class="s1">&#39;deepbiome&#39;</span><span class="p">,</span> <span class="s1">&#39;tests/data/count&#39;</span><span class="p">),</span>
        <span class="s1">&#39;data_path&#39;</span><span class="p">:</span> <span class="n">resource_filename</span><span class="p">(</span><span class="s1">&#39;deepbiome&#39;</span><span class="p">,</span> <span class="s1">&#39;tests/data&#39;</span><span class="p">),</span>
        <span class="s1">&#39;idx_path&#39;</span><span class="p">:</span> <span class="n">resource_filename</span><span class="p">(</span><span class="s1">&#39;deepbiome&#39;</span><span class="p">,</span> <span class="s1">&#39;tests/data/classification_idx.csv&#39;</span><span class="p">),</span>
        <span class="s1">&#39;tree_info_path&#39;</span><span class="p">:</span> <span class="n">resource_filename</span><span class="p">(</span><span class="s1">&#39;deepbiome&#39;</span><span class="p">,</span> <span class="s1">&#39;tests/data/genus48_dic.csv&#39;</span><span class="p">),</span>
        <span class="s1">&#39;x_path&#39;</span><span class="p">:</span> <span class="s1">&#39;&#39;</span><span class="p">,</span>
        <span class="s1">&#39;y_path&#39;</span><span class="p">:</span> <span class="s1">&#39;classification_y.csv&#39;</span>
    <span class="p">},</span>
    <span class="s1">&#39;model_info&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;evaluation&#39;</span><span class="p">:</span> <span class="s1">&#39;eval.npy&#39;</span><span class="p">,</span>
        <span class="s1">&#39;history&#39;</span><span class="p">:</span> <span class="s1">&#39;hist.json&#39;</span><span class="p">,</span>
        <span class="s1">&#39;model_dir&#39;</span><span class="p">:</span> <span class="s1">&#39;./example_result/&#39;</span><span class="p">,</span>
        <span class="s1">&#39;weight&#39;</span><span class="p">:</span> <span class="s1">&#39;weight.h5&#39;</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="4.-Deepbiome-Training">
<h2>4. Deepbiome Training<a class="headerlink" href="#4.-Deepbiome-Training" title="Permalink to this headline">¶</a></h2>
<p>Now we can train the deepbiome network base on the configurations.</p>
<p>For logging, we used the python logging library.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">format</span> <span class="o">=</span> <span class="s1">&#39;[</span><span class="si">%(name)-8s</span><span class="s1">|</span><span class="si">%(levelname)s</span><span class="s1">|</span><span class="si">%(filename)s</span><span class="s1">:</span><span class="si">%(lineno)s</span><span class="s1">] </span><span class="si">%(message)s</span><span class="s1">&#39;</span><span class="p">,</span>
                    <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">DEBUG</span><span class="p">)</span>
<span class="n">log</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>The deeobiome_train function provide the test evaluation, train evaluation and the deepbiome network instance.</p>
<p>If we set <code class="docutils literal notranslate"><span class="pre">number_of_fold</span></code>, then the deepbiome package do the cross-validation based on that value. If not, the deepbiome package do the cross-validation based on the index file. If both <code class="docutils literal notranslate"><span class="pre">number_of_fold</span></code> option and the index file is not given, then the library do leave-one-out-cross-validation (LOOCV).</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">test_evaluation</span><span class="p">,</span> <span class="n">train_evaluation</span><span class="p">,</span> <span class="n">network</span> <span class="o">=</span> <span class="n">deepbiome</span><span class="o">.</span><span class="n">deepbiome_train</span><span class="p">(</span><span class="n">log</span><span class="p">,</span> <span class="n">network_info</span><span class="p">,</span> <span class="n">path_info</span><span class="p">,</span> <span class="n">number_of_fold</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|deepbiome.py:100] -----------------------------------------------------------------
[root    |INFO|deepbiome.py:137] -------1 simulation start!----------------------------------
[root    |INFO|readers.py:58] -----------------------------------------------------------------------
[root    |INFO|readers.py:59] Construct Dataset
[root    |INFO|readers.py:60] -----------------------------------------------------------------------
[root    |INFO|readers.py:61] Load data
[root    |INFO|deepbiome.py:147] -----------------------------------------------------------------
[root    |INFO|deepbiome.py:148] Build network for 1 simulation
[root    |INFO|build_network.py:513] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:514] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv
[root    |INFO|build_network.py:518] Phylogenetic tree level list: [&#39;Genus&#39;, &#39;Family&#39;, &#39;Order&#39;, &#39;Class&#39;, &#39;Phylum&#39;]
[root    |INFO|build_network.py:519] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:526]      Genus: 48
[root    |INFO|build_network.py:526]     Family: 40
[root    |INFO|build_network.py:526]      Order: 23
[root    |INFO|build_network.py:526]      Class: 17
[root    |INFO|build_network.py:526]     Phylum: 9
[root    |INFO|build_network.py:529] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:530] Phylogenetic_tree_dict info: [&#39;Genus&#39;, &#39;Number&#39;, &#39;Order&#39;, &#39;Class&#39;, &#39;Phylum&#39;, &#39;Family&#39;]
[root    |INFO|build_network.py:531] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:541] Build edge weights between [ Genus, Family]
[root    |INFO|build_network.py:541] Build edge weights between [Family,  Order]
[root    |INFO|build_network.py:541] Build edge weights between [ Order,  Class]
[root    |INFO|build_network.py:541] Build edge weights between [ Class, Phylum]
[root    |INFO|build_network.py:554] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:570] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:571] Build network based on phylogenetic tree information
[root    |INFO|build_network.py:572] ------------------------------------------------------------------------------------------
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:432: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[tensorflow|WARNING|deprecation.py:328] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:432: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
[root    |INFO|build_network.py:648] ------------------------------------------------------------------------------------------
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Model: &#34;model_1&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input (InputLayer)           (None, 48)                0
_________________________________________________________________
l1_dense (Dense_with_tree)   (None, 40)                1960
_________________________________________________________________
l1_activation (Activation)   (None, 40)                0
_________________________________________________________________
l2_dense (Dense_with_tree)   (None, 23)                943
_________________________________________________________________
l2_activation (Activation)   (None, 23)                0
_________________________________________________________________
l3_dense (Dense_with_tree)   (None, 17)                408
_________________________________________________________________
l3_activation (Activation)   (None, 17)                0
_________________________________________________________________
l4_dense (Dense_with_tree)   (None, 9)                 162
_________________________________________________________________
l4_activation (Activation)   (None, 9)                 0
_________________________________________________________________
last_dense_h (Dense)         (None, 1)                 10
_________________________________________________________________
p_hat (Activation)           (None, 1)                 0
=================================================================
Total params: 3,483
Trainable params: 3,483
Non-trainable params: 0
_________________________________________________________________
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:57] Build Network
[root    |INFO|build_network.py:58] Optimizer = adam
[root    |INFO|build_network.py:59] Loss = binary_crossentropy
[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc
[root    |INFO|deepbiome.py:157] -----------------------------------------------------------------
[root    |INFO|deepbiome.py:158] 1 fold computing start!----------------------------------
[root    |INFO|build_network.py:141] Training start!
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:2862: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[tensorflow|WARNING|deprecation.py:328] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:2862: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Train on 600 samples, validate on 150 samples
Epoch 1/100
600/600 [==============================] - 1s 1ms/step - loss: 0.6576 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5630 - val_loss: 0.6175 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5267
Epoch 2/100
600/600 [==============================] - 0s 198us/step - loss: 0.6214 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6014 - val_loss: 0.6127 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5567
Epoch 3/100
600/600 [==============================] - 0s 196us/step - loss: 0.6238 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6623 - val_loss: 0.6111 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6093
Epoch 4/100
600/600 [==============================] - 0s 174us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7046 - val_loss: 0.6114 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6240
Epoch 5/100
600/600 [==============================] - 0s 203us/step - loss: 0.6209 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7137 - val_loss: 0.6116 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6208
Epoch 6/100
600/600 [==============================] - 0s 189us/step - loss: 0.6205 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7245 - val_loss: 0.6111 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6041
Epoch 7/100
600/600 [==============================] - 0s 194us/step - loss: 0.6209 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7234 - val_loss: 0.6108 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6189
Epoch 8/100
600/600 [==============================] - 0s 192us/step - loss: 0.6209 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7370 - val_loss: 0.6112 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6249
Epoch 9/100
600/600 [==============================] - 0s 205us/step - loss: 0.6208 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7297 - val_loss: 0.6112 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6374
Epoch 10/100
600/600 [==============================] - 0s 197us/step - loss: 0.6201 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7240 - val_loss: 0.6107 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6425
Epoch 11/100
600/600 [==============================] - 0s 223us/step - loss: 0.6201 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7257 - val_loss: 0.6101 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6467
Epoch 12/100
600/600 [==============================] - 0s 189us/step - loss: 0.6212 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7192 - val_loss: 0.6103 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6459
Epoch 13/100
600/600 [==============================] - 0s 210us/step - loss: 0.6185 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7220 - val_loss: 0.6088 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6560
Epoch 14/100
600/600 [==============================] - 0s 191us/step - loss: 0.6178 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7234 - val_loss: 0.6078 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6582
Epoch 15/100
600/600 [==============================] - 0s 174us/step - loss: 0.6155 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7313 - val_loss: 0.6061 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6627
Epoch 16/100
600/600 [==============================] - 0s 178us/step - loss: 0.6128 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7211 - val_loss: 0.6048 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6617
Epoch 17/100
600/600 [==============================] - 0s 201us/step - loss: 0.6094 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7245 - val_loss: 0.6012 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6652
Epoch 18/100
600/600 [==============================] - 0s 209us/step - loss: 0.6051 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7240 - val_loss: 0.5978 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6675
Epoch 19/100
600/600 [==============================] - 0s 200us/step - loss: 0.6000 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7317 - val_loss: 0.5937 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6670
Epoch 20/100
600/600 [==============================] - 0s 193us/step - loss: 0.5930 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7236 - val_loss: 0.5892 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6681
Epoch 21/100
600/600 [==============================] - 0s 184us/step - loss: 0.5828 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7364 - val_loss: 0.5857 - val_binary_accuracy: 0.7000 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6706
Epoch 22/100
600/600 [==============================] - 0s 211us/step - loss: 0.5696 - binary_accuracy: 0.7167 - sensitivity: 0.9898 - specificity: 0.1067 - gmeasure: 0.2599 - auc: 0.7340 - val_loss: 0.5781 - val_binary_accuracy: 0.6800 - val_sensitivity: 0.9278 - val_specificity: 0.0874 - val_gmeasure: 0.2839 - val_auc: 0.6735
Epoch 23/100
600/600 [==============================] - 0s 216us/step - loss: 0.5606 - binary_accuracy: 0.7183 - sensitivity: 0.9634 - specificity: 0.1826 - gmeasure: 0.4093 - auc: 0.7270 - val_loss: 0.5774 - val_binary_accuracy: 0.6800 - val_sensitivity: 0.9370 - val_specificity: 0.0708 - val_gmeasure: 0.2565 - val_auc: 0.6789
Epoch 24/100
600/600 [==============================] - ETA: 0s - loss: 0.5374 - binary_accuracy: 0.7371 - sensitivity: 0.9557 - specificity: 0.2258 - gmeasure: 0.4586 - auc: 0.73 - 0s 211us/step - loss: 0.5482 - binary_accuracy: 0.7217 - sensitivity: 0.9570 - specificity: 0.2095 - gmeasure: 0.4416 - auc: 0.7392 - val_loss: 0.5711 - val_binary_accuracy: 0.7067 - val_sensitivity: 0.9167 - val_specificity: 0.1916 - val_gmeasure: 0.4149 - val_auc: 0.6809
Epoch 25/100
600/600 [==============================] - 0s 190us/step - loss: 0.5410 - binary_accuracy: 0.7283 - sensitivity: 0.9541 - specificity: 0.2243 - gmeasure: 0.4527 - auc: 0.7495 - val_loss: 0.5706 - val_binary_accuracy: 0.7067 - val_sensitivity: 0.8996 - val_specificity: 0.2385 - val_gmeasure: 0.4568 - val_auc: 0.6852
Epoch 26/100
600/600 [==============================] - 0s 188us/step - loss: 0.5349 - binary_accuracy: 0.7267 - sensitivity: 0.9477 - specificity: 0.2443 - gmeasure: 0.4747 - auc: 0.7590 - val_loss: 0.5638 - val_binary_accuracy: 0.7067 - val_sensitivity: 0.9167 - val_specificity: 0.1916 - val_gmeasure: 0.4149 - val_auc: 0.6937
Epoch 27/100
600/600 [==============================] - 0s 208us/step - loss: 0.5259 - binary_accuracy: 0.7333 - sensitivity: 0.9486 - specificity: 0.2729 - gmeasure: 0.4970 - auc: 0.7647 - val_loss: 0.5645 - val_binary_accuracy: 0.6933 - val_sensitivity: 0.8799 - val_specificity: 0.2385 - val_gmeasure: 0.4512 - val_auc: 0.6994
Epoch 28/100
600/600 [==============================] - 0s 206us/step - loss: 0.5193 - binary_accuracy: 0.7417 - sensitivity: 0.9419 - specificity: 0.3002 - gmeasure: 0.5128 - auc: 0.7719 - val_loss: 0.5646 - val_binary_accuracy: 0.7067 - val_sensitivity: 0.9167 - val_specificity: 0.1916 - val_gmeasure: 0.4149 - val_auc: 0.7094
Epoch 29/100
600/600 [==============================] - 0s 194us/step - loss: 0.5195 - binary_accuracy: 0.7450 - sensitivity: 0.9371 - specificity: 0.3220 - gmeasure: 0.5334 - auc: 0.7779 - val_loss: 0.5617 - val_binary_accuracy: 0.7000 - val_sensitivity: 0.9174 - val_specificity: 0.1613 - val_gmeasure: 0.3738 - val_auc: 0.7151
Epoch 30/100
600/600 [==============================] - 0s 197us/step - loss: 0.5083 - binary_accuracy: 0.7450 - sensitivity: 0.9377 - specificity: 0.3321 - gmeasure: 0.5519 - auc: 0.7883 - val_loss: 0.5526 - val_binary_accuracy: 0.7067 - val_sensitivity: 0.9174 - val_specificity: 0.1916 - val_gmeasure: 0.4149 - val_auc: 0.7189
Epoch 31/100
600/600 [==============================] - 0s 211us/step - loss: 0.5204 - binary_accuracy: 0.7367 - sensitivity: 0.8967 - specificity: 0.4135 - gmeasure: 0.5758 - auc: 0.7954 - val_loss: 0.5814 - val_binary_accuracy: 0.7000 - val_sensitivity: 0.9370 - val_specificity: 0.1279 - val_gmeasure: 0.3431 - val_auc: 0.7222
Epoch 32/100
600/600 [==============================] - 0s 212us/step - loss: 0.5274 - binary_accuracy: 0.7150 - sensitivity: 0.8618 - specificity: 0.3909 - gmeasure: 0.5340 - auc: 0.7910 - val_loss: 0.5414 - val_binary_accuracy: 0.7200 - val_sensitivity: 0.8707 - val_specificity: 0.3361 - val_gmeasure: 0.5352 - val_auc: 0.7297
Epoch 33/100
600/600 [==============================] - 0s 198us/step - loss: 0.5197 - binary_accuracy: 0.7317 - sensitivity: 0.9171 - specificity: 0.3443 - gmeasure: 0.5242 - auc: 0.8053 - val_loss: 0.5558 - val_binary_accuracy: 0.6867 - val_sensitivity: 0.7608 - val_specificity: 0.5015 - val_gmeasure: 0.6170 - val_auc: 0.7414
Epoch 34/100
600/600 [==============================] - 0s 189us/step - loss: 0.5024 - binary_accuracy: 0.7550 - sensitivity: 0.9402 - specificity: 0.3513 - gmeasure: 0.5525 - auc: 0.8037 - val_loss: 0.5329 - val_binary_accuracy: 0.7200 - val_sensitivity: 0.8707 - val_specificity: 0.3361 - val_gmeasure: 0.5352 - val_auc: 0.7439
Epoch 35/100
600/600 [==============================] - 0s 199us/step - loss: 0.4963 - binary_accuracy: 0.7567 - sensitivity: 0.9143 - specificity: 0.4218 - gmeasure: 0.5999 - auc: 0.8171 - val_loss: 0.5310 - val_binary_accuracy: 0.7067 - val_sensitivity: 0.9174 - val_specificity: 0.1916 - val_gmeasure: 0.4149 - val_auc: 0.7458
Epoch 36/100
600/600 [==============================] - 0s 206us/step - loss: 0.4801 - binary_accuracy: 0.7650 - sensitivity: 0.9325 - specificity: 0.3923 - gmeasure: 0.5957 - auc: 0.8124 - val_loss: 0.5239 - val_binary_accuracy: 0.7333 - val_sensitivity: 0.8977 - val_specificity: 0.3195 - val_gmeasure: 0.5313 - val_auc: 0.7512
Epoch 37/100
600/600 [==============================] - 0s 211us/step - loss: 0.4753 - binary_accuracy: 0.7567 - sensitivity: 0.9431 - specificity: 0.3619 - gmeasure: 0.5743 - auc: 0.8261 - val_loss: 0.5185 - val_binary_accuracy: 0.7267 - val_sensitivity: 0.8799 - val_specificity: 0.3361 - val_gmeasure: 0.5378 - val_auc: 0.7591
Epoch 38/100
600/600 [==============================] - 0s 198us/step - loss: 0.4705 - binary_accuracy: 0.7633 - sensitivity: 0.9178 - specificity: 0.4264 - gmeasure: 0.6163 - auc: 0.8272 - val_loss: 0.5171 - val_binary_accuracy: 0.7267 - val_sensitivity: 0.9174 - val_specificity: 0.2558 - val_gmeasure: 0.4801 - val_auc: 0.7662
Epoch 39/100
600/600 [==============================] - 0s 179us/step - loss: 0.4643 - binary_accuracy: 0.7667 - sensitivity: 0.9175 - specificity: 0.4337 - gmeasure: 0.6232 - auc: 0.8392 - val_loss: 0.5136 - val_binary_accuracy: 0.7333 - val_sensitivity: 0.9285 - val_specificity: 0.2558 - val_gmeasure: 0.4834 - val_auc: 0.7775
Epoch 40/100
600/600 [==============================] - 0s 191us/step - loss: 0.4549 - binary_accuracy: 0.7817 - sensitivity: 0.9429 - specificity: 0.4292 - gmeasure: 0.6255 - auc: 0.8349 - val_loss: 0.5045 - val_binary_accuracy: 0.7200 - val_sensitivity: 0.8083 - val_specificity: 0.5015 - val_gmeasure: 0.6358 - val_auc: 0.7827
Epoch 41/100
600/600 [==============================] - 0s 183us/step - loss: 0.4534 - binary_accuracy: 0.7733 - sensitivity: 0.9169 - specificity: 0.4668 - gmeasure: 0.6395 - auc: 0.8438 - val_loss: 0.4935 - val_binary_accuracy: 0.7400 - val_sensitivity: 0.8654 - val_specificity: 0.4236 - val_gmeasure: 0.6013 - val_auc: 0.7896
Epoch 42/100
600/600 [==============================] - 0s 196us/step - loss: 0.4449 - binary_accuracy: 0.7900 - sensitivity: 0.9140 - specificity: 0.5159 - gmeasure: 0.6761 - auc: 0.8460 - val_loss: 0.4974 - val_binary_accuracy: 0.7400 - val_sensitivity: 0.9285 - val_specificity: 0.2725 - val_gmeasure: 0.4970 - val_auc: 0.7917
Epoch 43/100
600/600 [==============================] - 0s 206us/step - loss: 0.4470 - binary_accuracy: 0.7683 - sensitivity: 0.8836 - specificity: 0.5144 - gmeasure: 0.6561 - auc: 0.8522 - val_loss: 0.4973 - val_binary_accuracy: 0.7467 - val_sensitivity: 0.9507 - val_specificity: 0.2558 - val_gmeasure: 0.4898 - val_auc: 0.7995
Epoch 44/100
600/600 [==============================] - 0s 209us/step - loss: 0.4405 - binary_accuracy: 0.7750 - sensitivity: 0.9100 - specificity: 0.4737 - gmeasure: 0.6308 - auc: 0.8621 - val_loss: 0.4784 - val_binary_accuracy: 0.7333 - val_sensitivity: 0.8298 - val_specificity: 0.5015 - val_gmeasure: 0.6448 - val_auc: 0.8074
Epoch 45/100
600/600 [==============================] - 0s 201us/step - loss: 0.4265 - binary_accuracy: 0.8017 - sensitivity: 0.9268 - specificity: 0.5117 - gmeasure: 0.6771 - auc: 0.8634 - val_loss: 0.4723 - val_binary_accuracy: 0.7400 - val_sensitivity: 0.8943 - val_specificity: 0.3528 - val_gmeasure: 0.5551 - val_auc: 0.8162
Epoch 46/100
600/600 [==============================] - 0s 211us/step - loss: 0.4245 - binary_accuracy: 0.8017 - sensitivity: 0.9123 - specificity: 0.5588 - gmeasure: 0.7101 - auc: 0.8679 - val_loss: 0.4650 - val_binary_accuracy: 0.7267 - val_sensitivity: 0.8383 - val_specificity: 0.4474 - val_gmeasure: 0.6102 - val_auc: 0.8232
Epoch 47/100
600/600 [==============================] - 0s 198us/step - loss: 0.4153 - binary_accuracy: 0.8033 - sensitivity: 0.9115 - specificity: 0.5528 - gmeasure: 0.6995 - auc: 0.8728 - val_loss: 0.4702 - val_binary_accuracy: 0.7533 - val_sensitivity: 0.9396 - val_specificity: 0.2892 - val_gmeasure: 0.5135 - val_auc: 0.8276
Epoch 48/100
600/600 [==============================] - 0s 212us/step - loss: 0.4065 - binary_accuracy: 0.8200 - sensitivity: 0.9286 - specificity: 0.5740 - gmeasure: 0.7270 - auc: 0.8812 - val_loss: 0.4793 - val_binary_accuracy: 0.7400 - val_sensitivity: 0.7449 - val_specificity: 0.7377 - val_gmeasure: 0.7409 - val_auc: 0.8383
Epoch 49/100
600/600 [==============================] - 0s 193us/step - loss: 0.4357 - binary_accuracy: 0.7800 - sensitivity: 0.8681 - specificity: 0.5711 - gmeasure: 0.6786 - auc: 0.8751 - val_loss: 0.5041 - val_binary_accuracy: 0.7533 - val_sensitivity: 0.7067 - val_specificity: 0.8864 - val_gmeasure: 0.7894 - val_auc: 0.8374
Epoch 50/100
600/600 [==============================] - 0s 203us/step - loss: 0.4252 - binary_accuracy: 0.7867 - sensitivity: 0.8722 - specificity: 0.5805 - gmeasure: 0.6600 - auc: 0.8785 - val_loss: 0.4504 - val_binary_accuracy: 0.7600 - val_sensitivity: 0.8298 - val_specificity: 0.5890 - val_gmeasure: 0.6986 - val_auc: 0.8453
Epoch 51/100
600/600 [==============================] - 0s 199us/step - loss: 0.4026 - binary_accuracy: 0.8050 - sensitivity: 0.9074 - specificity: 0.5862 - gmeasure: 0.7222 - auc: 0.8882 - val_loss: 0.4401 - val_binary_accuracy: 0.7600 - val_sensitivity: 0.8580 - val_specificity: 0.5110 - val_gmeasure: 0.6594 - val_auc: 0.8482
Epoch 52/100
600/600 [==============================] - 0s 212us/step - loss: 0.4085 - binary_accuracy: 0.8067 - sensitivity: 0.8934 - specificity: 0.6049 - gmeasure: 0.7200 - auc: 0.8825 - val_loss: 0.4404 - val_binary_accuracy: 0.7667 - val_sensitivity: 0.8409 - val_specificity: 0.5890 - val_gmeasure: 0.7035 - val_auc: 0.8531
Epoch 53/100
600/600 [==============================] - 0s 195us/step - loss: 0.3928 - binary_accuracy: 0.8167 - sensitivity: 0.8953 - specificity: 0.6509 - gmeasure: 0.7542 - auc: 0.8907 - val_loss: 0.4340 - val_binary_accuracy: 0.7733 - val_sensitivity: 0.9251 - val_specificity: 0.3998 - val_gmeasure: 0.6024 - val_auc: 0.8549
Epoch 54/100
600/600 [==============================] - 0s 200us/step - loss: 0.3890 - binary_accuracy: 0.8250 - sensitivity: 0.9336 - specificity: 0.5893 - gmeasure: 0.7371 - auc: 0.8957 - val_loss: 0.4241 - val_binary_accuracy: 0.7667 - val_sensitivity: 0.8665 - val_specificity: 0.5110 - val_gmeasure: 0.6625 - val_auc: 0.8602
Epoch 55/100
600/600 [==============================] - 0s 206us/step - loss: 0.3844 - binary_accuracy: 0.8350 - sensitivity: 0.9057 - specificity: 0.6767 - gmeasure: 0.7751 - auc: 0.8998 - val_loss: 0.4247 - val_binary_accuracy: 0.7733 - val_sensitivity: 0.9066 - val_specificity: 0.4331 - val_gmeasure: 0.6180 - val_auc: 0.8614
Epoch 56/100
600/600 [==============================] - 0s 216us/step - loss: 0.3823 - binary_accuracy: 0.8350 - sensitivity: 0.8976 - specificity: 0.6914 - gmeasure: 0.7784 - auc: 0.8991 - val_loss: 0.4730 - val_binary_accuracy: 0.7467 - val_sensitivity: 0.9711 - val_specificity: 0.2089 - val_gmeasure: 0.4388 - val_auc: 0.8620
Epoch 57/100
600/600 [==============================] - 0s 214us/step - loss: 0.3906 - binary_accuracy: 0.8250 - sensitivity: 0.8966 - specificity: 0.6561 - gmeasure: 0.7449 - auc: 0.9015 - val_loss: 0.4831 - val_binary_accuracy: 0.7400 - val_sensitivity: 0.9711 - val_specificity: 0.1851 - val_gmeasure: 0.4152 - val_auc: 0.8634
Epoch 58/100
600/600 [==============================] - 0s 204us/step - loss: 0.3848 - binary_accuracy: 0.8200 - sensitivity: 0.8965 - specificity: 0.6419 - gmeasure: 0.7430 - auc: 0.9059 - val_loss: 0.4332 - val_binary_accuracy: 0.7667 - val_sensitivity: 0.9514 - val_specificity: 0.3195 - val_gmeasure: 0.5489 - val_auc: 0.8683
Epoch 59/100
600/600 [==============================] - 0s 208us/step - loss: 0.3827 - binary_accuracy: 0.8100 - sensitivity: 0.8930 - specificity: 0.6513 - gmeasure: 0.7500 - auc: 0.9118 - val_loss: 0.4755 - val_binary_accuracy: 0.7467 - val_sensitivity: 0.9711 - val_specificity: 0.2154 - val_gmeasure: 0.4563 - val_auc: 0.8662
Epoch 60/100
600/600 [==============================] - 0s 208us/step - loss: 0.4265 - binary_accuracy: 0.7967 - sensitivity: 0.8874 - specificity: 0.5990 - gmeasure: 0.6891 - auc: 0.9077 - val_loss: 0.5220 - val_binary_accuracy: 0.7400 - val_sensitivity: 0.9822 - val_specificity: 0.1684 - val_gmeasure: 0.4004 - val_auc: 0.8705
Epoch 61/100
600/600 [==============================] - ETA: 0s - loss: 0.4746 - binary_accuracy: 0.7657 - sensitivity: 0.8017 - specificity: 0.7252 - gmeasure: 0.7270 - auc: 0.90 - 0s 213us/step - loss: 0.4165 - binary_accuracy: 0.7950 - sensitivity: 0.8647 - specificity: 0.6438 - gmeasure: 0.7185 - auc: 0.9103 - val_loss: 0.4776 - val_binary_accuracy: 0.7467 - val_sensitivity: 0.9711 - val_specificity: 0.2154 - val_gmeasure: 0.4563 - val_auc: 0.8723
Epoch 62/100
600/600 [==============================] - 0s 209us/step - loss: 0.3678 - binary_accuracy: 0.8300 - sensitivity: 0.9207 - specificity: 0.6399 - gmeasure: 0.7579 - auc: 0.9118 - val_loss: 0.4014 - val_binary_accuracy: 0.8000 - val_sensitivity: 0.9073 - val_specificity: 0.5413 - val_gmeasure: 0.6984 - val_auc: 0.8800
Epoch 63/100
600/600 [==============================] - 0s 194us/step - loss: 0.3555 - binary_accuracy: 0.8517 - sensitivity: 0.9193 - specificity: 0.7055 - gmeasure: 0.8025 - auc: 0.9104 - val_loss: 0.3994 - val_binary_accuracy: 0.7933 - val_sensitivity: 0.9158 - val_specificity: 0.4872 - val_gmeasure: 0.6633 - val_auc: 0.8820
Epoch 64/100
600/600 [==============================] - 0s 192us/step - loss: 0.3604 - binary_accuracy: 0.8250 - sensitivity: 0.8891 - specificity: 0.6938 - gmeasure: 0.7796 - auc: 0.9182 - val_loss: 0.4044 - val_binary_accuracy: 0.8067 - val_sensitivity: 0.8316 - val_specificity: 0.7478 - val_gmeasure: 0.7875 - val_auc: 0.8851
Epoch 65/100
600/600 [==============================] - 0s 194us/step - loss: 0.3497 - binary_accuracy: 0.8500 - sensitivity: 0.9262 - specificity: 0.6821 - gmeasure: 0.7891 - auc: 0.9169 - val_loss: 0.3982 - val_binary_accuracy: 0.8067 - val_sensitivity: 0.8402 - val_specificity: 0.7312 - val_gmeasure: 0.7822 - val_auc: 0.8874
Epoch 66/100
600/600 [==============================] - 0s 193us/step - loss: 0.3575 - binary_accuracy: 0.8300 - sensitivity: 0.8935 - specificity: 0.6902 - gmeasure: 0.7719 - auc: 0.9128 - val_loss: 0.3931 - val_binary_accuracy: 0.8000 - val_sensitivity: 0.8494 - val_specificity: 0.6907 - val_gmeasure: 0.7652 - val_auc: 0.8893
Epoch 67/100
600/600 [==============================] - 0s 209us/step - loss: 0.3528 - binary_accuracy: 0.8433 - sensitivity: 0.8999 - specificity: 0.7291 - gmeasure: 0.8052 - auc: 0.9176 - val_loss: 0.3912 - val_binary_accuracy: 0.7867 - val_sensitivity: 0.9158 - val_specificity: 0.4641 - val_gmeasure: 0.6488 - val_auc: 0.8892
Epoch 68/100
600/600 [==============================] - 0s 201us/step - loss: 0.3404 - binary_accuracy: 0.8617 - sensitivity: 0.9370 - specificity: 0.7022 - gmeasure: 0.8043 - auc: 0.9186 - val_loss: 0.3831 - val_binary_accuracy: 0.8000 - val_sensitivity: 0.8980 - val_specificity: 0.5652 - val_gmeasure: 0.7117 - val_auc: 0.8944
Epoch 69/100
600/600 [==============================] - 0s 201us/step - loss: 0.3413 - binary_accuracy: 0.8600 - sensitivity: 0.9172 - specificity: 0.7319 - gmeasure: 0.8140 - auc: 0.9269 - val_loss: 0.3794 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8802 - val_specificity: 0.6604 - val_gmeasure: 0.7603 - val_auc: 0.8965
Epoch 70/100
600/600 [==============================] - 0s 200us/step - loss: 0.3450 - binary_accuracy: 0.8450 - sensitivity: 0.9060 - specificity: 0.7206 - gmeasure: 0.8033 - auc: 0.9224 - val_loss: 0.3816 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8494 - val_specificity: 0.7615 - val_gmeasure: 0.8035 - val_auc: 0.9009
Epoch 71/100
600/600 [==============================] - 0s 199us/step - loss: 0.3442 - binary_accuracy: 0.8433 - sensitivity: 0.9012 - specificity: 0.7112 - gmeasure: 0.7965 - auc: 0.9258 - val_loss: 0.3755 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8802 - val_specificity: 0.6842 - val_gmeasure: 0.7725 - val_auc: 0.9011
Epoch 72/100
600/600 [==============================] - 0s 196us/step - loss: 0.3367 - binary_accuracy: 0.8483 - sensitivity: 0.9155 - specificity: 0.7152 - gmeasure: 0.8020 - auc: 0.9289 - val_loss: 0.3750 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8802 - val_specificity: 0.7615 - val_gmeasure: 0.8173 - val_auc: 0.9037
Epoch 73/100
600/600 [==============================] - 0s 193us/step - loss: 0.3377 - binary_accuracy: 0.8567 - sensitivity: 0.9134 - specificity: 0.7242 - gmeasure: 0.8051 - auc: 0.9304 - val_loss: 0.3696 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8980 - val_specificity: 0.6128 - val_gmeasure: 0.7415 - val_auc: 0.9037
Epoch 74/100
600/600 [==============================] - 0s 207us/step - loss: 0.3236 - binary_accuracy: 0.8700 - sensitivity: 0.9366 - specificity: 0.7099 - gmeasure: 0.8112 - auc: 0.9267 - val_loss: 0.3763 - val_binary_accuracy: 0.7733 - val_sensitivity: 0.9066 - val_specificity: 0.4474 - val_gmeasure: 0.6350 - val_auc: 0.9044
Epoch 75/100
600/600 [==============================] - 0s 210us/step - loss: 0.3180 - binary_accuracy: 0.8550 - sensitivity: 0.9260 - specificity: 0.7070 - gmeasure: 0.8075 - auc: 0.9312 - val_loss: 0.3761 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.8494 - val_specificity: 0.8561 - val_gmeasure: 0.8512 - val_auc: 0.9067
Epoch 76/100
600/600 [==============================] - 0s 190us/step - loss: 0.3287 - binary_accuracy: 0.8550 - sensitivity: 0.8982 - specificity: 0.7656 - gmeasure: 0.8258 - auc: 0.9329 - val_loss: 0.3720 - val_binary_accuracy: 0.7800 - val_sensitivity: 0.9066 - val_specificity: 0.4641 - val_gmeasure: 0.6456 - val_auc: 0.9076
Epoch 77/100
600/600 [==============================] - 0s 206us/step - loss: 0.3274 - binary_accuracy: 0.8550 - sensitivity: 0.9280 - specificity: 0.6790 - gmeasure: 0.7736 - auc: 0.9322 - val_loss: 0.3632 - val_binary_accuracy: 0.8067 - val_sensitivity: 0.9066 - val_specificity: 0.5658 - val_gmeasure: 0.7143 - val_auc: 0.9062
Epoch 78/100
600/600 [==============================] - 0s 199us/step - loss: 0.3119 - binary_accuracy: 0.8750 - sensitivity: 0.9331 - specificity: 0.7385 - gmeasure: 0.8254 - auc: 0.9336 - val_loss: 0.3598 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8895 - val_specificity: 0.7442 - val_gmeasure: 0.8124 - val_auc: 0.9067
Epoch 79/100
600/600 [==============================] - 0s 206us/step - loss: 0.3192 - binary_accuracy: 0.8583 - sensitivity: 0.9052 - specificity: 0.7642 - gmeasure: 0.8255 - auc: 0.9365 - val_loss: 0.3565 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8895 - val_specificity: 0.7442 - val_gmeasure: 0.8124 - val_auc: 0.9085
Epoch 80/100
600/600 [==============================] - 0s 194us/step - loss: 0.3134 - binary_accuracy: 0.8667 - sensitivity: 0.9246 - specificity: 0.7507 - gmeasure: 0.8302 - auc: 0.9336 - val_loss: 0.3579 - val_binary_accuracy: 0.7933 - val_sensitivity: 0.9066 - val_specificity: 0.5117 - val_gmeasure: 0.6774 - val_auc: 0.9079
Epoch 81/100
600/600 [==============================] - 0s 217us/step - loss: 0.3016 - binary_accuracy: 0.8733 - sensitivity: 0.9288 - specificity: 0.7390 - gmeasure: 0.8248 - auc: 0.9371 - val_loss: 0.3594 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.8605 - val_specificity: 0.8489 - val_gmeasure: 0.8540 - val_auc: 0.9115
Epoch 82/100
600/600 [==============================] - 0s 208us/step - loss: 0.3021 - binary_accuracy: 0.8717 - sensitivity: 0.9201 - specificity: 0.7699 - gmeasure: 0.8398 - auc: 0.9350 - val_loss: 0.3505 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8895 - val_specificity: 0.7305 - val_gmeasure: 0.8060 - val_auc: 0.9119
Epoch 83/100
600/600 [==============================] - 0s 190us/step - loss: 0.3103 - binary_accuracy: 0.8700 - sensitivity: 0.9204 - specificity: 0.7775 - gmeasure: 0.8434 - auc: 0.9434 - val_loss: 0.3623 - val_binary_accuracy: 0.7867 - val_sensitivity: 0.9262 - val_specificity: 0.4545 - val_gmeasure: 0.6471 - val_auc: 0.9118
Epoch 84/100
600/600 [==============================] - 0s 202us/step - loss: 0.2970 - binary_accuracy: 0.8833 - sensitivity: 0.9317 - specificity: 0.7774 - gmeasure: 0.8497 - auc: 0.9394 - val_loss: 0.3504 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.9066 - val_specificity: 0.5896 - val_gmeasure: 0.7279 - val_auc: 0.9124
Epoch 85/100
600/600 [==============================] - 0s 209us/step - loss: 0.2989 - binary_accuracy: 0.8750 - sensitivity: 0.9237 - specificity: 0.7640 - gmeasure: 0.8364 - auc: 0.9432 - val_loss: 0.3454 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.8895 - val_specificity: 0.7543 - val_gmeasure: 0.8189 - val_auc: 0.9118
Epoch 86/100
600/600 [==============================] - 0s 190us/step - loss: 0.2909 - binary_accuracy: 0.8817 - sensitivity: 0.9216 - specificity: 0.7906 - gmeasure: 0.8508 - auc: 0.9437 - val_loss: 0.3441 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8980 - val_specificity: 0.6835 - val_gmeasure: 0.7833 - val_auc: 0.9123
Epoch 87/100
600/600 [==============================] - 0s 208us/step - loss: 0.2881 - binary_accuracy: 0.8750 - sensitivity: 0.9159 - specificity: 0.7817 - gmeasure: 0.8426 - auc: 0.9396 - val_loss: 0.3818 - val_binary_accuracy: 0.7933 - val_sensitivity: 0.9459 - val_specificity: 0.4379 - val_gmeasure: 0.6416 - val_auc: 0.9138
Epoch 88/100
600/600 [==============================] - 0s 207us/step - loss: 0.3188 - binary_accuracy: 0.8367 - sensitivity: 0.9039 - specificity: 0.7118 - gmeasure: 0.7916 - auc: 0.9502 - val_loss: 0.3550 - val_binary_accuracy: 0.8000 - val_sensitivity: 0.9262 - val_specificity: 0.5087 - val_gmeasure: 0.6852 - val_auc: 0.9138
Epoch 89/100
600/600 [==============================] - 0s 201us/step - loss: 0.2883 - binary_accuracy: 0.8800 - sensitivity: 0.9090 - specificity: 0.8045 - gmeasure: 0.8514 - auc: 0.9437 - val_loss: 0.3414 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.9066 - val_specificity: 0.5896 - val_gmeasure: 0.7279 - val_auc: 0.9149
Epoch 90/100
600/600 [==============================] - 0s 231us/step - loss: 0.2894 - binary_accuracy: 0.8717 - sensitivity: 0.9246 - specificity: 0.7667 - gmeasure: 0.8389 - auc: 0.9475 - val_loss: 0.3408 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.9177 - val_specificity: 0.5896 - val_gmeasure: 0.7324 - val_auc: 0.9163
Epoch 91/100
600/600 [==============================] - 0s 203us/step - loss: 0.3045 - binary_accuracy: 0.8683 - sensitivity: 0.9183 - specificity: 0.7585 - gmeasure: 0.8249 - auc: 0.9488 - val_loss: 0.3685 - val_binary_accuracy: 0.8000 - val_sensitivity: 0.9459 - val_specificity: 0.4682 - val_gmeasure: 0.6647 - val_auc: 0.9156
Epoch 92/100
600/600 [==============================] - 0s 217us/step - loss: 0.2883 - binary_accuracy: 0.8733 - sensitivity: 0.9116 - specificity: 0.7920 - gmeasure: 0.8452 - auc: 0.9451 - val_loss: 0.3391 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.9177 - val_specificity: 0.6437 - val_gmeasure: 0.7656 - val_auc: 0.9173
Epoch 93/100
600/600 [==============================] - 0s 200us/step - loss: 0.2859 - binary_accuracy: 0.8767 - sensitivity: 0.9191 - specificity: 0.7788 - gmeasure: 0.8421 - auc: 0.9486 - val_loss: 0.3457 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.9262 - val_specificity: 0.5420 - val_gmeasure: 0.7073 - val_auc: 0.9163
Epoch 94/100
600/600 [==============================] - 0s 200us/step - loss: 0.3216 - binary_accuracy: 0.8517 - sensitivity: 0.9146 - specificity: 0.7188 - gmeasure: 0.7958 - auc: 0.9505 - val_loss: 0.3349 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8698 - val_specificity: 0.8656 - val_gmeasure: 0.8675 - val_auc: 0.9184
Epoch 95/100
600/600 [==============================] - 0s 202us/step - loss: 0.3055 - binary_accuracy: 0.8733 - sensitivity: 0.9111 - specificity: 0.8139 - gmeasure: 0.8546 - auc: 0.9492 - val_loss: 0.3435 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8520 - val_specificity: 0.9197 - val_gmeasure: 0.8842 - val_auc: 0.9190
Epoch 96/100
600/600 [==============================] - 0s 194us/step - loss: 0.2751 - binary_accuracy: 0.8783 - sensitivity: 0.9206 - specificity: 0.7799 - gmeasure: 0.8436 - auc: 0.9456 - val_loss: 0.3332 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8613 - val_specificity: 0.8656 - val_gmeasure: 0.8632 - val_auc: 0.9214
Epoch 97/100
600/600 [==============================] - 0s 191us/step - loss: 0.2769 - binary_accuracy: 0.8683 - sensitivity: 0.9184 - specificity: 0.7673 - gmeasure: 0.8360 - auc: 0.9560 - val_loss: 0.3525 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8434 - val_specificity: 0.9364 - val_gmeasure: 0.8881 - val_auc: 0.9199
Epoch 98/100
600/600 [==============================] - 0s 201us/step - loss: 0.2814 - binary_accuracy: 0.8717 - sensitivity: 0.9236 - specificity: 0.7561 - gmeasure: 0.8274 - auc: 0.9562 - val_loss: 0.3576 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8342 - val_specificity: 0.9667 - val_gmeasure: 0.8966 - val_auc: 0.9188
Epoch 99/100
600/600 [==============================] - 0s 210us/step - loss: 0.2682 - binary_accuracy: 0.8783 - sensitivity: 0.9277 - specificity: 0.7606 - gmeasure: 0.8372 - auc: 0.9497 - val_loss: 0.3329 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8613 - val_specificity: 0.8656 - val_gmeasure: 0.8632 - val_auc: 0.9214
Epoch 100/100
600/600 [==============================] - 0s 207us/step - loss: 0.2683 - binary_accuracy: 0.8850 - sensitivity: 0.9215 - specificity: 0.8035 - gmeasure: 0.8568 - auc: 0.9540 - val_loss: 0.3433 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8613 - val_specificity: 0.9061 - val_gmeasure: 0.8831 - val_auc: 0.9216
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:151] Training end with time 15.206775903701782!
[root    |INFO|build_network.py:79] Saved trained model weight at ./example_result/weight_0.h5
[root    |DEBUG|deepbiome.py:166] Save weight at ./example_result/weight_0.h5
[root    |DEBUG|deepbiome.py:169] Save history at ./example_result/hist_0.json
[root    |INFO|build_network.py:177] Evaluation start!
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
750/750 [==============================] - 0s 6us/step
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:182] Evaluation end with time 0.012238025665283203!
[root    |INFO|build_network.py:183] Evaluation: [0.28811851143836975, 0.8826666474342346, 0.8803088665008545, 0.8879310488700867, 0.884111762046814, 0.9488500356674194]
[root    |INFO|build_network.py:177] Evaluation start!
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
250/250 [==============================] - 0s 22us/step
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:182] Evaluation end with time 0.013002872467041016!
[root    |INFO|build_network.py:183] Evaluation: [0.32086846232414246, 0.8479999899864197, 0.8639053106307983, 0.8148148059844971, 0.8390011191368103, 0.928482711315155]
[root    |INFO|deepbiome.py:179] Compute time : 18.531410217285156
[root    |INFO|deepbiome.py:180] 1 fold computing end!---------------------------------------------
[root    |INFO|deepbiome.py:137] -------2 simulation start!----------------------------------
[root    |INFO|readers.py:58] -----------------------------------------------------------------------
[root    |INFO|readers.py:59] Construct Dataset
[root    |INFO|readers.py:60] -----------------------------------------------------------------------
[root    |INFO|readers.py:61] Load data
[root    |INFO|deepbiome.py:147] -----------------------------------------------------------------
[root    |INFO|deepbiome.py:148] Build network for 2 simulation
[root    |INFO|build_network.py:513] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:514] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv
[root    |INFO|build_network.py:518] Phylogenetic tree level list: [&#39;Genus&#39;, &#39;Family&#39;, &#39;Order&#39;, &#39;Class&#39;, &#39;Phylum&#39;]
[root    |INFO|build_network.py:519] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:526]      Genus: 48
[root    |INFO|build_network.py:526]     Family: 40
[root    |INFO|build_network.py:526]      Order: 23
[root    |INFO|build_network.py:526]      Class: 17
[root    |INFO|build_network.py:526]     Phylum: 9
[root    |INFO|build_network.py:529] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:530] Phylogenetic_tree_dict info: [&#39;Genus&#39;, &#39;Number&#39;, &#39;Order&#39;, &#39;Class&#39;, &#39;Phylum&#39;, &#39;Family&#39;]
[root    |INFO|build_network.py:531] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:541] Build edge weights between [ Genus, Family]
[root    |INFO|build_network.py:541] Build edge weights between [Family,  Order]
[root    |INFO|build_network.py:541] Build edge weights between [ Order,  Class]
[root    |INFO|build_network.py:541] Build edge weights between [ Class, Phylum]
[root    |INFO|build_network.py:554] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:570] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:571] Build network based on phylogenetic tree information
[root    |INFO|build_network.py:572] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:648] ------------------------------------------------------------------------------------------
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Model: &#34;model_1&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input (InputLayer)           (None, 48)                0
_________________________________________________________________
l1_dense (Dense_with_tree)   (None, 40)                1960
_________________________________________________________________
l1_activation (Activation)   (None, 40)                0
_________________________________________________________________
l2_dense (Dense_with_tree)   (None, 23)                943
_________________________________________________________________
l2_activation (Activation)   (None, 23)                0
_________________________________________________________________
l3_dense (Dense_with_tree)   (None, 17)                408
_________________________________________________________________
l3_activation (Activation)   (None, 17)                0
_________________________________________________________________
l4_dense (Dense_with_tree)   (None, 9)                 162
_________________________________________________________________
l4_activation (Activation)   (None, 9)                 0
_________________________________________________________________
last_dense_h (Dense)         (None, 1)                 10
_________________________________________________________________
p_hat (Activation)           (None, 1)                 0
=================================================================
Total params: 3,483
Trainable params: 3,483
Non-trainable params: 0
_________________________________________________________________
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:57] Build Network
[root    |INFO|build_network.py:58] Optimizer = adam
[root    |INFO|build_network.py:59] Loss = binary_crossentropy
[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc
[root    |INFO|deepbiome.py:157] -----------------------------------------------------------------
[root    |INFO|deepbiome.py:158] 2 fold computing start!----------------------------------
[root    |INFO|build_network.py:141] Training start!
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Train on 600 samples, validate on 150 samples
Epoch 1/100
600/600 [==============================] - 1s 1ms/step - loss: 0.6748 - binary_accuracy: 0.7150 - sensitivity: 0.9401 - specificity: 0.0694 - gmeasure: 0.0403 - auc: 0.4768 - val_loss: 0.6556 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5487
Epoch 2/100
600/600 [==============================] - 0s 217us/step - loss: 0.6400 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.4948 - val_loss: 0.6234 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5511
Epoch 3/100
600/600 [==============================] - 0s 213us/step - loss: 0.6096 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.4915 - val_loss: 0.6018 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5747
Epoch 4/100
600/600 [==============================] - 0s 214us/step - loss: 0.5932 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.4703 - val_loss: 0.5900 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5622
Epoch 5/100
600/600 [==============================] - 0s 211us/step - loss: 0.5862 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.4978 - val_loss: 0.5867 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5617
Epoch 6/100
600/600 [==============================] - 0s 228us/step - loss: 0.5852 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5087 - val_loss: 0.5867 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5888
Epoch 7/100
600/600 [==============================] - 0s 226us/step - loss: 0.5852 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5378 - val_loss: 0.5867 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6049
Epoch 8/100
600/600 [==============================] - 0s 196us/step - loss: 0.5855 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5658 - val_loss: 0.5867 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6290
Epoch 9/100
600/600 [==============================] - 0s 196us/step - loss: 0.5851 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5707 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6489
Epoch 10/100
600/600 [==============================] - 0s 231us/step - loss: 0.5850 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5912 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6555
Epoch 11/100
600/600 [==============================] - 0s 205us/step - loss: 0.5851 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6007 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6609
Epoch 12/100
600/600 [==============================] - 0s 206us/step - loss: 0.5850 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6039 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6673
Epoch 13/100
600/600 [==============================] - 0s 216us/step - loss: 0.5851 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6073 - val_loss: 0.5866 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6761
Epoch 14/100
600/600 [==============================] - 0s 204us/step - loss: 0.5850 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6273 - val_loss: 0.5866 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6799
Epoch 15/100
600/600 [==============================] - 0s 216us/step - loss: 0.5854 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6281 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6887
Epoch 16/100
600/600 [==============================] - 0s 226us/step - loss: 0.5853 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6401 - val_loss: 0.5866 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6883
Epoch 17/100
600/600 [==============================] - 0s 206us/step - loss: 0.5850 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6386 - val_loss: 0.5866 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6957
Epoch 18/100
600/600 [==============================] - 0s 201us/step - loss: 0.5850 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6617 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7064
Epoch 19/100
600/600 [==============================] - 0s 210us/step - loss: 0.5850 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6467 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7129
Epoch 20/100
600/600 [==============================] - 0s 203us/step - loss: 0.5850 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6634 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7193
Epoch 21/100
600/600 [==============================] - 0s 194us/step - loss: 0.5849 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6677 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7251
Epoch 22/100
600/600 [==============================] - 0s 198us/step - loss: 0.5854 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6765 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7390
Epoch 23/100
600/600 [==============================] - 0s 213us/step - loss: 0.5849 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6904 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7384
Epoch 24/100
600/600 [==============================] - 0s 203us/step - loss: 0.5852 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6941 - val_loss: 0.5866 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7389
Epoch 25/100
600/600 [==============================] - 0s 215us/step - loss: 0.5857 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6852 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7438
Epoch 26/100
600/600 [==============================] - 0s 212us/step - loss: 0.5858 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6832 - val_loss: 0.5866 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7445
Epoch 27/100
600/600 [==============================] - 0s 185us/step - loss: 0.5850 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6890 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7536
Epoch 28/100
600/600 [==============================] - 0s 203us/step - loss: 0.5851 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6966 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7555
Epoch 29/100
600/600 [==============================] - 0s 206us/step - loss: 0.5849 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7159 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7603
Epoch 30/100
600/600 [==============================] - 0s 230us/step - loss: 0.5849 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7035 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7627
Epoch 31/100
600/600 [==============================] - 0s 223us/step - loss: 0.5851 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7222 - val_loss: 0.5865 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7658
Epoch 32/100
600/600 [==============================] - 0s 207us/step - loss: 0.5850 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7149 - val_loss: 0.5864 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7742
Epoch 33/100
600/600 [==============================] - 0s 201us/step - loss: 0.5851 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7015 - val_loss: 0.5864 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7798
Epoch 34/100
600/600 [==============================] - 0s 220us/step - loss: 0.5851 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7173 - val_loss: 0.5864 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7816
Epoch 35/100
600/600 [==============================] - 0s 213us/step - loss: 0.5852 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7254 - val_loss: 0.5864 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7869
Epoch 36/100
600/600 [==============================] - 0s 210us/step - loss: 0.5849 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7346 - val_loss: 0.5864 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7927
Epoch 37/100
600/600 [==============================] - 0s 207us/step - loss: 0.5848 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7466 - val_loss: 0.5848 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8130
Epoch 38/100
600/600 [==============================] - 0s 205us/step - loss: 0.5820 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8258 - val_loss: 0.5798 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8508
Epoch 39/100
600/600 [==============================] - 0s 196us/step - loss: 0.5741 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8366 - val_loss: 0.5715 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8628
Epoch 40/100
600/600 [==============================] - 0s 189us/step - loss: 0.5634 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8389 - val_loss: 0.5540 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8606
Epoch 41/100
600/600 [==============================] - 0s 207us/step - loss: 0.5503 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8414 - val_loss: 0.5367 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8596
Epoch 42/100
600/600 [==============================] - 0s 210us/step - loss: 0.5342 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8417 - val_loss: 0.5184 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8583
Epoch 43/100
600/600 [==============================] - 0s 200us/step - loss: 0.5169 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8411 - val_loss: 0.5000 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8564
Epoch 44/100
600/600 [==============================] - 0s 210us/step - loss: 0.4984 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8439 - val_loss: 0.4819 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8570
Epoch 45/100
600/600 [==============================] - 0s 202us/step - loss: 0.4807 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8465 - val_loss: 0.4675 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8597
Epoch 46/100
600/600 [==============================] - 0s 197us/step - loss: 0.4681 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8479 - val_loss: 0.4569 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8605
Epoch 47/100
600/600 [==============================] - 0s 208us/step - loss: 0.4504 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8466 - val_loss: 0.4479 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8617
Epoch 48/100
600/600 [==============================] - 0s 197us/step - loss: 0.4371 - binary_accuracy: 0.7283 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8543 - val_loss: 0.4506 - val_binary_accuracy: 0.7267 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8639
Epoch 49/100
600/600 [==============================] - 0s 216us/step - loss: 0.4322 - binary_accuracy: 0.7333 - sensitivity: 0.9599 - specificity: 0.1712 - gmeasure: 0.1893 - auc: 0.8509 - val_loss: 0.4266 - val_binary_accuracy: 0.7800 - val_sensitivity: 0.8649 - val_specificity: 0.5689 - val_gmeasure: 0.6944 - val_auc: 0.8544
Epoch 50/100
600/600 [==============================] - 0s 232us/step - loss: 0.4191 - binary_accuracy: 0.7783 - sensitivity: 0.8645 - specificity: 0.5634 - gmeasure: 0.6816 - auc: 0.8608 - val_loss: 0.4174 - val_binary_accuracy: 0.7867 - val_sensitivity: 0.8461 - val_specificity: 0.6501 - val_gmeasure: 0.7324 - val_auc: 0.8616
Epoch 51/100
600/600 [==============================] - 0s 200us/step - loss: 0.4152 - binary_accuracy: 0.8067 - sensitivity: 0.8519 - specificity: 0.6743 - gmeasure: 0.7529 - auc: 0.8576 - val_loss: 0.4426 - val_binary_accuracy: 0.7667 - val_sensitivity: 0.8649 - val_specificity: 0.5224 - val_gmeasure: 0.6648 - val_auc: 0.8586
Epoch 52/100
600/600 [==============================] - 0s 191us/step - loss: 0.4046 - binary_accuracy: 0.7950 - sensitivity: 0.8547 - specificity: 0.6314 - gmeasure: 0.7316 - auc: 0.8628 - val_loss: 0.4088 - val_binary_accuracy: 0.7933 - val_sensitivity: 0.8461 - val_specificity: 0.6709 - val_gmeasure: 0.7470 - val_auc: 0.8703
Epoch 53/100
600/600 [==============================] - 0s 200us/step - loss: 0.4143 - binary_accuracy: 0.8000 - sensitivity: 0.8517 - specificity: 0.6926 - gmeasure: 0.7631 - auc: 0.8695 - val_loss: 0.4146 - val_binary_accuracy: 0.7800 - val_sensitivity: 0.8461 - val_specificity: 0.6223 - val_gmeasure: 0.7166 - val_auc: 0.8602
Epoch 54/100
600/600 [==============================] - 0s 209us/step - loss: 0.4065 - binary_accuracy: 0.8133 - sensitivity: 0.8479 - specificity: 0.7194 - gmeasure: 0.7788 - auc: 0.8653 - val_loss: 0.4290 - val_binary_accuracy: 0.7667 - val_sensitivity: 0.8461 - val_specificity: 0.5689 - val_gmeasure: 0.6867 - val_auc: 0.8696
Epoch 55/100
600/600 [==============================] - 0s 216us/step - loss: 0.3975 - binary_accuracy: 0.8050 - sensitivity: 0.8508 - specificity: 0.6738 - gmeasure: 0.7527 - auc: 0.8580 - val_loss: 0.4000 - val_binary_accuracy: 0.7933 - val_sensitivity: 0.8461 - val_specificity: 0.6709 - val_gmeasure: 0.7470 - val_auc: 0.8630
Epoch 56/100
600/600 [==============================] - 0s 204us/step - loss: 0.3961 - binary_accuracy: 0.8133 - sensitivity: 0.8534 - specificity: 0.6956 - gmeasure: 0.7669 - auc: 0.8600 - val_loss: 0.4039 - val_binary_accuracy: 0.7867 - val_sensitivity: 0.8461 - val_specificity: 0.6501 - val_gmeasure: 0.7324 - val_auc: 0.8629
Epoch 57/100
600/600 [==============================] - 0s 195us/step - loss: 0.3969 - binary_accuracy: 0.8100 - sensitivity: 0.8378 - specificity: 0.7136 - gmeasure: 0.7681 - auc: 0.8647 - val_loss: 0.4011 - val_binary_accuracy: 0.7933 - val_sensitivity: 0.8461 - val_specificity: 0.6709 - val_gmeasure: 0.7470 - val_auc: 0.8708
Epoch 58/100
600/600 [==============================] - 0s 197us/step - loss: 0.3895 - binary_accuracy: 0.8133 - sensitivity: 0.8491 - specificity: 0.7210 - gmeasure: 0.7800 - auc: 0.8701 - val_loss: 0.3973 - val_binary_accuracy: 0.7933 - val_sensitivity: 0.8461 - val_specificity: 0.6709 - val_gmeasure: 0.7470 - val_auc: 0.8739
Epoch 59/100
600/600 [==============================] - 0s 179us/step - loss: 0.3858 - binary_accuracy: 0.8233 - sensitivity: 0.8464 - specificity: 0.7657 - gmeasure: 0.8027 - auc: 0.8701 - val_loss: 0.4002 - val_binary_accuracy: 0.7933 - val_sensitivity: 0.8461 - val_specificity: 0.6709 - val_gmeasure: 0.7470 - val_auc: 0.8717
Epoch 60/100
600/600 [==============================] - 0s 189us/step - loss: 0.3844 - binary_accuracy: 0.8233 - sensitivity: 0.8426 - specificity: 0.7720 - gmeasure: 0.8027 - auc: 0.8715 - val_loss: 0.4042 - val_binary_accuracy: 0.7867 - val_sensitivity: 0.8461 - val_specificity: 0.6501 - val_gmeasure: 0.7324 - val_auc: 0.8711
Epoch 61/100
600/600 [==============================] - 0s 174us/step - loss: 0.3860 - binary_accuracy: 0.8150 - sensitivity: 0.8466 - specificity: 0.7363 - gmeasure: 0.7857 - auc: 0.8746 - val_loss: 0.3853 - val_binary_accuracy: 0.8067 - val_sensitivity: 0.8461 - val_specificity: 0.7174 - val_gmeasure: 0.7729 - val_auc: 0.8752
Epoch 62/100
600/600 [==============================] - 0s 197us/step - loss: 0.3899 - binary_accuracy: 0.8067 - sensitivity: 0.8407 - specificity: 0.7186 - gmeasure: 0.7705 - auc: 0.8730 - val_loss: 0.3876 - val_binary_accuracy: 0.8000 - val_sensitivity: 0.8185 - val_specificity: 0.7660 - val_gmeasure: 0.7876 - val_auc: 0.8785
Epoch 63/100
600/600 [==============================] - 0s 201us/step - loss: 0.3965 - binary_accuracy: 0.8150 - sensitivity: 0.8318 - specificity: 0.7590 - gmeasure: 0.7881 - auc: 0.8742 - val_loss: 0.4021 - val_binary_accuracy: 0.7933 - val_sensitivity: 0.8461 - val_specificity: 0.6709 - val_gmeasure: 0.7470 - val_auc: 0.8678
Epoch 64/100
600/600 [==============================] - 0s 217us/step - loss: 0.3937 - binary_accuracy: 0.8050 - sensitivity: 0.8169 - specificity: 0.7757 - gmeasure: 0.7910 - auc: 0.8712 - val_loss: 0.4175 - val_binary_accuracy: 0.7800 - val_sensitivity: 0.8461 - val_specificity: 0.6223 - val_gmeasure: 0.7166 - val_auc: 0.8677
Epoch 65/100
600/600 [==============================] - 0s 188us/step - loss: 0.3793 - binary_accuracy: 0.8183 - sensitivity: 0.8509 - specificity: 0.7099 - gmeasure: 0.7693 - auc: 0.8701 - val_loss: 0.3803 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8461 - val_specificity: 0.7452 - val_gmeasure: 0.7878 - val_auc: 0.8767
Epoch 66/100
600/600 [==============================] - 0s 195us/step - loss: 0.3736 - binary_accuracy: 0.8200 - sensitivity: 0.8423 - specificity: 0.7697 - gmeasure: 0.8023 - auc: 0.8802 - val_loss: 0.3812 - val_binary_accuracy: 0.8067 - val_sensitivity: 0.8461 - val_specificity: 0.7174 - val_gmeasure: 0.7729 - val_auc: 0.8771
Epoch 67/100
600/600 [==============================] - 0s 210us/step - loss: 0.3702 - binary_accuracy: 0.8200 - sensitivity: 0.8422 - specificity: 0.7605 - gmeasure: 0.7981 - auc: 0.8773 - val_loss: 0.3905 - val_binary_accuracy: 0.8000 - val_sensitivity: 0.8461 - val_specificity: 0.6918 - val_gmeasure: 0.7606 - val_auc: 0.8730
Epoch 68/100
600/600 [==============================] - 0s 202us/step - loss: 0.3686 - binary_accuracy: 0.8233 - sensitivity: 0.8368 - specificity: 0.7784 - gmeasure: 0.8039 - auc: 0.8766 - val_loss: 0.3804 - val_binary_accuracy: 0.8067 - val_sensitivity: 0.8461 - val_specificity: 0.7174 - val_gmeasure: 0.7729 - val_auc: 0.8764
Epoch 69/100
600/600 [==============================] - 0s 207us/step - loss: 0.3668 - binary_accuracy: 0.8283 - sensitivity: 0.8464 - specificity: 0.7856 - gmeasure: 0.8130 - auc: 0.8805 - val_loss: 0.3789 - val_binary_accuracy: 0.8067 - val_sensitivity: 0.8461 - val_specificity: 0.7174 - val_gmeasure: 0.7729 - val_auc: 0.8771
Epoch 70/100
600/600 [==============================] - 0s 210us/step - loss: 0.3639 - binary_accuracy: 0.8267 - sensitivity: 0.8435 - specificity: 0.7863 - gmeasure: 0.8135 - auc: 0.8844 - val_loss: 0.3832 - val_binary_accuracy: 0.8067 - val_sensitivity: 0.8461 - val_specificity: 0.7174 - val_gmeasure: 0.7729 - val_auc: 0.8727
Epoch 71/100
600/600 [==============================] - 0s 203us/step - loss: 0.3628 - binary_accuracy: 0.8283 - sensitivity: 0.8401 - specificity: 0.8057 - gmeasure: 0.8211 - auc: 0.8854 - val_loss: 0.3837 - val_binary_accuracy: 0.8067 - val_sensitivity: 0.8461 - val_specificity: 0.7174 - val_gmeasure: 0.7729 - val_auc: 0.8752
Epoch 72/100
600/600 [==============================] - 0s 191us/step - loss: 0.3615 - binary_accuracy: 0.8217 - sensitivity: 0.8478 - specificity: 0.7529 - gmeasure: 0.7955 - auc: 0.8836 - val_loss: 0.3692 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8371 - val_specificity: 0.7660 - val_gmeasure: 0.7959 - val_auc: 0.8799
Epoch 73/100
600/600 [==============================] - 0s 193us/step - loss: 0.3618 - binary_accuracy: 0.8283 - sensitivity: 0.8454 - specificity: 0.7907 - gmeasure: 0.8127 - auc: 0.8901 - val_loss: 0.3802 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8461 - val_specificity: 0.7452 - val_gmeasure: 0.7878 - val_auc: 0.8741
Epoch 74/100
600/600 [==============================] - 0s 202us/step - loss: 0.3519 - binary_accuracy: 0.8333 - sensitivity: 0.8468 - specificity: 0.8078 - gmeasure: 0.8249 - auc: 0.8912 - val_loss: 0.3672 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8371 - val_specificity: 0.7660 - val_gmeasure: 0.7959 - val_auc: 0.8849
Epoch 75/100
600/600 [==============================] - 0s 212us/step - loss: 0.3545 - binary_accuracy: 0.8250 - sensitivity: 0.8373 - specificity: 0.8000 - gmeasure: 0.8170 - auc: 0.8907 - val_loss: 0.3812 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8461 - val_specificity: 0.7452 - val_gmeasure: 0.7878 - val_auc: 0.8725
Epoch 76/100
600/600 [==============================] - 0s 211us/step - loss: 0.3530 - binary_accuracy: 0.8333 - sensitivity: 0.8454 - specificity: 0.8078 - gmeasure: 0.8232 - auc: 0.8907 - val_loss: 0.3658 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8371 - val_specificity: 0.7660 - val_gmeasure: 0.7959 - val_auc: 0.8811
Epoch 77/100
600/600 [==============================] - 0s 185us/step - loss: 0.3518 - binary_accuracy: 0.8233 - sensitivity: 0.8322 - specificity: 0.7906 - gmeasure: 0.8093 - auc: 0.8917 - val_loss: 0.3951 - val_binary_accuracy: 0.8000 - val_sensitivity: 0.8461 - val_specificity: 0.6918 - val_gmeasure: 0.7606 - val_auc: 0.8743
Epoch 78/100
600/600 [==============================] - 0s 186us/step - loss: 0.3619 - binary_accuracy: 0.8200 - sensitivity: 0.8307 - specificity: 0.7989 - gmeasure: 0.8104 - auc: 0.8843 - val_loss: 0.3909 - val_binary_accuracy: 0.8000 - val_sensitivity: 0.8461 - val_specificity: 0.6918 - val_gmeasure: 0.7606 - val_auc: 0.8737
Epoch 79/100
600/600 [==============================] - 0s 195us/step - loss: 0.3469 - binary_accuracy: 0.8283 - sensitivity: 0.8476 - specificity: 0.7962 - gmeasure: 0.8185 - auc: 0.8892 - val_loss: 0.3623 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8371 - val_specificity: 0.7660 - val_gmeasure: 0.7959 - val_auc: 0.8833
Epoch 80/100
600/600 [==============================] - 0s 184us/step - loss: 0.3465 - binary_accuracy: 0.8283 - sensitivity: 0.8391 - specificity: 0.8099 - gmeasure: 0.8210 - auc: 0.8953 - val_loss: 0.3758 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8461 - val_specificity: 0.7660 - val_gmeasure: 0.8007 - val_auc: 0.8791
Epoch 81/100
600/600 [==============================] - 0s 202us/step - loss: 0.3413 - binary_accuracy: 0.8317 - sensitivity: 0.8322 - specificity: 0.8160 - gmeasure: 0.8218 - auc: 0.8892 - val_loss: 0.3740 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8461 - val_specificity: 0.7660 - val_gmeasure: 0.8007 - val_auc: 0.8752
Epoch 82/100
600/600 [==============================] - 0s 189us/step - loss: 0.3439 - binary_accuracy: 0.8283 - sensitivity: 0.8415 - specificity: 0.7888 - gmeasure: 0.8125 - auc: 0.8883 - val_loss: 0.3615 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8371 - val_specificity: 0.7660 - val_gmeasure: 0.7959 - val_auc: 0.8826
Epoch 83/100
600/600 [==============================] - 0s 208us/step - loss: 0.3363 - binary_accuracy: 0.8350 - sensitivity: 0.8455 - specificity: 0.8044 - gmeasure: 0.8232 - auc: 0.8963 - val_loss: 0.3628 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8461 - val_specificity: 0.7660 - val_gmeasure: 0.8007 - val_auc: 0.8819
Epoch 84/100
600/600 [==============================] - 0s 200us/step - loss: 0.3345 - binary_accuracy: 0.8367 - sensitivity: 0.8411 - specificity: 0.8245 - gmeasure: 0.8301 - auc: 0.8939 - val_loss: 0.3715 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8461 - val_specificity: 0.7660 - val_gmeasure: 0.8007 - val_auc: 0.8817
Epoch 85/100
600/600 [==============================] - 0s 203us/step - loss: 0.3347 - binary_accuracy: 0.8417 - sensitivity: 0.8414 - specificity: 0.8414 - gmeasure: 0.8400 - auc: 0.8920 - val_loss: 0.3569 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8371 - val_specificity: 0.7869 - val_gmeasure: 0.8082 - val_auc: 0.8819
Epoch 86/100
600/600 [==============================] - 0s 211us/step - loss: 0.3308 - binary_accuracy: 0.8400 - sensitivity: 0.8476 - specificity: 0.8215 - gmeasure: 0.8327 - auc: 0.8895 - val_loss: 0.3605 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8371 - val_specificity: 0.7660 - val_gmeasure: 0.7959 - val_auc: 0.8825
Epoch 87/100
600/600 [==============================] - 0s 215us/step - loss: 0.3293 - binary_accuracy: 0.8417 - sensitivity: 0.8485 - specificity: 0.8259 - gmeasure: 0.8351 - auc: 0.8975 - val_loss: 0.3623 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8461 - val_specificity: 0.7660 - val_gmeasure: 0.8007 - val_auc: 0.8819
Epoch 88/100
600/600 [==============================] - 0s 210us/step - loss: 0.3256 - binary_accuracy: 0.8417 - sensitivity: 0.8446 - specificity: 0.8370 - gmeasure: 0.8395 - auc: 0.8965 - val_loss: 0.3607 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8461 - val_specificity: 0.7660 - val_gmeasure: 0.8007 - val_auc: 0.8826
Epoch 89/100
600/600 [==============================] - 0s 213us/step - loss: 0.3285 - binary_accuracy: 0.8400 - sensitivity: 0.8491 - specificity: 0.8067 - gmeasure: 0.8241 - auc: 0.8969 - val_loss: 0.3535 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8283 - val_specificity: 0.7869 - val_gmeasure: 0.8039 - val_auc: 0.8835
Epoch 90/100
600/600 [==============================] - 0s 204us/step - loss: 0.3326 - binary_accuracy: 0.8400 - sensitivity: 0.8437 - specificity: 0.8403 - gmeasure: 0.8404 - auc: 0.8932 - val_loss: 0.3546 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8371 - val_specificity: 0.7869 - val_gmeasure: 0.8082 - val_auc: 0.8876
Epoch 91/100
600/600 [==============================] - 0s 201us/step - loss: 0.3214 - binary_accuracy: 0.8483 - sensitivity: 0.8424 - specificity: 0.8690 - gmeasure: 0.8546 - auc: 0.9004 - val_loss: 0.3707 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8461 - val_specificity: 0.7660 - val_gmeasure: 0.8007 - val_auc: 0.8904
Epoch 92/100
600/600 [==============================] - 0s 202us/step - loss: 0.3197 - binary_accuracy: 0.8417 - sensitivity: 0.8410 - specificity: 0.8423 - gmeasure: 0.8404 - auc: 0.8940 - val_loss: 0.3552 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8371 - val_specificity: 0.7869 - val_gmeasure: 0.8082 - val_auc: 0.8839
Epoch 93/100
600/600 [==============================] - 0s 214us/step - loss: 0.3221 - binary_accuracy: 0.8433 - sensitivity: 0.8415 - specificity: 0.8437 - gmeasure: 0.8392 - auc: 0.9042 - val_loss: 0.3694 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8461 - val_specificity: 0.7660 - val_gmeasure: 0.8007 - val_auc: 0.8817
Epoch 94/100
600/600 [==============================] - 0s 207us/step - loss: 0.3180 - binary_accuracy: 0.8450 - sensitivity: 0.8405 - specificity: 0.8654 - gmeasure: 0.8504 - auc: 0.9027 - val_loss: 0.3601 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8371 - val_specificity: 0.7869 - val_gmeasure: 0.8082 - val_auc: 0.8817
Epoch 95/100
600/600 [==============================] - 0s 203us/step - loss: 0.3114 - binary_accuracy: 0.8500 - sensitivity: 0.8490 - specificity: 0.8480 - gmeasure: 0.8472 - auc: 0.9000 - val_loss: 0.3493 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8371 - val_specificity: 0.7869 - val_gmeasure: 0.8082 - val_auc: 0.8857
Epoch 96/100
600/600 [==============================] - 0s 190us/step - loss: 0.3118 - binary_accuracy: 0.8417 - sensitivity: 0.8415 - specificity: 0.8427 - gmeasure: 0.8379 - auc: 0.9075 - val_loss: 0.3853 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8461 - val_specificity: 0.7382 - val_gmeasure: 0.7858 - val_auc: 0.8862
Epoch 97/100
600/600 [==============================] - 0s 202us/step - loss: 0.3190 - binary_accuracy: 0.8417 - sensitivity: 0.8378 - specificity: 0.8604 - gmeasure: 0.8461 - auc: 0.9055 - val_loss: 0.3702 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8461 - val_specificity: 0.7660 - val_gmeasure: 0.8007 - val_auc: 0.8855
Epoch 98/100
600/600 [==============================] - 0s 212us/step - loss: 0.3108 - binary_accuracy: 0.8483 - sensitivity: 0.8422 - specificity: 0.8636 - gmeasure: 0.8510 - auc: 0.9016 - val_loss: 0.3504 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8371 - val_specificity: 0.7869 - val_gmeasure: 0.8082 - val_auc: 0.8848
Epoch 99/100
600/600 [==============================] - 0s 206us/step - loss: 0.3078 - binary_accuracy: 0.8483 - sensitivity: 0.8474 - specificity: 0.8468 - gmeasure: 0.8457 - auc: 0.9029 - val_loss: 0.3527 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8371 - val_specificity: 0.7869 - val_gmeasure: 0.8082 - val_auc: 0.8839
Epoch 100/100
600/600 [==============================] - 0s 204us/step - loss: 0.3054 - binary_accuracy: 0.8500 - sensitivity: 0.8483 - specificity: 0.8591 - gmeasure: 0.8523 - auc: 0.9031 - val_loss: 0.3503 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8371 - val_specificity: 0.7869 - val_gmeasure: 0.8082 - val_auc: 0.8848
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:151] Training end with time 14.606053829193115!
[root    |INFO|build_network.py:79] Saved trained model weight at ./example_result/weight_1.h5
[root    |DEBUG|deepbiome.py:166] Save weight at ./example_result/weight_1.h5
[root    |DEBUG|deepbiome.py:169] Save history at ./example_result/hist_1.json
[root    |INFO|build_network.py:177] Evaluation start!
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
750/750 [==============================] - 0s 8us/step
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:182] Evaluation end with time 0.013545036315917969!
[root    |INFO|build_network.py:183] Evaluation: [0.3132685422897339, 0.8493333458900452, 0.8424908518791199, 0.8676470518112183, 0.8549764156341553, 0.9006454944610596]
[root    |INFO|build_network.py:177] Evaluation start!
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
250/250 [==============================] - 0s 24us/step
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:182] Evaluation end with time 0.012819051742553711!
[root    |INFO|build_network.py:183] Evaluation: [0.37076306343078613, 0.8240000009536743, 0.8448275923728943, 0.7763158082962036, 0.8098475337028503, 0.8821082711219788]
[root    |INFO|deepbiome.py:179] Compute time : 16.150416612625122
[root    |INFO|deepbiome.py:180] 2 fold computing end!---------------------------------------------
[root    |INFO|deepbiome.py:137] -------3 simulation start!----------------------------------
[root    |INFO|readers.py:58] -----------------------------------------------------------------------
[root    |INFO|readers.py:59] Construct Dataset
[root    |INFO|readers.py:60] -----------------------------------------------------------------------
[root    |INFO|readers.py:61] Load data
[root    |INFO|deepbiome.py:147] -----------------------------------------------------------------
[root    |INFO|deepbiome.py:148] Build network for 3 simulation
[root    |INFO|build_network.py:513] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:514] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv
[root    |INFO|build_network.py:518] Phylogenetic tree level list: [&#39;Genus&#39;, &#39;Family&#39;, &#39;Order&#39;, &#39;Class&#39;, &#39;Phylum&#39;]
[root    |INFO|build_network.py:519] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:526]      Genus: 48
[root    |INFO|build_network.py:526]     Family: 40
[root    |INFO|build_network.py:526]      Order: 23
[root    |INFO|build_network.py:526]      Class: 17
[root    |INFO|build_network.py:526]     Phylum: 9
[root    |INFO|build_network.py:529] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:530] Phylogenetic_tree_dict info: [&#39;Genus&#39;, &#39;Number&#39;, &#39;Order&#39;, &#39;Class&#39;, &#39;Phylum&#39;, &#39;Family&#39;]
[root    |INFO|build_network.py:531] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:541] Build edge weights between [ Genus, Family]
[root    |INFO|build_network.py:541] Build edge weights between [Family,  Order]
[root    |INFO|build_network.py:541] Build edge weights between [ Order,  Class]
[root    |INFO|build_network.py:541] Build edge weights between [ Class, Phylum]
[root    |INFO|build_network.py:554] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:570] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:571] Build network based on phylogenetic tree information
[root    |INFO|build_network.py:572] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:648] ------------------------------------------------------------------------------------------
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Model: &#34;model_1&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input (InputLayer)           (None, 48)                0
_________________________________________________________________
l1_dense (Dense_with_tree)   (None, 40)                1960
_________________________________________________________________
l1_activation (Activation)   (None, 40)                0
_________________________________________________________________
l2_dense (Dense_with_tree)   (None, 23)                943
_________________________________________________________________
l2_activation (Activation)   (None, 23)                0
_________________________________________________________________
l3_dense (Dense_with_tree)   (None, 17)                408
_________________________________________________________________
l3_activation (Activation)   (None, 17)                0
_________________________________________________________________
l4_dense (Dense_with_tree)   (None, 9)                 162
_________________________________________________________________
l4_activation (Activation)   (None, 9)                 0
_________________________________________________________________
last_dense_h (Dense)         (None, 1)                 10
_________________________________________________________________
p_hat (Activation)           (None, 1)                 0
=================================================================
Total params: 3,483
Trainable params: 3,483
Non-trainable params: 0
_________________________________________________________________
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:57] Build Network
[root    |INFO|build_network.py:58] Optimizer = adam
[root    |INFO|build_network.py:59] Loss = binary_crossentropy
[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc
[root    |INFO|deepbiome.py:157] -----------------------------------------------------------------
[root    |INFO|deepbiome.py:158] 3 fold computing start!----------------------------------
[root    |INFO|build_network.py:141] Training start!
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Train on 600 samples, validate on 150 samples
Epoch 1/100
600/600 [==============================] - 1s 945us/step - loss: 0.6727 - binary_accuracy: 0.6700 - sensitivity: 0.9571 - specificity: 0.0294 - gmeasure: 0.0345 - auc: 0.5526 - val_loss: 0.6562 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5027
Epoch 2/100
600/600 [==============================] - 0s 212us/step - loss: 0.6327 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5571 - val_loss: 0.6471 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5108
Epoch 3/100
600/600 [==============================] - 0s 211us/step - loss: 0.6201 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.5831 - val_loss: 0.6511 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5466
Epoch 4/100
600/600 [==============================] - 0s 202us/step - loss: 0.6213 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6055 - val_loss: 0.6493 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.5886
Epoch 5/100
600/600 [==============================] - 0s 205us/step - loss: 0.6207 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6283 - val_loss: 0.6494 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6166
Epoch 6/100
600/600 [==============================] - 0s 221us/step - loss: 0.6242 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6610 - val_loss: 0.6460 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6523
Epoch 7/100
600/600 [==============================] - 0s 204us/step - loss: 0.6212 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6802 - val_loss: 0.6485 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6555
Epoch 8/100
600/600 [==============================] - 0s 165us/step - loss: 0.6208 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6604 - val_loss: 0.6487 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6724
Epoch 9/100
600/600 [==============================] - 0s 209us/step - loss: 0.6206 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6819 - val_loss: 0.6475 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6786
Epoch 10/100
600/600 [==============================] - 0s 209us/step - loss: 0.6204 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.6902 - val_loss: 0.6478 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6921
Epoch 11/100
600/600 [==============================] - 0s 208us/step - loss: 0.6214 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7055 - val_loss: 0.6507 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.6955
Epoch 12/100
600/600 [==============================] - 0s 213us/step - loss: 0.6210 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7163 - val_loss: 0.6474 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7108
Epoch 13/100
600/600 [==============================] - 0s 200us/step - loss: 0.6214 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7424 - val_loss: 0.6469 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7362
Epoch 14/100
600/600 [==============================] - 0s 208us/step - loss: 0.6210 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7609 - val_loss: 0.6490 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7546
Epoch 15/100
600/600 [==============================] - 0s 211us/step - loss: 0.6209 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.7896 - val_loss: 0.6483 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7750
Epoch 16/100
600/600 [==============================] - 0s 200us/step - loss: 0.6196 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8109 - val_loss: 0.6468 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7822
Epoch 17/100
600/600 [==============================] - 0s 207us/step - loss: 0.6197 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8398 - val_loss: 0.6454 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.7954
Epoch 18/100
600/600 [==============================] - 0s 218us/step - loss: 0.6186 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8429 - val_loss: 0.6461 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8070
Epoch 19/100
600/600 [==============================] - 0s 195us/step - loss: 0.6180 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8581 - val_loss: 0.6461 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8090
Epoch 20/100
600/600 [==============================] - 0s 193us/step - loss: 0.6168 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8637 - val_loss: 0.6444 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8159
Epoch 21/100
600/600 [==============================] - 0s 209us/step - loss: 0.6152 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8666 - val_loss: 0.6439 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8237
Epoch 22/100
600/600 [==============================] - 0s 205us/step - loss: 0.6125 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8643 - val_loss: 0.6409 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8274
Epoch 23/100
600/600 [==============================] - 0s 205us/step - loss: 0.6088 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8666 - val_loss: 0.6376 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8309
Epoch 24/100
600/600 [==============================] - 0s 211us/step - loss: 0.6052 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8658 - val_loss: 0.6340 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8308
Epoch 25/100
600/600 [==============================] - 0s 208us/step - loss: 0.5990 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8621 - val_loss: 0.6355 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8311
Epoch 26/100
600/600 [==============================] - 0s 215us/step - loss: 0.5922 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8571 - val_loss: 0.6257 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8350
Epoch 27/100
600/600 [==============================] - 0s 190us/step - loss: 0.5834 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8621 - val_loss: 0.6234 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8313
Epoch 28/100
600/600 [==============================] - 0s 196us/step - loss: 0.5703 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8611 - val_loss: 0.6218 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8326
Epoch 29/100
600/600 [==============================] - 0s 206us/step - loss: 0.5568 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8695 - val_loss: 0.6038 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8427
Epoch 30/100
600/600 [==============================] - 0s 217us/step - loss: 0.5402 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8784 - val_loss: 0.6115 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8486
Epoch 31/100
600/600 [==============================] - 0s 214us/step - loss: 0.5244 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.8889 - val_loss: 0.5794 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8590
Epoch 32/100
600/600 [==============================] - 0s 212us/step - loss: 0.5045 - binary_accuracy: 0.6883 - sensitivity: 1.0000 - specificity: 0.0000e+00 - gmeasure: 0.0000e+00 - auc: 0.9018 - val_loss: 0.5636 - val_binary_accuracy: 0.6533 - val_sensitivity: 1.0000 - val_specificity: 0.0000e+00 - val_gmeasure: 0.0000e+00 - val_auc: 0.8688
Epoch 33/100
600/600 [==============================] - 0s 225us/step - loss: 0.4838 - binary_accuracy: 0.7467 - sensitivity: 0.9901 - specificity: 0.2166 - gmeasure: 0.4113 - auc: 0.8978 - val_loss: 0.5656 - val_binary_accuracy: 0.7200 - val_sensitivity: 1.0000 - val_specificity: 0.2022 - val_gmeasure: 0.4388 - val_auc: 0.8721
Epoch 34/100
600/600 [==============================] - 0s 209us/step - loss: 0.4699 - binary_accuracy: 0.7817 - sensitivity: 0.9828 - specificity: 0.3467 - gmeasure: 0.5740 - auc: 0.9062 - val_loss: 0.5281 - val_binary_accuracy: 0.7600 - val_sensitivity: 1.0000 - val_specificity: 0.3342 - val_gmeasure: 0.5680 - val_auc: 0.8860
Epoch 35/100
600/600 [==============================] - 0s 210us/step - loss: 0.4492 - binary_accuracy: 0.8017 - sensitivity: 0.9904 - specificity: 0.3937 - gmeasure: 0.6147 - auc: 0.9158 - val_loss: 0.5507 - val_binary_accuracy: 0.7467 - val_sensitivity: 1.0000 - val_specificity: 0.2881 - val_gmeasure: 0.5307 - val_auc: 0.8858
Epoch 36/100
600/600 [==============================] - 0s 179us/step - loss: 0.4478 - binary_accuracy: 0.8200 - sensitivity: 0.9852 - specificity: 0.4695 - gmeasure: 0.6449 - auc: 0.9294 - val_loss: 0.4947 - val_binary_accuracy: 0.7800 - val_sensitivity: 1.0000 - val_specificity: 0.3854 - val_gmeasure: 0.6162 - val_auc: 0.8947
Epoch 37/100
600/600 [==============================] - 0s 184us/step - loss: 0.4121 - binary_accuracy: 0.8017 - sensitivity: 0.9882 - specificity: 0.3980 - gmeasure: 0.6138 - auc: 0.9307 - val_loss: 0.4736 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.9660 - val_specificity: 0.6095 - val_gmeasure: 0.7660 - val_auc: 0.9047
Epoch 38/100
600/600 [==============================] - 0s 209us/step - loss: 0.4105 - binary_accuracy: 0.8400 - sensitivity: 0.9864 - specificity: 0.5395 - gmeasure: 0.7159 - auc: 0.9408 - val_loss: 0.4539 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.9660 - val_specificity: 0.6095 - val_gmeasure: 0.7660 - val_auc: 0.9083
Epoch 39/100
600/600 [==============================] - 0s 199us/step - loss: 0.3914 - binary_accuracy: 0.8367 - sensitivity: 0.9907 - specificity: 0.5032 - gmeasure: 0.6967 - auc: 0.9467 - val_loss: 0.4417 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.9565 - val_specificity: 0.6462 - val_gmeasure: 0.7855 - val_auc: 0.9118
Epoch 40/100
600/600 [==============================] - 0s 204us/step - loss: 0.3771 - binary_accuracy: 0.8517 - sensitivity: 0.9815 - specificity: 0.5919 - gmeasure: 0.7529 - auc: 0.9539 - val_loss: 0.4259 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.9877 - val_specificity: 0.5805 - val_gmeasure: 0.7566 - val_auc: 0.9148
Epoch 41/100
600/600 [==============================] - 0s 187us/step - loss: 0.3744 - binary_accuracy: 0.8517 - sensitivity: 0.9748 - specificity: 0.5967 - gmeasure: 0.7536 - auc: 0.9517 - val_loss: 0.4203 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.9877 - val_specificity: 0.5438 - val_gmeasure: 0.7312 - val_auc: 0.9170
Epoch 42/100
600/600 [==============================] - 0s 211us/step - loss: 0.3696 - binary_accuracy: 0.8733 - sensitivity: 0.9674 - specificity: 0.6891 - gmeasure: 0.8062 - auc: 0.9602 - val_loss: 0.4314 - val_binary_accuracy: 0.8133 - val_sensitivity: 1.0000 - val_specificity: 0.4671 - val_gmeasure: 0.6832 - val_auc: 0.9152
Epoch 43/100
600/600 [==============================] - 0s 208us/step - loss: 0.3451 - binary_accuracy: 0.8867 - sensitivity: 0.9789 - specificity: 0.6729 - gmeasure: 0.8028 - auc: 0.9577 - val_loss: 0.4361 - val_binary_accuracy: 0.8000 - val_sensitivity: 1.0000 - val_specificity: 0.4288 - val_gmeasure: 0.6544 - val_auc: 0.9202
Epoch 44/100
600/600 [==============================] - 0s 209us/step - loss: 0.3369 - binary_accuracy: 0.8850 - sensitivity: 0.9759 - specificity: 0.6827 - gmeasure: 0.8103 - auc: 0.9609 - val_loss: 0.4210 - val_binary_accuracy: 0.8200 - val_sensitivity: 1.0000 - val_specificity: 0.4816 - val_gmeasure: 0.6939 - val_auc: 0.9232
Epoch 45/100
600/600 [==============================] - 0s 217us/step - loss: 0.3284 - binary_accuracy: 0.8717 - sensitivity: 0.9810 - specificity: 0.6311 - gmeasure: 0.7737 - auc: 0.9663 - val_loss: 0.3899 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.9877 - val_specificity: 0.5438 - val_gmeasure: 0.7312 - val_auc: 0.9259
Epoch 46/100
600/600 [==============================] - 0s 176us/step - loss: 0.3125 - binary_accuracy: 0.9067 - sensitivity: 0.9812 - specificity: 0.7430 - gmeasure: 0.8518 - auc: 0.9646 - val_loss: 0.4486 - val_binary_accuracy: 0.8067 - val_sensitivity: 1.0000 - val_specificity: 0.4433 - val_gmeasure: 0.6657 - val_auc: 0.9294
Epoch 47/100
600/600 [==============================] - 0s 175us/step - loss: 0.3150 - binary_accuracy: 0.8917 - sensitivity: 0.9783 - specificity: 0.6934 - gmeasure: 0.8143 - auc: 0.9655 - val_loss: 0.3610 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.9658 - val_specificity: 0.6317 - val_gmeasure: 0.7808 - val_auc: 0.9327
Epoch 48/100
600/600 [==============================] - 0s 192us/step - loss: 0.2954 - binary_accuracy: 0.9000 - sensitivity: 0.9757 - specificity: 0.7291 - gmeasure: 0.8372 - auc: 0.9656 - val_loss: 0.3510 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.9658 - val_specificity: 0.6684 - val_gmeasure: 0.8030 - val_auc: 0.9364
Epoch 49/100
600/600 [==============================] - 0s 220us/step - loss: 0.2864 - binary_accuracy: 0.9100 - sensitivity: 0.9764 - specificity: 0.7661 - gmeasure: 0.8629 - auc: 0.9690 - val_loss: 0.3460 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.9658 - val_specificity: 0.6462 - val_gmeasure: 0.7893 - val_auc: 0.9388
Epoch 50/100
600/600 [==============================] - 0s 216us/step - loss: 0.2779 - binary_accuracy: 0.9150 - sensitivity: 0.9692 - specificity: 0.8009 - gmeasure: 0.8797 - auc: 0.9745 - val_loss: 0.3476 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.9781 - val_specificity: 0.6317 - val_gmeasure: 0.7859 - val_auc: 0.9368
Epoch 51/100
600/600 [==============================] - 0s 199us/step - loss: 0.2694 - binary_accuracy: 0.9117 - sensitivity: 0.9723 - specificity: 0.7651 - gmeasure: 0.8605 - auc: 0.9716 - val_loss: 0.3379 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.9658 - val_specificity: 0.6317 - val_gmeasure: 0.7808 - val_auc: 0.9408
Epoch 52/100
600/600 [==============================] - 0s 200us/step - loss: 0.2641 - binary_accuracy: 0.9200 - sensitivity: 0.9709 - specificity: 0.8161 - gmeasure: 0.8867 - auc: 0.9754 - val_loss: 0.3404 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.9781 - val_specificity: 0.6317 - val_gmeasure: 0.7859 - val_auc: 0.9429
Epoch 53/100
600/600 [==============================] - 0s 205us/step - loss: 0.2620 - binary_accuracy: 0.9117 - sensitivity: 0.9662 - specificity: 0.7884 - gmeasure: 0.8681 - auc: 0.9772 - val_loss: 0.3261 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.9563 - val_specificity: 0.6684 - val_gmeasure: 0.7991 - val_auc: 0.9450
Epoch 54/100
600/600 [==============================] - 0s 207us/step - loss: 0.2560 - binary_accuracy: 0.9167 - sensitivity: 0.9698 - specificity: 0.7896 - gmeasure: 0.8696 - auc: 0.9738 - val_loss: 0.3195 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.9563 - val_specificity: 0.6829 - val_gmeasure: 0.8073 - val_auc: 0.9467
Epoch 55/100
600/600 [==============================] - 0s 194us/step - loss: 0.2519 - binary_accuracy: 0.9267 - sensitivity: 0.9670 - specificity: 0.8420 - gmeasure: 0.9004 - auc: 0.9798 - val_loss: 0.3144 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.9439 - val_specificity: 0.7289 - val_gmeasure: 0.8291 - val_auc: 0.9474
Epoch 56/100
600/600 [==============================] - 0s 207us/step - loss: 0.2410 - binary_accuracy: 0.9233 - sensitivity: 0.9723 - specificity: 0.8135 - gmeasure: 0.8876 - auc: 0.9769 - val_loss: 0.3050 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.9316 - val_specificity: 0.7765 - val_gmeasure: 0.8503 - val_auc: 0.9500
Epoch 57/100
600/600 [==============================] - 0s 198us/step - loss: 0.2459 - binary_accuracy: 0.9233 - sensitivity: 0.9620 - specificity: 0.8446 - gmeasure: 0.8999 - auc: 0.9790 - val_loss: 0.2995 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.9316 - val_specificity: 0.7765 - val_gmeasure: 0.8503 - val_auc: 0.9525
Epoch 58/100
600/600 [==============================] - 0s 198us/step - loss: 0.2425 - binary_accuracy: 0.9283 - sensitivity: 0.9743 - specificity: 0.8324 - gmeasure: 0.8974 - auc: 0.9780 - val_loss: 0.2976 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9128 - val_specificity: 0.8371 - val_gmeasure: 0.8740 - val_auc: 0.9538
Epoch 59/100
600/600 [==============================] - 0s 212us/step - loss: 0.2341 - binary_accuracy: 0.9233 - sensitivity: 0.9729 - specificity: 0.8176 - gmeasure: 0.8899 - auc: 0.9795 - val_loss: 0.2913 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9316 - val_specificity: 0.7988 - val_gmeasure: 0.8626 - val_auc: 0.9555
Epoch 60/100
600/600 [==============================] - 0s 211us/step - loss: 0.2234 - binary_accuracy: 0.9317 - sensitivity: 0.9647 - specificity: 0.8515 - gmeasure: 0.9042 - auc: 0.9777 - val_loss: 0.3086 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.9686 - val_specificity: 0.6462 - val_gmeasure: 0.7908 - val_auc: 0.9542
Epoch 61/100
600/600 [==============================] - 0s 213us/step - loss: 0.2173 - binary_accuracy: 0.9383 - sensitivity: 0.9662 - specificity: 0.8869 - gmeasure: 0.9230 - auc: 0.9811 - val_loss: 0.3353 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.9781 - val_specificity: 0.6095 - val_gmeasure: 0.7713 - val_auc: 0.9534
Epoch 62/100
600/600 [==============================] - 0s 210us/step - loss: 0.2263 - binary_accuracy: 0.9267 - sensitivity: 0.9670 - specificity: 0.8481 - gmeasure: 0.9029 - auc: 0.9786 - val_loss: 0.2922 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.9439 - val_specificity: 0.7305 - val_gmeasure: 0.8299 - val_auc: 0.9549
Epoch 63/100
600/600 [==============================] - 0s 208us/step - loss: 0.2155 - binary_accuracy: 0.9350 - sensitivity: 0.9682 - specificity: 0.8658 - gmeasure: 0.9138 - auc: 0.9810 - val_loss: 0.2812 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9316 - val_specificity: 0.7988 - val_gmeasure: 0.8626 - val_auc: 0.9557
Epoch 64/100
600/600 [==============================] - 0s 227us/step - loss: 0.2108 - binary_accuracy: 0.9333 - sensitivity: 0.9714 - specificity: 0.8520 - gmeasure: 0.9075 - auc: 0.9784 - val_loss: 0.2812 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9128 - val_specificity: 0.8371 - val_gmeasure: 0.8740 - val_auc: 0.9548
Epoch 65/100
600/600 [==============================] - 0s 234us/step - loss: 0.2220 - binary_accuracy: 0.9333 - sensitivity: 0.9640 - specificity: 0.8665 - gmeasure: 0.9106 - auc: 0.9813 - val_loss: 0.2871 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.9439 - val_specificity: 0.7382 - val_gmeasure: 0.8348 - val_auc: 0.9574
Epoch 66/100
600/600 [==============================] - 0s 208us/step - loss: 0.2127 - binary_accuracy: 0.9300 - sensitivity: 0.9582 - specificity: 0.8632 - gmeasure: 0.9069 - auc: 0.9821 - val_loss: 0.3111 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.9686 - val_specificity: 0.6700 - val_gmeasure: 0.8053 - val_auc: 0.9584
Epoch 67/100
600/600 [==============================] - 0s 202us/step - loss: 0.1997 - binary_accuracy: 0.9450 - sensitivity: 0.9660 - specificity: 0.8976 - gmeasure: 0.9300 - auc: 0.9814 - val_loss: 0.3083 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.9686 - val_specificity: 0.6700 - val_gmeasure: 0.8053 - val_auc: 0.9578
Epoch 68/100
600/600 [==============================] - 0s 205us/step - loss: 0.2006 - binary_accuracy: 0.9467 - sensitivity: 0.9703 - specificity: 0.8902 - gmeasure: 0.9277 - auc: 0.9827 - val_loss: 0.2855 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.9563 - val_specificity: 0.7382 - val_gmeasure: 0.8402 - val_auc: 0.9579
Epoch 69/100
600/600 [==============================] - 0s 207us/step - loss: 0.1968 - binary_accuracy: 0.9433 - sensitivity: 0.9690 - specificity: 0.8936 - gmeasure: 0.9297 - auc: 0.9861 - val_loss: 0.2958 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9686 - val_specificity: 0.7382 - val_gmeasure: 0.8455 - val_auc: 0.9574
Epoch 70/100
600/600 [==============================] - 0s 209us/step - loss: 0.1914 - binary_accuracy: 0.9350 - sensitivity: 0.9663 - specificity: 0.8612 - gmeasure: 0.9111 - auc: 0.9806 - val_loss: 0.2761 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9439 - val_specificity: 0.7765 - val_gmeasure: 0.8560 - val_auc: 0.9581
Epoch 71/100
600/600 [==============================] - 0s 203us/step - loss: 0.1855 - binary_accuracy: 0.9433 - sensitivity: 0.9696 - specificity: 0.8715 - gmeasure: 0.9185 - auc: 0.9817 - val_loss: 0.2642 - val_binary_accuracy: 0.9000 - val_sensitivity: 0.9223 - val_specificity: 0.8516 - val_gmeasure: 0.8858 - val_auc: 0.9589
Epoch 72/100
600/600 [==============================] - 0s 211us/step - loss: 0.1876 - binary_accuracy: 0.9433 - sensitivity: 0.9665 - specificity: 0.8968 - gmeasure: 0.9303 - auc: 0.9828 - val_loss: 0.2695 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9439 - val_specificity: 0.7765 - val_gmeasure: 0.8560 - val_auc: 0.9594
Epoch 73/100
600/600 [==============================] - 0s 210us/step - loss: 0.1838 - binary_accuracy: 0.9467 - sensitivity: 0.9679 - specificity: 0.9025 - gmeasure: 0.9333 - auc: 0.9831 - val_loss: 0.2681 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.9439 - val_specificity: 0.7988 - val_gmeasure: 0.8683 - val_auc: 0.9584
Epoch 74/100
600/600 [==============================] - 0s 209us/step - loss: 0.1825 - binary_accuracy: 0.9400 - sensitivity: 0.9682 - specificity: 0.8806 - gmeasure: 0.9223 - auc: 0.9822 - val_loss: 0.2622 - val_binary_accuracy: 0.9000 - val_sensitivity: 0.9128 - val_specificity: 0.8754 - val_gmeasure: 0.8934 - val_auc: 0.9578
Epoch 75/100
600/600 [==============================] - 0s 214us/step - loss: 0.1825 - binary_accuracy: 0.9333 - sensitivity: 0.9657 - specificity: 0.8641 - gmeasure: 0.9126 - auc: 0.9853 - val_loss: 0.2787 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9686 - val_specificity: 0.7382 - val_gmeasure: 0.8455 - val_auc: 0.9604
Epoch 76/100
600/600 [==============================] - 0s 194us/step - loss: 0.1794 - binary_accuracy: 0.9483 - sensitivity: 0.9687 - specificity: 0.9020 - gmeasure: 0.9339 - auc: 0.9848 - val_loss: 0.3300 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.9781 - val_specificity: 0.6555 - val_gmeasure: 0.8004 - val_auc: 0.9592
Epoch 77/100
600/600 [==============================] - 0s 192us/step - loss: 0.1827 - binary_accuracy: 0.9433 - sensitivity: 0.9647 - specificity: 0.9030 - gmeasure: 0.9320 - auc: 0.9823 - val_loss: 0.3038 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9781 - val_specificity: 0.7160 - val_gmeasure: 0.8365 - val_auc: 0.9605
Epoch 78/100
600/600 [==============================] - 0s 219us/step - loss: 0.1862 - binary_accuracy: 0.9383 - sensitivity: 0.9671 - specificity: 0.8742 - gmeasure: 0.9165 - auc: 0.9846 - val_loss: 0.2766 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9686 - val_specificity: 0.7382 - val_gmeasure: 0.8455 - val_auc: 0.9611
Epoch 79/100
600/600 [==============================] - 0s 199us/step - loss: 0.1971 - binary_accuracy: 0.9333 - sensitivity: 0.9607 - specificity: 0.8784 - gmeasure: 0.9168 - auc: 0.9824 - val_loss: 0.2595 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9439 - val_specificity: 0.7765 - val_gmeasure: 0.8560 - val_auc: 0.9611
Epoch 80/100
600/600 [==============================] - 0s 206us/step - loss: 0.1824 - binary_accuracy: 0.9400 - sensitivity: 0.9703 - specificity: 0.8695 - gmeasure: 0.9160 - auc: 0.9821 - val_loss: 0.2501 - val_binary_accuracy: 0.9000 - val_sensitivity: 0.9128 - val_specificity: 0.8754 - val_gmeasure: 0.8934 - val_auc: 0.9601
Epoch 81/100
600/600 [==============================] - 0s 208us/step - loss: 0.1698 - binary_accuracy: 0.9433 - sensitivity: 0.9685 - specificity: 0.8874 - gmeasure: 0.9257 - auc: 0.9850 - val_loss: 0.2483 - val_binary_accuracy: 0.9000 - val_sensitivity: 0.9223 - val_specificity: 0.8516 - val_gmeasure: 0.8858 - val_auc: 0.9606
Epoch 82/100
600/600 [==============================] - 0s 199us/step - loss: 0.1690 - binary_accuracy: 0.9533 - sensitivity: 0.9708 - specificity: 0.9219 - gmeasure: 0.9450 - auc: 0.9855 - val_loss: 0.2471 - val_binary_accuracy: 0.9133 - val_sensitivity: 0.9439 - val_specificity: 0.8516 - val_gmeasure: 0.8963 - val_auc: 0.9613
Epoch 83/100
600/600 [==============================] - 0s 210us/step - loss: 0.1671 - binary_accuracy: 0.9500 - sensitivity: 0.9663 - specificity: 0.9151 - gmeasure: 0.9397 - auc: 0.9858 - val_loss: 0.2789 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9686 - val_specificity: 0.7382 - val_gmeasure: 0.8455 - val_auc: 0.9634
Epoch 84/100
600/600 [==============================] - 0s 211us/step - loss: 0.1699 - binary_accuracy: 0.9450 - sensitivity: 0.9606 - specificity: 0.9153 - gmeasure: 0.9362 - auc: 0.9867 - val_loss: 0.2655 - val_binary_accuracy: 0.9000 - val_sensitivity: 0.9686 - val_specificity: 0.7765 - val_gmeasure: 0.8670 - val_auc: 0.9629
Epoch 85/100
600/600 [==============================] - 0s 210us/step - loss: 0.1679 - binary_accuracy: 0.9450 - sensitivity: 0.9700 - specificity: 0.8891 - gmeasure: 0.9268 - auc: 0.9875 - val_loss: 0.2474 - val_binary_accuracy: 0.9133 - val_sensitivity: 0.9439 - val_specificity: 0.8516 - val_gmeasure: 0.8963 - val_auc: 0.9625
Epoch 86/100
600/600 [==============================] - 0s 208us/step - loss: 0.1597 - binary_accuracy: 0.9533 - sensitivity: 0.9713 - specificity: 0.9162 - gmeasure: 0.9428 - auc: 0.9856 - val_loss: 0.2502 - val_binary_accuracy: 0.9000 - val_sensitivity: 0.9439 - val_specificity: 0.8226 - val_gmeasure: 0.8809 - val_auc: 0.9625
Epoch 87/100
600/600 [==============================] - 0s 205us/step - loss: 0.1568 - binary_accuracy: 0.9517 - sensitivity: 0.9706 - specificity: 0.9132 - gmeasure: 0.9407 - auc: 0.9871 - val_loss: 0.2563 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9439 - val_specificity: 0.7765 - val_gmeasure: 0.8560 - val_auc: 0.9635
Epoch 88/100
600/600 [==============================] - 0s 219us/step - loss: 0.1593 - binary_accuracy: 0.9550 - sensitivity: 0.9685 - specificity: 0.9223 - gmeasure: 0.9441 - auc: 0.9868 - val_loss: 0.2485 - val_binary_accuracy: 0.9067 - val_sensitivity: 0.9439 - val_specificity: 0.8371 - val_gmeasure: 0.8887 - val_auc: 0.9631
Epoch 89/100
600/600 [==============================] - 0s 205us/step - loss: 0.1523 - binary_accuracy: 0.9500 - sensitivity: 0.9713 - specificity: 0.9040 - gmeasure: 0.9362 - auc: 0.9887 - val_loss: 0.2770 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.9686 - val_specificity: 0.7620 - val_gmeasure: 0.8586 - val_auc: 0.9636
Epoch 90/100
600/600 [==============================] - 0s 191us/step - loss: 0.1606 - binary_accuracy: 0.9467 - sensitivity: 0.9691 - specificity: 0.9015 - gmeasure: 0.9337 - auc: 0.9868 - val_loss: 0.2698 - val_binary_accuracy: 0.9000 - val_sensitivity: 0.9686 - val_specificity: 0.7765 - val_gmeasure: 0.8670 - val_auc: 0.9641
Epoch 91/100
600/600 [==============================] - 0s 201us/step - loss: 0.1522 - binary_accuracy: 0.9467 - sensitivity: 0.9713 - specificity: 0.8975 - gmeasure: 0.9330 - auc: 0.9896 - val_loss: 0.2484 - val_binary_accuracy: 0.9067 - val_sensitivity: 0.9439 - val_specificity: 0.8371 - val_gmeasure: 0.8887 - val_auc: 0.9636
Epoch 92/100
600/600 [==============================] - 0s 208us/step - loss: 0.1505 - binary_accuracy: 0.9483 - sensitivity: 0.9662 - specificity: 0.9097 - gmeasure: 0.9368 - auc: 0.9884 - val_loss: 0.2395 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9439 - val_specificity: 0.8754 - val_gmeasure: 0.9085 - val_auc: 0.9643
Epoch 93/100
600/600 [==============================] - 0s 213us/step - loss: 0.1490 - binary_accuracy: 0.9500 - sensitivity: 0.9715 - specificity: 0.9028 - gmeasure: 0.9353 - auc: 0.9870 - val_loss: 0.2480 - val_binary_accuracy: 0.9067 - val_sensitivity: 0.9439 - val_specificity: 0.8371 - val_gmeasure: 0.8887 - val_auc: 0.9654
Epoch 94/100
600/600 [==============================] - 0s 209us/step - loss: 0.1479 - binary_accuracy: 0.9533 - sensitivity: 0.9703 - specificity: 0.9117 - gmeasure: 0.9400 - auc: 0.9865 - val_loss: 0.2493 - val_binary_accuracy: 0.9067 - val_sensitivity: 0.9439 - val_specificity: 0.8371 - val_gmeasure: 0.8887 - val_auc: 0.9631
Epoch 95/100
600/600 [==============================] - 0s 214us/step - loss: 0.1477 - binary_accuracy: 0.9517 - sensitivity: 0.9718 - specificity: 0.9124 - gmeasure: 0.9411 - auc: 0.9864 - val_loss: 0.2448 - val_binary_accuracy: 0.9067 - val_sensitivity: 0.9439 - val_specificity: 0.8371 - val_gmeasure: 0.8887 - val_auc: 0.9643
Epoch 96/100
600/600 [==============================] - 0s 200us/step - loss: 0.1449 - binary_accuracy: 0.9500 - sensitivity: 0.9669 - specificity: 0.9066 - gmeasure: 0.9356 - auc: 0.9884 - val_loss: 0.2648 - val_binary_accuracy: 0.9000 - val_sensitivity: 0.9686 - val_specificity: 0.7765 - val_gmeasure: 0.8670 - val_auc: 0.9659
Epoch 97/100
600/600 [==============================] - 0s 201us/step - loss: 0.1471 - binary_accuracy: 0.9550 - sensitivity: 0.9705 - specificity: 0.9218 - gmeasure: 0.9453 - auc: 0.9873 - val_loss: 0.2602 - val_binary_accuracy: 0.9000 - val_sensitivity: 0.9686 - val_specificity: 0.7765 - val_gmeasure: 0.8670 - val_auc: 0.9653
Epoch 98/100
600/600 [==============================] - 0s 199us/step - loss: 0.1464 - binary_accuracy: 0.9517 - sensitivity: 0.9615 - specificity: 0.9249 - gmeasure: 0.9426 - auc: 0.9851 - val_loss: 0.2350 - val_binary_accuracy: 0.9267 - val_sensitivity: 0.9439 - val_specificity: 0.8976 - val_gmeasure: 0.9203 - val_auc: 0.9629
Epoch 99/100
600/600 [==============================] - 0s 197us/step - loss: 0.1455 - binary_accuracy: 0.9483 - sensitivity: 0.9713 - specificity: 0.9031 - gmeasure: 0.9356 - auc: 0.9886 - val_loss: 0.2334 - val_binary_accuracy: 0.9267 - val_sensitivity: 0.9439 - val_specificity: 0.8976 - val_gmeasure: 0.9203 - val_auc: 0.9607
Epoch 100/100
600/600 [==============================] - 0s 216us/step - loss: 0.1571 - binary_accuracy: 0.9567 - sensitivity: 0.9737 - specificity: 0.9207 - gmeasure: 0.9460 - auc: 0.9895 - val_loss: 0.2467 - val_binary_accuracy: 0.9067 - val_sensitivity: 0.9035 - val_specificity: 0.9121 - val_gmeasure: 0.9075 - val_auc: 0.9590
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:151] Training end with time 14.564016819000244!
[root    |INFO|build_network.py:79] Saved trained model weight at ./example_result/weight_2.h5
[root    |DEBUG|deepbiome.py:166] Save weight at ./example_result/weight_2.h5
[root    |DEBUG|deepbiome.py:169] Save history at ./example_result/hist_2.json
[root    |INFO|build_network.py:177] Evaluation start!
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
750/750 [==============================] - 0s 8us/step
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:182] Evaluation end with time 0.013907432556152344!
[root    |INFO|build_network.py:183] Evaluation: [0.1908942461013794, 0.9333333373069763, 0.9217221140861511, 0.9581589698791504, 0.9397639632225037, 0.9826658964157104]
[root    |INFO|build_network.py:177] Evaluation start!
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
250/250 [==============================] - 0s 22us/step
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:182] Evaluation end with time 0.013725757598876953!
[root    |INFO|build_network.py:183] Evaluation: [0.3346303701400757, 0.8519999980926514, 0.8314606547355652, 0.9027777910232544, 0.8663857579231262, 0.930282473564148]
[root    |INFO|deepbiome.py:179] Compute time : 16.148853302001953
[root    |INFO|deepbiome.py:180] 3 fold computing end!---------------------------------------------
[root    |INFO|deepbiome.py:183] -----------------------------------------------------------------
[root    |INFO|deepbiome.py:185] Train Evaluation : [&#39;loss&#39; &#39;binary_accuracy&#39; &#39;sensitivity&#39; &#39;specificity&#39; &#39;gmeasure&#39; &#39;auc&#39;]
[root    |INFO|deepbiome.py:188]       mean : [0.26409377 0.88844444 0.88150728 0.90457902 0.89295071 0.94405381]
[root    |INFO|deepbiome.py:189]        std : [0.05276841 0.03453536 0.03235713 0.03878116 0.03517411 0.033656  ]
[root    |INFO|deepbiome.py:190] -----------------------------------------------------------------
[root    |INFO|deepbiome.py:192] Test Evaluation : [&#39;loss&#39; &#39;binary_accuracy&#39; &#39;sensitivity&#39; &#39;specificity&#39; &#39;gmeasure&#39; &#39;auc&#39;]
[root    |INFO|deepbiome.py:195]       mean : [0.3420873  0.84133333 0.84673119 0.8313028  0.83841147 0.91362449]
[root    |INFO|deepbiome.py:196]        std : [0.02104079 0.01236482 0.01331369 0.05292793 0.0230854  0.02229744]
[root    |INFO|deepbiome.py:197] -----------------------------------------------------------------
[root    |INFO|deepbiome.py:206] Total Computing Ended
[root    |INFO|deepbiome.py:207] -----------------------------------------------------------------
</pre></div></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">deepbiome_train</span></code> save the trained model weights, evaluation results and history based on the path information from the configuration.</p>
<p>From the example above, we can check that <code class="docutils literal notranslate"><span class="pre">hist_*.json</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_*.h5</span></code>, <code class="docutils literal notranslate"><span class="pre">test_eval.npy</span></code>, <code class="docutils literal notranslate"><span class="pre">train_eval.npy</span></code> files were saved.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">path_info</span><span class="p">[</span><span class="s1">&#39;model_info&#39;</span><span class="p">][</span><span class="s1">&#39;model_dir&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>[&#39;hist_0.json&#39;,
 &#39;weight_2.h5&#39;,
 &#39;test_eval.npy&#39;,
 &#39;weight_0.h5&#39;,
 &#39;train_eval.npy&#39;,
 &#39;hist_2.json&#39;,
 &#39;weight_1.h5&#39;,
 &#39;hist_1.json&#39;]
</pre></div>
</div>
</div>
<p>Lets check the history files.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[24]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;./</span><span class="si">%s</span><span class="s1">/hist_0.json&#39;</span> <span class="o">%</span> <span class="n">path_info</span><span class="p">[</span><span class="s1">&#39;model_info&#39;</span><span class="p">][</span><span class="s1">&#39;model_dir&#39;</span><span class="p">],</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">history</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epochs&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/example_with_the_list_of_inputs_43_0.png" src="_images/example_with_the_list_of_inputs_43_0.png" />
</div>
</div>
<p>Test evauation and train evauation is the numpy array of the shape (number of fold, number of evaluation measures).</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[25]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">test_evaluation</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[25]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>array([[0.32086846, 0.84799999, 0.86390531, 0.81481481, 0.83900112,
        0.92848271],
       [0.37076306, 0.824     , 0.84482759, 0.77631581, 0.80984753,
        0.88210827],
       [0.33463037, 0.852     , 0.83146065, 0.90277779, 0.86638576,
        0.93028247]])
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[26]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">train_evaluation</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[26]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>array([[0.28811851, 0.88266665, 0.88030887, 0.88793105, 0.88411176,
        0.94885004],
       [0.31326854, 0.84933335, 0.84249085, 0.86764705, 0.85497642,
        0.90064549],
       [0.19089425, 0.93333334, 0.92172211, 0.95815897, 0.93976396,
        0.9826659 ]])
</pre></div>
</div>
</div>
</div>
<div class="section" id="5.-Load-the-pretrained-network-for-training">
<h2>5. Load the pretrained network for training<a class="headerlink" href="#5.-Load-the-pretrained-network-for-training" title="Permalink to this headline">¶</a></h2>
<p>If you have pre-trianed model, you can use the pre-trained weight for next training. For using pre-trained weights, you have to use <code class="docutils literal notranslate"><span class="pre">warm_start</span></code> option in <code class="docutils literal notranslate"><span class="pre">training_inro</span></code> with addding the file path of the pre-trained weights in the <code class="docutils literal notranslate"><span class="pre">warm_start_model</span></code> option. Below is the example:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[27]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">network_info</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;architecture_info&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;batch_normalization&#39;</span><span class="p">:</span> <span class="s1">&#39;False&#39;</span><span class="p">,</span>
        <span class="s1">&#39;drop_out&#39;</span><span class="p">:</span> <span class="s1">&#39;0&#39;</span><span class="p">,</span>
        <span class="s1">&#39;weight_initial&#39;</span><span class="p">:</span> <span class="s1">&#39;glorot_uniform&#39;</span><span class="p">,</span>
        <span class="s1">&#39;weight_l1_penalty&#39;</span><span class="p">:</span><span class="s1">&#39;0.01&#39;</span><span class="p">,</span>
        <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="s1">&#39;phylogenetic_tree&#39;</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s1">&#39;model_info&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;decay&#39;</span><span class="p">:</span> <span class="s1">&#39;0.001&#39;</span><span class="p">,</span>
        <span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span>
        <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="s1">&#39;0.01&#39;</span><span class="p">,</span>
        <span class="s1">&#39;metrics&#39;</span><span class="p">:</span> <span class="s1">&#39;binary_accuracy, sensitivity, specificity, gmeasure, auc&#39;</span><span class="p">,</span>
        <span class="s1">&#39;network_class&#39;</span><span class="p">:</span> <span class="s1">&#39;DeepBiomeNetwork&#39;</span><span class="p">,</span>
        <span class="s1">&#39;normalizer&#39;</span><span class="p">:</span> <span class="s1">&#39;normalize_minmax&#39;</span><span class="p">,</span>
        <span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="s1">&#39;adam&#39;</span><span class="p">,</span>
        <span class="s1">&#39;reader_class&#39;</span><span class="p">:</span> <span class="s1">&#39;MicroBiomeClassificationReader&#39;</span><span class="p">,</span>
        <span class="s1">&#39;texa_selection_metrics&#39;</span><span class="p">:</span> <span class="s1">&#39;accuracy, sensitivity, specificity, gmeasure&#39;</span>
    <span class="p">},</span>
    <span class="s1">&#39;training_info&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;warm_start&#39;</span><span class="p">:</span><span class="s1">&#39;True&#39;</span><span class="p">,</span>
        <span class="s1">&#39;warm_start_model&#39;</span><span class="p">:</span><span class="s1">&#39;./example_result/weight.h5&#39;</span><span class="p">,</span>
        <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="s1">&#39;200&#39;</span><span class="p">,</span>
        <span class="s1">&#39;epochs&#39;</span><span class="p">:</span> <span class="s1">&#39;100&#39;</span>
    <span class="p">},</span>
    <span class="s1">&#39;validation_info&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="s1">&#39;None&#39;</span><span class="p">,</span>
        <span class="s1">&#39;validation_size&#39;</span><span class="p">:</span> <span class="s1">&#39;0.2&#39;</span>
    <span class="p">},</span>
    <span class="s1">&#39;test_info&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="s1">&#39;None&#39;</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[28]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">test_evaluation</span><span class="p">,</span> <span class="n">train_evaluation</span><span class="p">,</span> <span class="n">network</span> <span class="o">=</span> <span class="n">deepbiome</span><span class="o">.</span><span class="n">deepbiome_train</span><span class="p">(</span><span class="n">log</span><span class="p">,</span> <span class="n">network_info</span><span class="p">,</span> <span class="n">path_info</span><span class="p">,</span> <span class="n">number_of_fold</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|deepbiome.py:100] -----------------------------------------------------------------
[root    |INFO|deepbiome.py:137] -------1 simulation start!----------------------------------
[root    |INFO|readers.py:58] -----------------------------------------------------------------------
[root    |INFO|readers.py:59] Construct Dataset
[root    |INFO|readers.py:60] -----------------------------------------------------------------------
[root    |INFO|readers.py:61] Load data
[root    |INFO|deepbiome.py:147] -----------------------------------------------------------------
[root    |INFO|deepbiome.py:148] Build network for 1 simulation
[root    |INFO|build_network.py:513] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:514] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv
[root    |INFO|build_network.py:518] Phylogenetic tree level list: [&#39;Genus&#39;, &#39;Family&#39;, &#39;Order&#39;, &#39;Class&#39;, &#39;Phylum&#39;]
[root    |INFO|build_network.py:519] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:526]      Genus: 48
[root    |INFO|build_network.py:526]     Family: 40
[root    |INFO|build_network.py:526]      Order: 23
[root    |INFO|build_network.py:526]      Class: 17
[root    |INFO|build_network.py:526]     Phylum: 9
[root    |INFO|build_network.py:529] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:530] Phylogenetic_tree_dict info: [&#39;Genus&#39;, &#39;Number&#39;, &#39;Order&#39;, &#39;Class&#39;, &#39;Phylum&#39;, &#39;Family&#39;]
[root    |INFO|build_network.py:531] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:541] Build edge weights between [ Genus, Family]
[root    |INFO|build_network.py:541] Build edge weights between [Family,  Order]
[root    |INFO|build_network.py:541] Build edge weights between [ Order,  Class]
[root    |INFO|build_network.py:541] Build edge weights between [ Class, Phylum]
[root    |INFO|build_network.py:554] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:570] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:571] Build network based on phylogenetic tree information
[root    |INFO|build_network.py:572] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:648] ------------------------------------------------------------------------------------------
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Model: &#34;model_1&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input (InputLayer)           (None, 48)                0
_________________________________________________________________
l1_dense (Dense_with_tree)   (None, 40)                1960
_________________________________________________________________
l1_activation (Activation)   (None, 40)                0
_________________________________________________________________
l2_dense (Dense_with_tree)   (None, 23)                943
_________________________________________________________________
l2_activation (Activation)   (None, 23)                0
_________________________________________________________________
l3_dense (Dense_with_tree)   (None, 17)                408
_________________________________________________________________
l3_activation (Activation)   (None, 17)                0
_________________________________________________________________
l4_dense (Dense_with_tree)   (None, 9)                 162
_________________________________________________________________
l4_activation (Activation)   (None, 9)                 0
_________________________________________________________________
last_dense_h (Dense)         (None, 1)                 10
_________________________________________________________________
p_hat (Activation)           (None, 1)                 0
=================================================================
Total params: 3,483
Trainable params: 3,483
Non-trainable params: 0
_________________________________________________________________
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:57] Build Network
[root    |INFO|build_network.py:58] Optimizer = adam
[root    |INFO|build_network.py:59] Loss = binary_crossentropy
[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc
[root    |INFO|build_network.py:83] Load trained model weight at ./example_result/weight_0.h5
[root    |INFO|deepbiome.py:157] -----------------------------------------------------------------
[root    |INFO|deepbiome.py:158] 1 fold computing start!----------------------------------
[root    |INFO|build_network.py:141] Training start!
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Train on 600 samples, validate on 150 samples
Epoch 1/100
600/600 [==============================] - 1s 890us/step - loss: 0.2947 - binary_accuracy: 0.8783 - sensitivity: 0.9535 - specificity: 0.6973 - gmeasure: 0.8042 - auc: 0.9525 - val_loss: 0.3781 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.7905 - val_specificity: 0.9556 - val_gmeasure: 0.8691 - val_auc: 0.9179
Epoch 2/100
600/600 [==============================] - 0s 64us/step - loss: 0.2898 - binary_accuracy: 0.8800 - sensitivity: 0.8689 - specificity: 0.9034 - gmeasure: 0.8838 - auc: 0.9554 - val_loss: 0.3385 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.9333 - val_specificity: 0.6000 - val_gmeasure: 0.7483 - val_auc: 0.9189
Epoch 3/100
600/600 [==============================] - 0s 67us/step - loss: 0.2723 - binary_accuracy: 0.8800 - sensitivity: 0.9597 - specificity: 0.7100 - gmeasure: 0.8240 - auc: 0.9572 - val_loss: 0.3262 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8667 - val_specificity: 0.8444 - val_gmeasure: 0.8555 - val_auc: 0.9183
Epoch 4/100
600/600 [==============================] - 0s 63us/step - loss: 0.2589 - binary_accuracy: 0.8900 - sensitivity: 0.9235 - specificity: 0.8214 - gmeasure: 0.8695 - auc: 0.9561 - val_loss: 0.3290 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8571 - val_specificity: 0.8667 - val_gmeasure: 0.8619 - val_auc: 0.9192
Epoch 5/100
600/600 [==============================] - 0s 63us/step - loss: 0.2574 - binary_accuracy: 0.8917 - sensitivity: 0.9194 - specificity: 0.8275 - gmeasure: 0.8720 - auc: 0.9556 - val_loss: 0.3257 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.8667 - val_specificity: 0.8000 - val_gmeasure: 0.8327 - val_auc: 0.9192
Epoch 6/100
600/600 [==============================] - 0s 64us/step - loss: 0.2590 - binary_accuracy: 0.8900 - sensitivity: 0.9396 - specificity: 0.7837 - gmeasure: 0.8578 - auc: 0.9567 - val_loss: 0.3251 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.8667 - val_specificity: 0.8222 - val_gmeasure: 0.8442 - val_auc: 0.9200
Epoch 7/100
600/600 [==============================] - 0s 58us/step - loss: 0.2620 - binary_accuracy: 0.8933 - sensitivity: 0.9143 - specificity: 0.8529 - gmeasure: 0.8825 - auc: 0.9595 - val_loss: 0.3245 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8667 - val_specificity: 0.8667 - val_gmeasure: 0.8667 - val_auc: 0.9208
Epoch 8/100
600/600 [==============================] - 0s 58us/step - loss: 0.2560 - binary_accuracy: 0.8850 - sensitivity: 0.9344 - specificity: 0.7757 - gmeasure: 0.8506 - auc: 0.9565 - val_loss: 0.3236 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.8952 - val_specificity: 0.7333 - val_gmeasure: 0.8103 - val_auc: 0.9208
Epoch 9/100
600/600 [==============================] - 0s 61us/step - loss: 0.2623 - binary_accuracy: 0.8900 - sensitivity: 0.9107 - specificity: 0.8452 - gmeasure: 0.8756 - auc: 0.9585 - val_loss: 0.3237 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8571 - val_specificity: 0.8667 - val_gmeasure: 0.8619 - val_auc: 0.9219
Epoch 10/100
600/600 [==============================] - 0s 59us/step - loss: 0.2645 - binary_accuracy: 0.8967 - sensitivity: 0.9440 - specificity: 0.8021 - gmeasure: 0.8693 - auc: 0.9581 - val_loss: 0.3224 - val_binary_accuracy: 0.8467 - val_sensitivity: 0.8952 - val_specificity: 0.7333 - val_gmeasure: 0.8103 - val_auc: 0.9221
Epoch 11/100
600/600 [==============================] - 0s 68us/step - loss: 0.2553 - binary_accuracy: 0.8900 - sensitivity: 0.9188 - specificity: 0.8281 - gmeasure: 0.8722 - auc: 0.9564 - val_loss: 0.3275 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8571 - val_specificity: 0.8889 - val_gmeasure: 0.8729 - val_auc: 0.9232
Epoch 12/100
600/600 [==============================] - 0s 59us/step - loss: 0.2537 - binary_accuracy: 0.8950 - sensitivity: 0.9304 - specificity: 0.8190 - gmeasure: 0.8728 - auc: 0.9574 - val_loss: 0.3235 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.9143 - val_specificity: 0.7111 - val_gmeasure: 0.8063 - val_auc: 0.9219
Epoch 13/100
600/600 [==============================] - 0s 61us/step - loss: 0.2554 - binary_accuracy: 0.8867 - sensitivity: 0.9339 - specificity: 0.7710 - gmeasure: 0.8455 - auc: 0.9575 - val_loss: 0.3226 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8571 - val_specificity: 0.8667 - val_gmeasure: 0.8619 - val_auc: 0.9234
Epoch 14/100
600/600 [==============================] - 0s 65us/step - loss: 0.2489 - binary_accuracy: 0.8983 - sensitivity: 0.9184 - specificity: 0.8593 - gmeasure: 0.8879 - auc: 0.9619 - val_loss: 0.3192 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.8857 - val_specificity: 0.7778 - val_gmeasure: 0.8300 - val_auc: 0.9223
Epoch 15/100
600/600 [==============================] - 0s 69us/step - loss: 0.2565 - binary_accuracy: 0.8817 - sensitivity: 0.9493 - specificity: 0.7331 - gmeasure: 0.8341 - auc: 0.9579 - val_loss: 0.3177 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.8667 - val_specificity: 0.8222 - val_gmeasure: 0.8442 - val_auc: 0.9228
Epoch 16/100
600/600 [==============================] - 0s 58us/step - loss: 0.2518 - binary_accuracy: 0.8850 - sensitivity: 0.9018 - specificity: 0.8424 - gmeasure: 0.8706 - auc: 0.9602 - val_loss: 0.3224 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8571 - val_specificity: 0.8889 - val_gmeasure: 0.8729 - val_auc: 0.9236
Epoch 17/100
600/600 [==============================] - 0s 61us/step - loss: 0.2477 - binary_accuracy: 0.8950 - sensitivity: 0.9366 - specificity: 0.8002 - gmeasure: 0.8644 - auc: 0.9587 - val_loss: 0.3201 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.9143 - val_specificity: 0.7556 - val_gmeasure: 0.8311 - val_auc: 0.9228
Epoch 18/100
600/600 [==============================] - 0s 61us/step - loss: 0.2439 - binary_accuracy: 0.9000 - sensitivity: 0.9495 - specificity: 0.7890 - gmeasure: 0.8655 - auc: 0.9621 - val_loss: 0.3266 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8571 - val_specificity: 0.8889 - val_gmeasure: 0.8729 - val_auc: 0.9244
Epoch 19/100
600/600 [==============================] - 0s 61us/step - loss: 0.2499 - binary_accuracy: 0.8950 - sensitivity: 0.9055 - specificity: 0.8746 - gmeasure: 0.8896 - auc: 0.9608 - val_loss: 0.3169 - val_binary_accuracy: 0.8533 - val_sensitivity: 0.8857 - val_specificity: 0.7778 - val_gmeasure: 0.8300 - val_auc: 0.9238
Epoch 20/100
600/600 [==============================] - 0s 61us/step - loss: 0.2530 - binary_accuracy: 0.8850 - sensitivity: 0.9520 - specificity: 0.7410 - gmeasure: 0.8384 - auc: 0.9618 - val_loss: 0.3162 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8667 - val_specificity: 0.8444 - val_gmeasure: 0.8555 - val_auc: 0.9242
Epoch 21/100
600/600 [==============================] - 0s 61us/step - loss: 0.2446 - binary_accuracy: 0.8983 - sensitivity: 0.9128 - specificity: 0.8665 - gmeasure: 0.8893 - auc: 0.9619 - val_loss: 0.3169 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8571 - val_specificity: 0.8667 - val_gmeasure: 0.8619 - val_auc: 0.9255
Epoch 22/100
600/600 [==============================] - 0s 60us/step - loss: 0.2419 - binary_accuracy: 0.8917 - sensitivity: 0.9348 - specificity: 0.7949 - gmeasure: 0.8616 - auc: 0.9610 - val_loss: 0.3161 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.9048 - val_specificity: 0.7556 - val_gmeasure: 0.8268 - val_auc: 0.9247
Epoch 23/100
600/600 [==============================] - 0s 62us/step - loss: 0.2427 - binary_accuracy: 0.8983 - sensitivity: 0.9274 - specificity: 0.8346 - gmeasure: 0.8789 - auc: 0.9624 - val_loss: 0.3185 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8571 - val_specificity: 0.8889 - val_gmeasure: 0.8729 - val_auc: 0.9259
Epoch 24/100
600/600 [==============================] - 0s 64us/step - loss: 0.2368 - binary_accuracy: 0.9100 - sensitivity: 0.9375 - specificity: 0.8442 - gmeasure: 0.8893 - auc: 0.9609 - val_loss: 0.3163 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.9048 - val_specificity: 0.7556 - val_gmeasure: 0.8268 - val_auc: 0.9253
Epoch 25/100
600/600 [==============================] - 0s 67us/step - loss: 0.2376 - binary_accuracy: 0.9083 - sensitivity: 0.9490 - specificity: 0.8152 - gmeasure: 0.8789 - auc: 0.9631 - val_loss: 0.3156 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8571 - val_specificity: 0.8667 - val_gmeasure: 0.8619 - val_auc: 0.9263
Epoch 26/100
600/600 [==============================] - 0s 63us/step - loss: 0.2352 - binary_accuracy: 0.9033 - sensitivity: 0.9322 - specificity: 0.8394 - gmeasure: 0.8839 - auc: 0.9641 - val_loss: 0.3120 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8857 - val_specificity: 0.8444 - val_gmeasure: 0.8648 - val_auc: 0.9266
Epoch 27/100
600/600 [==============================] - 0s 62us/step - loss: 0.2328 - binary_accuracy: 0.9083 - sensitivity: 0.9308 - specificity: 0.8506 - gmeasure: 0.8891 - auc: 0.9626 - val_loss: 0.3119 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8762 - val_specificity: 0.8667 - val_gmeasure: 0.8714 - val_auc: 0.9266
Epoch 28/100
600/600 [==============================] - 0s 65us/step - loss: 0.2346 - binary_accuracy: 0.9017 - sensitivity: 0.9468 - specificity: 0.8001 - gmeasure: 0.8695 - auc: 0.9637 - val_loss: 0.3129 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8667 - val_specificity: 0.8667 - val_gmeasure: 0.8667 - val_auc: 0.9266
Epoch 29/100
600/600 [==============================] - 0s 64us/step - loss: 0.2322 - binary_accuracy: 0.9050 - sensitivity: 0.9226 - specificity: 0.8670 - gmeasure: 0.8943 - auc: 0.9640 - val_loss: 0.3117 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8857 - val_specificity: 0.8667 - val_gmeasure: 0.8761 - val_auc: 0.9263
Epoch 30/100
600/600 [==============================] - 0s 61us/step - loss: 0.2296 - binary_accuracy: 0.9067 - sensitivity: 0.9445 - specificity: 0.8228 - gmeasure: 0.8814 - auc: 0.9640 - val_loss: 0.3102 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8857 - val_specificity: 0.8667 - val_gmeasure: 0.8761 - val_auc: 0.9263
Epoch 31/100
600/600 [==============================] - 0s 74us/step - loss: 0.2278 - binary_accuracy: 0.9117 - sensitivity: 0.9417 - specificity: 0.8456 - gmeasure: 0.8921 - auc: 0.9647 - val_loss: 0.3104 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8762 - val_specificity: 0.8667 - val_gmeasure: 0.8714 - val_auc: 0.9266
Epoch 32/100
600/600 [==============================] - 0s 78us/step - loss: 0.2300 - binary_accuracy: 0.9100 - sensitivity: 0.9439 - specificity: 0.8362 - gmeasure: 0.8883 - auc: 0.9642 - val_loss: 0.3085 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8857 - val_specificity: 0.8667 - val_gmeasure: 0.8761 - val_auc: 0.9276
Epoch 33/100
600/600 [==============================] - 0s 76us/step - loss: 0.2310 - binary_accuracy: 0.9100 - sensitivity: 0.9275 - specificity: 0.8715 - gmeasure: 0.8986 - auc: 0.9675 - val_loss: 0.3081 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8857 - val_specificity: 0.8667 - val_gmeasure: 0.8761 - val_auc: 0.9276
Epoch 34/100
600/600 [==============================] - 0s 66us/step - loss: 0.2280 - binary_accuracy: 0.9000 - sensitivity: 0.9447 - specificity: 0.8051 - gmeasure: 0.8717 - auc: 0.9659 - val_loss: 0.3089 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8762 - val_specificity: 0.8667 - val_gmeasure: 0.8714 - val_auc: 0.9278
Epoch 35/100
600/600 [==============================] - 0s 66us/step - loss: 0.2268 - binary_accuracy: 0.9083 - sensitivity: 0.9278 - specificity: 0.8672 - gmeasure: 0.8967 - auc: 0.9672 - val_loss: 0.3111 - val_binary_accuracy: 0.8600 - val_sensitivity: 0.8952 - val_specificity: 0.7778 - val_gmeasure: 0.8344 - val_auc: 0.9272
Epoch 36/100
600/600 [==============================] - 0s 68us/step - loss: 0.2341 - binary_accuracy: 0.8917 - sensitivity: 0.9504 - specificity: 0.7715 - gmeasure: 0.8546 - auc: 0.9646 - val_loss: 0.3250 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8476 - val_specificity: 0.9333 - val_gmeasure: 0.8894 - val_auc: 0.9285
Epoch 37/100
600/600 [==============================] - 0s 68us/step - loss: 0.2307 - binary_accuracy: 0.9067 - sensitivity: 0.9028 - specificity: 0.9139 - gmeasure: 0.9081 - auc: 0.9688 - val_loss: 0.3207 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.9333 - val_specificity: 0.7111 - val_gmeasure: 0.8147 - val_auc: 0.9276
Epoch 38/100
600/600 [==============================] - 0s 60us/step - loss: 0.2348 - binary_accuracy: 0.8933 - sensitivity: 0.9591 - specificity: 0.7493 - gmeasure: 0.8462 - auc: 0.9669 - val_loss: 0.3156 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8571 - val_specificity: 0.9111 - val_gmeasure: 0.8837 - val_auc: 0.9287
Epoch 39/100
600/600 [==============================] - 0s 67us/step - loss: 0.2244 - binary_accuracy: 0.9083 - sensitivity: 0.9179 - specificity: 0.8888 - gmeasure: 0.9030 - auc: 0.9689 - val_loss: 0.3058 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8857 - val_specificity: 0.8667 - val_gmeasure: 0.8761 - val_auc: 0.9287
Epoch 40/100
600/600 [==============================] - 0s 64us/step - loss: 0.2229 - binary_accuracy: 0.9067 - sensitivity: 0.9527 - specificity: 0.8005 - gmeasure: 0.8730 - auc: 0.9667 - val_loss: 0.3087 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8667 - val_specificity: 0.8667 - val_gmeasure: 0.8667 - val_auc: 0.9285
Epoch 41/100
600/600 [==============================] - 0s 64us/step - loss: 0.2188 - binary_accuracy: 0.9083 - sensitivity: 0.9267 - specificity: 0.8663 - gmeasure: 0.8958 - auc: 0.9692 - val_loss: 0.3043 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8857 - val_specificity: 0.8667 - val_gmeasure: 0.8761 - val_auc: 0.9297
Epoch 42/100
600/600 [==============================] - 0s 64us/step - loss: 0.2208 - binary_accuracy: 0.9117 - sensitivity: 0.9428 - specificity: 0.8553 - gmeasure: 0.8968 - auc: 0.9703 - val_loss: 0.3057 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8857 - val_specificity: 0.8667 - val_gmeasure: 0.8761 - val_auc: 0.9293
Epoch 43/100
600/600 [==============================] - 0s 63us/step - loss: 0.2170 - binary_accuracy: 0.9083 - sensitivity: 0.9517 - specificity: 0.8134 - gmeasure: 0.8794 - auc: 0.9698 - val_loss: 0.3051 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8667 - val_specificity: 0.8667 - val_gmeasure: 0.8667 - val_auc: 0.9299
Epoch 44/100
600/600 [==============================] - 0s 68us/step - loss: 0.2167 - binary_accuracy: 0.9183 - sensitivity: 0.9418 - specificity: 0.8734 - gmeasure: 0.9065 - auc: 0.9716 - val_loss: 0.3045 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9048 - val_specificity: 0.8444 - val_gmeasure: 0.8741 - val_auc: 0.9310
Epoch 45/100
600/600 [==============================] - 0s 68us/step - loss: 0.2139 - binary_accuracy: 0.9150 - sensitivity: 0.9568 - specificity: 0.8269 - gmeasure: 0.8889 - auc: 0.9701 - val_loss: 0.3047 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8667 - val_specificity: 0.8667 - val_gmeasure: 0.8667 - val_auc: 0.9306
Epoch 46/100
600/600 [==============================] - 0s 69us/step - loss: 0.2158 - binary_accuracy: 0.9133 - sensitivity: 0.9203 - specificity: 0.8995 - gmeasure: 0.9095 - auc: 0.9700 - val_loss: 0.3051 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9048 - val_specificity: 0.8444 - val_gmeasure: 0.8741 - val_auc: 0.9310
Epoch 47/100
600/600 [==============================] - 0s 65us/step - loss: 0.2169 - binary_accuracy: 0.9050 - sensitivity: 0.9615 - specificity: 0.7785 - gmeasure: 0.8648 - auc: 0.9703 - val_loss: 0.3052 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8667 - val_specificity: 0.8667 - val_gmeasure: 0.8667 - val_auc: 0.9314
Epoch 48/100
600/600 [==============================] - 0s 63us/step - loss: 0.2262 - binary_accuracy: 0.9083 - sensitivity: 0.9093 - specificity: 0.9129 - gmeasure: 0.9104 - auc: 0.9717 - val_loss: 0.3140 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.9238 - val_specificity: 0.7333 - val_gmeasure: 0.8231 - val_auc: 0.9302
Epoch 49/100
600/600 [==============================] - 0s 65us/step - loss: 0.2235 - binary_accuracy: 0.9000 - sensitivity: 0.9657 - specificity: 0.7514 - gmeasure: 0.8516 - auc: 0.9708 - val_loss: 0.3068 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8667 - val_specificity: 0.8889 - val_gmeasure: 0.8777 - val_auc: 0.9331
Epoch 50/100
600/600 [==============================] - 0s 63us/step - loss: 0.2266 - binary_accuracy: 0.9017 - sensitivity: 0.8961 - specificity: 0.9152 - gmeasure: 0.9049 - auc: 0.9719 - val_loss: 0.3039 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9048 - val_specificity: 0.8444 - val_gmeasure: 0.8741 - val_auc: 0.9316
Epoch 51/100
600/600 [==============================] - 0s 77us/step - loss: 0.2225 - binary_accuracy: 0.9033 - sensitivity: 0.9663 - specificity: 0.7647 - gmeasure: 0.8596 - auc: 0.9707 - val_loss: 0.3004 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8667 - val_specificity: 0.8667 - val_gmeasure: 0.8667 - val_auc: 0.9333
Epoch 52/100
600/600 [==============================] - 0s 69us/step - loss: 0.2177 - binary_accuracy: 0.9083 - sensitivity: 0.9085 - specificity: 0.9019 - gmeasure: 0.9044 - auc: 0.9737 - val_loss: 0.3005 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8952 - val_specificity: 0.8444 - val_gmeasure: 0.8695 - val_auc: 0.9316
Epoch 53/100
600/600 [==============================] - 0s 70us/step - loss: 0.2176 - binary_accuracy: 0.9083 - sensitivity: 0.9622 - specificity: 0.7960 - gmeasure: 0.8746 - auc: 0.9722 - val_loss: 0.3014 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8667 - val_specificity: 0.8889 - val_gmeasure: 0.8777 - val_auc: 0.9331
Epoch 54/100
600/600 [==============================] - 0s 77us/step - loss: 0.2125 - binary_accuracy: 0.9150 - sensitivity: 0.9127 - specificity: 0.9207 - gmeasure: 0.9161 - auc: 0.9742 - val_loss: 0.3005 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9048 - val_specificity: 0.8444 - val_gmeasure: 0.8741 - val_auc: 0.9323
Epoch 55/100
600/600 [==============================] - 0s 71us/step - loss: 0.2195 - binary_accuracy: 0.9000 - sensitivity: 0.9637 - specificity: 0.7587 - gmeasure: 0.8546 - auc: 0.9728 - val_loss: 0.2982 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8667 - val_specificity: 0.8889 - val_gmeasure: 0.8777 - val_auc: 0.9346
Epoch 56/100
600/600 [==============================] - 0s 70us/step - loss: 0.2248 - binary_accuracy: 0.9100 - sensitivity: 0.8980 - specificity: 0.9377 - gmeasure: 0.9174 - auc: 0.9733 - val_loss: 0.2980 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8952 - val_specificity: 0.8444 - val_gmeasure: 0.8695 - val_auc: 0.9321
Epoch 57/100
600/600 [==============================] - 0s 65us/step - loss: 0.2231 - binary_accuracy: 0.8967 - sensitivity: 0.9634 - specificity: 0.7436 - gmeasure: 0.8433 - auc: 0.9735 - val_loss: 0.2943 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8952 - val_specificity: 0.8667 - val_gmeasure: 0.8808 - val_auc: 0.9333
Epoch 58/100
600/600 [==============================] - 0s 61us/step - loss: 0.2052 - binary_accuracy: 0.9217 - sensitivity: 0.9222 - specificity: 0.9170 - gmeasure: 0.9192 - auc: 0.9747 - val_loss: 0.2943 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8857 - val_specificity: 0.8667 - val_gmeasure: 0.8761 - val_auc: 0.9333
Epoch 59/100
600/600 [==============================] - 0s 66us/step - loss: 0.2018 - binary_accuracy: 0.9150 - sensitivity: 0.9532 - specificity: 0.8242 - gmeasure: 0.8859 - auc: 0.9741 - val_loss: 0.2947 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8952 - val_specificity: 0.8667 - val_gmeasure: 0.8808 - val_auc: 0.9329
Epoch 60/100
600/600 [==============================] - 0s 66us/step - loss: 0.2093 - binary_accuracy: 0.9200 - sensitivity: 0.9300 - specificity: 0.8996 - gmeasure: 0.9136 - auc: 0.9748 - val_loss: 0.2952 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8952 - val_specificity: 0.8667 - val_gmeasure: 0.8808 - val_auc: 0.9344
Epoch 61/100
600/600 [==============================] - 0s 60us/step - loss: 0.2041 - binary_accuracy: 0.9083 - sensitivity: 0.9542 - specificity: 0.8092 - gmeasure: 0.8780 - auc: 0.9747 - val_loss: 0.2943 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8952 - val_specificity: 0.8667 - val_gmeasure: 0.8808 - val_auc: 0.9344
Epoch 62/100
600/600 [==============================] - 0s 63us/step - loss: 0.1964 - binary_accuracy: 0.9333 - sensitivity: 0.9472 - specificity: 0.9020 - gmeasure: 0.9243 - auc: 0.9740 - val_loss: 0.2927 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8952 - val_specificity: 0.8667 - val_gmeasure: 0.8808 - val_auc: 0.9344
Epoch 63/100
600/600 [==============================] - 0s 66us/step - loss: 0.1939 - binary_accuracy: 0.9267 - sensitivity: 0.9487 - specificity: 0.8759 - gmeasure: 0.9114 - auc: 0.9754 - val_loss: 0.2924 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8952 - val_specificity: 0.8667 - val_gmeasure: 0.8808 - val_auc: 0.9340
Epoch 64/100
600/600 [==============================] - 0s 64us/step - loss: 0.1948 - binary_accuracy: 0.9217 - sensitivity: 0.9538 - specificity: 0.8532 - gmeasure: 0.9019 - auc: 0.9754 - val_loss: 0.2923 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8762 - val_specificity: 0.8667 - val_gmeasure: 0.8714 - val_auc: 0.9354
Epoch 65/100
600/600 [==============================] - 0s 61us/step - loss: 0.1927 - binary_accuracy: 0.9300 - sensitivity: 0.9514 - specificity: 0.8828 - gmeasure: 0.9163 - auc: 0.9753 - val_loss: 0.2931 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8952 - val_specificity: 0.8667 - val_gmeasure: 0.8808 - val_auc: 0.9340
Epoch 66/100
600/600 [==============================] - 0s 63us/step - loss: 0.1941 - binary_accuracy: 0.9183 - sensitivity: 0.9475 - specificity: 0.8526 - gmeasure: 0.8983 - auc: 0.9766 - val_loss: 0.2902 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8952 - val_specificity: 0.8667 - val_gmeasure: 0.8808 - val_auc: 0.9350
Epoch 67/100
600/600 [==============================] - 0s 65us/step - loss: 0.1912 - binary_accuracy: 0.9250 - sensitivity: 0.9445 - specificity: 0.8854 - gmeasure: 0.9144 - auc: 0.9771 - val_loss: 0.2889 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8952 - val_specificity: 0.8667 - val_gmeasure: 0.8808 - val_auc: 0.9352
Epoch 68/100
600/600 [==============================] - 0s 66us/step - loss: 0.2157 - binary_accuracy: 0.9050 - sensitivity: 0.9650 - specificity: 0.7924 - gmeasure: 0.8693 - auc: 0.9790 - val_loss: 0.2970 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8667 - val_specificity: 0.9111 - val_gmeasure: 0.8886 - val_auc: 0.9382
Epoch 69/100
600/600 [==============================] - 0s 65us/step - loss: 0.2153 - binary_accuracy: 0.9167 - sensitivity: 0.8931 - specificity: 0.9671 - gmeasure: 0.9293 - auc: 0.9777 - val_loss: 0.2936 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9048 - val_specificity: 0.8444 - val_gmeasure: 0.8741 - val_auc: 0.9342
Epoch 70/100
600/600 [==============================] - 0s 70us/step - loss: 0.2031 - binary_accuracy: 0.9033 - sensitivity: 0.9688 - specificity: 0.7647 - gmeasure: 0.8592 - auc: 0.9761 - val_loss: 0.2877 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8857 - val_specificity: 0.8889 - val_gmeasure: 0.8873 - val_auc: 0.9376
Epoch 71/100
600/600 [==============================] - 0s 63us/step - loss: 0.1876 - binary_accuracy: 0.9317 - sensitivity: 0.9415 - specificity: 0.9099 - gmeasure: 0.9254 - auc: 0.9787 - val_loss: 0.2870 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8857 - val_specificity: 0.8889 - val_gmeasure: 0.8873 - val_auc: 0.9380
Epoch 72/100
600/600 [==============================] - 0s 64us/step - loss: 0.1861 - binary_accuracy: 0.9267 - sensitivity: 0.9491 - specificity: 0.8780 - gmeasure: 0.9127 - auc: 0.9779 - val_loss: 0.2885 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8952 - val_specificity: 0.8667 - val_gmeasure: 0.8808 - val_auc: 0.9365
Epoch 73/100
600/600 [==============================] - 0s 66us/step - loss: 0.1863 - binary_accuracy: 0.9217 - sensitivity: 0.9586 - specificity: 0.8415 - gmeasure: 0.8976 - auc: 0.9771 - val_loss: 0.2881 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8762 - val_specificity: 0.8889 - val_gmeasure: 0.8825 - val_auc: 0.9382
Epoch 74/100
600/600 [==============================] - 0s 65us/step - loss: 0.1883 - binary_accuracy: 0.9317 - sensitivity: 0.9371 - specificity: 0.9193 - gmeasure: 0.9280 - auc: 0.9793 - val_loss: 0.2910 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9048 - val_specificity: 0.8444 - val_gmeasure: 0.8741 - val_auc: 0.9352
Epoch 75/100
600/600 [==============================] - 0s 66us/step - loss: 0.1874 - binary_accuracy: 0.9200 - sensitivity: 0.9636 - specificity: 0.8235 - gmeasure: 0.8908 - auc: 0.9788 - val_loss: 0.2944 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8667 - val_specificity: 0.9111 - val_gmeasure: 0.8886 - val_auc: 0.9395
Epoch 76/100
600/600 [==============================] - 0s 65us/step - loss: 0.1898 - binary_accuracy: 0.9217 - sensitivity: 0.9193 - specificity: 0.9242 - gmeasure: 0.9213 - auc: 0.9793 - val_loss: 0.2913 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.9048 - val_specificity: 0.8222 - val_gmeasure: 0.8625 - val_auc: 0.9363
Epoch 77/100
600/600 [==============================] - 0s 68us/step - loss: 0.1972 - binary_accuracy: 0.9217 - sensitivity: 0.9592 - specificity: 0.8442 - gmeasure: 0.8975 - auc: 0.9798 - val_loss: 0.2886 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.8762 - val_specificity: 0.8889 - val_gmeasure: 0.8825 - val_auc: 0.9401
Epoch 78/100
600/600 [==============================] - 0s 68us/step - loss: 0.1866 - binary_accuracy: 0.9250 - sensitivity: 0.9440 - specificity: 0.8777 - gmeasure: 0.9085 - auc: 0.9786 - val_loss: 0.2887 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9048 - val_specificity: 0.8444 - val_gmeasure: 0.8741 - val_auc: 0.9357
Epoch 79/100
600/600 [==============================] - 0s 52us/step - loss: 0.1839 - binary_accuracy: 0.9283 - sensitivity: 0.9485 - specificity: 0.8857 - gmeasure: 0.9161 - auc: 0.9789 - val_loss: 0.2825 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8952 - val_specificity: 0.8667 - val_gmeasure: 0.8808 - val_auc: 0.9388
Epoch 80/100
600/600 [==============================] - 0s 57us/step - loss: 0.1791 - binary_accuracy: 0.9300 - sensitivity: 0.9566 - specificity: 0.8700 - gmeasure: 0.9120 - auc: 0.9798 - val_loss: 0.2821 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.9048 - val_specificity: 0.8667 - val_gmeasure: 0.8855 - val_auc: 0.9384
Epoch 81/100
600/600 [==============================] - 0s 65us/step - loss: 0.1774 - binary_accuracy: 0.9383 - sensitivity: 0.9546 - specificity: 0.9055 - gmeasure: 0.9293 - auc: 0.9805 - val_loss: 0.2819 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.9048 - val_specificity: 0.8667 - val_gmeasure: 0.8855 - val_auc: 0.9382
Epoch 82/100
600/600 [==============================] - 0s 58us/step - loss: 0.1793 - binary_accuracy: 0.9217 - sensitivity: 0.9595 - specificity: 0.8335 - gmeasure: 0.8934 - auc: 0.9798 - val_loss: 0.2829 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8857 - val_specificity: 0.8889 - val_gmeasure: 0.8873 - val_auc: 0.9399
Epoch 83/100
600/600 [==============================] - 0s 56us/step - loss: 0.1793 - binary_accuracy: 0.9350 - sensitivity: 0.9373 - specificity: 0.9292 - gmeasure: 0.9332 - auc: 0.9812 - val_loss: 0.2886 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9143 - val_specificity: 0.8222 - val_gmeasure: 0.8670 - val_auc: 0.9361
Epoch 84/100
600/600 [==============================] - 0s 68us/step - loss: 0.1833 - binary_accuracy: 0.9217 - sensitivity: 0.9541 - specificity: 0.8594 - gmeasure: 0.9043 - auc: 0.9808 - val_loss: 0.2826 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.8952 - val_specificity: 0.8889 - val_gmeasure: 0.8921 - val_auc: 0.9395
Epoch 85/100
600/600 [==============================] - 0s 68us/step - loss: 0.1753 - binary_accuracy: 0.9400 - sensitivity: 0.9539 - specificity: 0.9088 - gmeasure: 0.9309 - auc: 0.9811 - val_loss: 0.2840 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9143 - val_specificity: 0.8222 - val_gmeasure: 0.8670 - val_auc: 0.9376
Epoch 86/100
600/600 [==============================] - 0s 67us/step - loss: 0.1738 - binary_accuracy: 0.9267 - sensitivity: 0.9666 - specificity: 0.8416 - gmeasure: 0.9014 - auc: 0.9830 - val_loss: 0.3002 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8571 - val_specificity: 0.9556 - val_gmeasure: 0.9050 - val_auc: 0.9418
Epoch 87/100
600/600 [==============================] - 0s 66us/step - loss: 0.1915 - binary_accuracy: 0.9250 - sensitivity: 0.9209 - specificity: 0.9383 - gmeasure: 0.9286 - auc: 0.9827 - val_loss: 0.3095 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.9429 - val_specificity: 0.7778 - val_gmeasure: 0.8563 - val_auc: 0.9363
Epoch 88/100
600/600 [==============================] - 0s 68us/step - loss: 0.1848 - binary_accuracy: 0.9150 - sensitivity: 0.9685 - specificity: 0.7962 - gmeasure: 0.8772 - auc: 0.9814 - val_loss: 0.2971 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.8571 - val_specificity: 0.9556 - val_gmeasure: 0.9050 - val_auc: 0.9422
Epoch 89/100
600/600 [==============================] - 0s 69us/step - loss: 0.1954 - binary_accuracy: 0.9217 - sensitivity: 0.9193 - specificity: 0.9194 - gmeasure: 0.9172 - auc: 0.9810 - val_loss: 0.3221 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.9429 - val_specificity: 0.7111 - val_gmeasure: 0.8188 - val_auc: 0.9344
Epoch 90/100
600/600 [==============================] - 0s 67us/step - loss: 0.1900 - binary_accuracy: 0.9183 - sensitivity: 0.9669 - specificity: 0.8154 - gmeasure: 0.8864 - auc: 0.9807 - val_loss: 0.3091 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8286 - val_specificity: 0.9556 - val_gmeasure: 0.8898 - val_auc: 0.9439
Epoch 91/100
600/600 [==============================] - 0s 66us/step - loss: 0.1917 - binary_accuracy: 0.9250 - sensitivity: 0.9176 - specificity: 0.9404 - gmeasure: 0.9280 - auc: 0.9826 - val_loss: 0.3204 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.9429 - val_specificity: 0.7333 - val_gmeasure: 0.8315 - val_auc: 0.9365
Epoch 92/100
600/600 [==============================] - 0s 66us/step - loss: 0.1938 - binary_accuracy: 0.9167 - sensitivity: 0.9686 - specificity: 0.8049 - gmeasure: 0.8826 - auc: 0.9812 - val_loss: 0.3101 - val_binary_accuracy: 0.8667 - val_sensitivity: 0.8286 - val_specificity: 0.9556 - val_gmeasure: 0.8898 - val_auc: 0.9429
Epoch 93/100
600/600 [==============================] - 0s 69us/step - loss: 0.1966 - binary_accuracy: 0.9133 - sensitivity: 0.9099 - specificity: 0.9292 - gmeasure: 0.9185 - auc: 0.9833 - val_loss: 0.3116 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9333 - val_specificity: 0.7778 - val_gmeasure: 0.8520 - val_auc: 0.9357
Epoch 94/100
600/600 [==============================] - 0s 67us/step - loss: 0.1826 - binary_accuracy: 0.9233 - sensitivity: 0.9710 - specificity: 0.8196 - gmeasure: 0.8911 - auc: 0.9812 - val_loss: 0.2926 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8476 - val_specificity: 0.9333 - val_gmeasure: 0.8894 - val_auc: 0.9418
Epoch 95/100
600/600 [==============================] - 0s 67us/step - loss: 0.1815 - binary_accuracy: 0.9283 - sensitivity: 0.9128 - specificity: 0.9629 - gmeasure: 0.9374 - auc: 0.9830 - val_loss: 0.2984 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9238 - val_specificity: 0.8000 - val_gmeasure: 0.8597 - val_auc: 0.9369
Epoch 96/100
600/600 [==============================] - 0s 65us/step - loss: 0.1816 - binary_accuracy: 0.9183 - sensitivity: 0.9709 - specificity: 0.8024 - gmeasure: 0.8825 - auc: 0.9821 - val_loss: 0.2853 - val_binary_accuracy: 0.8733 - val_sensitivity: 0.8571 - val_specificity: 0.9111 - val_gmeasure: 0.8837 - val_auc: 0.9422
Epoch 97/100
600/600 [==============================] - 0s 65us/step - loss: 0.1889 - binary_accuracy: 0.9217 - sensitivity: 0.9060 - specificity: 0.9562 - gmeasure: 0.9305 - auc: 0.9843 - val_loss: 0.2935 - val_binary_accuracy: 0.8867 - val_sensitivity: 0.9238 - val_specificity: 0.8000 - val_gmeasure: 0.8597 - val_auc: 0.9386
Epoch 98/100
600/600 [==============================] - 0s 67us/step - loss: 0.1956 - binary_accuracy: 0.9117 - sensitivity: 0.9755 - specificity: 0.7681 - gmeasure: 0.8652 - auc: 0.9822 - val_loss: 0.2737 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.8952 - val_specificity: 0.8889 - val_gmeasure: 0.8921 - val_auc: 0.9424
Epoch 99/100
600/600 [==============================] - 0s 67us/step - loss: 0.1880 - binary_accuracy: 0.9300 - sensitivity: 0.9167 - specificity: 0.9642 - gmeasure: 0.9396 - auc: 0.9839 - val_loss: 0.2804 - val_binary_accuracy: 0.8800 - val_sensitivity: 0.9048 - val_specificity: 0.8222 - val_gmeasure: 0.8625 - val_auc: 0.9390
Epoch 100/100
600/600 [==============================] - 0s 66us/step - loss: 0.1839 - binary_accuracy: 0.9200 - sensitivity: 0.9711 - specificity: 0.8083 - gmeasure: 0.8858 - auc: 0.9832 - val_loss: 0.2771 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.9048 - val_specificity: 0.8667 - val_gmeasure: 0.8855 - val_auc: 0.9407
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:151] Training end with time 6.148927211761475!
[root    |INFO|build_network.py:79] Saved trained model weight at ./example_result/weight_0.h5
[root    |DEBUG|deepbiome.py:166] Save weight at ./example_result/weight_0.h5
[root    |DEBUG|deepbiome.py:169] Save history at ./example_result/hist_0.json
[root    |INFO|build_network.py:177] Evaluation start!
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
750/750 [==============================] - 0s 8us/step
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:182] Evaluation end with time 0.014052629470825195!
[root    |INFO|build_network.py:183] Evaluation: [0.1827818602323532, 0.9306666851043701, 0.9517374634742737, 0.8836206793785095, 0.917046844959259, 0.9758936762809753]
[root    |INFO|build_network.py:177] Evaluation start!
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
250/250 [==============================] - 0s 22us/step
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:182] Evaluation end with time 0.013617753982543945!
[root    |INFO|build_network.py:183] Evaluation: [0.282605916261673, 0.8960000276565552, 0.9349112510681152, 0.8148148059844971, 0.872799813747406, 0.9505442380905151]
[root    |INFO|deepbiome.py:179] Compute time : 7.753520727157593
[root    |INFO|deepbiome.py:180] 1 fold computing end!---------------------------------------------
[root    |INFO|deepbiome.py:137] -------2 simulation start!----------------------------------
[root    |INFO|readers.py:58] -----------------------------------------------------------------------
[root    |INFO|readers.py:59] Construct Dataset
[root    |INFO|readers.py:60] -----------------------------------------------------------------------
[root    |INFO|readers.py:61] Load data
[root    |INFO|deepbiome.py:147] -----------------------------------------------------------------
[root    |INFO|deepbiome.py:148] Build network for 2 simulation
[root    |INFO|build_network.py:513] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:514] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv
[root    |INFO|build_network.py:518] Phylogenetic tree level list: [&#39;Genus&#39;, &#39;Family&#39;, &#39;Order&#39;, &#39;Class&#39;, &#39;Phylum&#39;]
[root    |INFO|build_network.py:519] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:526]      Genus: 48
[root    |INFO|build_network.py:526]     Family: 40
[root    |INFO|build_network.py:526]      Order: 23
[root    |INFO|build_network.py:526]      Class: 17
[root    |INFO|build_network.py:526]     Phylum: 9
[root    |INFO|build_network.py:529] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:530] Phylogenetic_tree_dict info: [&#39;Genus&#39;, &#39;Number&#39;, &#39;Order&#39;, &#39;Class&#39;, &#39;Phylum&#39;, &#39;Family&#39;]
[root    |INFO|build_network.py:531] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:541] Build edge weights between [ Genus, Family]
[root    |INFO|build_network.py:541] Build edge weights between [Family,  Order]
[root    |INFO|build_network.py:541] Build edge weights between [ Order,  Class]
[root    |INFO|build_network.py:541] Build edge weights between [ Class, Phylum]
[root    |INFO|build_network.py:554] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:570] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:571] Build network based on phylogenetic tree information
[root    |INFO|build_network.py:572] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:648] ------------------------------------------------------------------------------------------
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Model: &#34;model_1&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input (InputLayer)           (None, 48)                0
_________________________________________________________________
l1_dense (Dense_with_tree)   (None, 40)                1960
_________________________________________________________________
l1_activation (Activation)   (None, 40)                0
_________________________________________________________________
l2_dense (Dense_with_tree)   (None, 23)                943
_________________________________________________________________
l2_activation (Activation)   (None, 23)                0
_________________________________________________________________
l3_dense (Dense_with_tree)   (None, 17)                408
_________________________________________________________________
l3_activation (Activation)   (None, 17)                0
_________________________________________________________________
l4_dense (Dense_with_tree)   (None, 9)                 162
_________________________________________________________________
l4_activation (Activation)   (None, 9)                 0
_________________________________________________________________
last_dense_h (Dense)         (None, 1)                 10
_________________________________________________________________
p_hat (Activation)           (None, 1)                 0
=================================================================
Total params: 3,483
Trainable params: 3,483
Non-trainable params: 0
_________________________________________________________________
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:57] Build Network
[root    |INFO|build_network.py:58] Optimizer = adam
[root    |INFO|build_network.py:59] Loss = binary_crossentropy
[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc
[root    |INFO|build_network.py:83] Load trained model weight at ./example_result/weight_1.h5
[root    |INFO|deepbiome.py:157] -----------------------------------------------------------------
[root    |INFO|deepbiome.py:158] 2 fold computing start!----------------------------------
[root    |INFO|build_network.py:141] Training start!
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Train on 600 samples, validate on 150 samples
Epoch 1/100
600/600 [==============================] - 1s 850us/step - loss: 0.3123 - binary_accuracy: 0.8533 - sensitivity: 0.8351 - specificity: 0.8991 - gmeasure: 0.8661 - auc: 0.9036 - val_loss: 0.3839 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8532 - val_specificity: 0.7317 - val_gmeasure: 0.7901 - val_auc: 0.8867
Epoch 2/100
600/600 [==============================] - 0s 66us/step - loss: 0.3246 - binary_accuracy: 0.8283 - sensitivity: 0.8603 - specificity: 0.7430 - gmeasure: 0.7993 - auc: 0.9008 - val_loss: 0.3616 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8866
Epoch 3/100
600/600 [==============================] - 0s 69us/step - loss: 0.3121 - binary_accuracy: 0.8517 - sensitivity: 0.8337 - specificity: 0.9067 - gmeasure: 0.8679 - auc: 0.9088 - val_loss: 0.3452 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8165 - val_specificity: 0.8293 - val_gmeasure: 0.8229 - val_auc: 0.8834
Epoch 4/100
600/600 [==============================] - 0s 68us/step - loss: 0.3089 - binary_accuracy: 0.8483 - sensitivity: 0.8219 - specificity: 0.9186 - gmeasure: 0.8687 - auc: 0.9062 - val_loss: 0.3602 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8870
Epoch 5/100
600/600 [==============================] - 0s 72us/step - loss: 0.3020 - binary_accuracy: 0.8450 - sensitivity: 0.8533 - specificity: 0.8233 - gmeasure: 0.8382 - auc: 0.9048 - val_loss: 0.3728 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8440 - val_specificity: 0.7805 - val_gmeasure: 0.8116 - val_auc: 0.8904
Epoch 6/100
600/600 [==============================] - 0s 69us/step - loss: 0.3043 - binary_accuracy: 0.8383 - sensitivity: 0.8512 - specificity: 0.8044 - gmeasure: 0.8266 - auc: 0.9048 - val_loss: 0.3533 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8874
Epoch 7/100
600/600 [==============================] - 0s 65us/step - loss: 0.3009 - binary_accuracy: 0.8567 - sensitivity: 0.8399 - specificity: 0.9012 - gmeasure: 0.8700 - auc: 0.9067 - val_loss: 0.3480 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8825
Epoch 8/100
600/600 [==============================] - 0s 64us/step - loss: 0.2997 - binary_accuracy: 0.8583 - sensitivity: 0.8423 - specificity: 0.9011 - gmeasure: 0.8712 - auc: 0.9072 - val_loss: 0.3593 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8892
Epoch 9/100
600/600 [==============================] - 0s 65us/step - loss: 0.2995 - binary_accuracy: 0.8467 - sensitivity: 0.8493 - specificity: 0.8415 - gmeasure: 0.8446 - auc: 0.9059 - val_loss: 0.3611 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8879
Epoch 10/100
600/600 [==============================] - 0s 70us/step - loss: 0.2992 - binary_accuracy: 0.8517 - sensitivity: 0.8431 - specificity: 0.8797 - gmeasure: 0.8605 - auc: 0.9084 - val_loss: 0.3484 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8845
Epoch 11/100
600/600 [==============================] - 0s 67us/step - loss: 0.2971 - binary_accuracy: 0.8600 - sensitivity: 0.8397 - specificity: 0.9141 - gmeasure: 0.8760 - auc: 0.9115 - val_loss: 0.3563 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8864
Epoch 12/100
600/600 [==============================] - 0s 65us/step - loss: 0.2993 - binary_accuracy: 0.8517 - sensitivity: 0.8566 - specificity: 0.8450 - gmeasure: 0.8502 - auc: 0.9062 - val_loss: 0.3645 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8866
Epoch 13/100
600/600 [==============================] - 0s 63us/step - loss: 0.2935 - binary_accuracy: 0.8600 - sensitivity: 0.8539 - specificity: 0.8788 - gmeasure: 0.8661 - auc: 0.9076 - val_loss: 0.3493 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8836
Epoch 14/100
600/600 [==============================] - 0s 63us/step - loss: 0.2947 - binary_accuracy: 0.8617 - sensitivity: 0.8408 - specificity: 0.9154 - gmeasure: 0.8772 - auc: 0.9097 - val_loss: 0.3498 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8854
Epoch 15/100
600/600 [==============================] - 0s 68us/step - loss: 0.2924 - binary_accuracy: 0.8600 - sensitivity: 0.8466 - specificity: 0.8932 - gmeasure: 0.8695 - auc: 0.9102 - val_loss: 0.3607 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8904
Epoch 16/100
600/600 [==============================] - 0s 62us/step - loss: 0.2924 - binary_accuracy: 0.8550 - sensitivity: 0.8534 - specificity: 0.8590 - gmeasure: 0.8562 - auc: 0.9077 - val_loss: 0.3571 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8891
Epoch 17/100
600/600 [==============================] - 0s 66us/step - loss: 0.2912 - binary_accuracy: 0.8600 - sensitivity: 0.8443 - specificity: 0.9049 - gmeasure: 0.8739 - auc: 0.9095 - val_loss: 0.3502 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8257 - val_specificity: 0.7805 - val_gmeasure: 0.8028 - val_auc: 0.8854
Epoch 18/100
600/600 [==============================] - 0s 66us/step - loss: 0.2903 - binary_accuracy: 0.8633 - sensitivity: 0.8451 - specificity: 0.9152 - gmeasure: 0.8793 - auc: 0.9122 - val_loss: 0.3567 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8906
Epoch 19/100
600/600 [==============================] - 0s 58us/step - loss: 0.2899 - binary_accuracy: 0.8617 - sensitivity: 0.8538 - specificity: 0.8845 - gmeasure: 0.8689 - auc: 0.9087 - val_loss: 0.3598 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8889
Epoch 20/100
600/600 [==============================] - 0s 60us/step - loss: 0.2880 - binary_accuracy: 0.8600 - sensitivity: 0.8515 - specificity: 0.8806 - gmeasure: 0.8658 - auc: 0.9096 - val_loss: 0.3508 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8257 - val_specificity: 0.7805 - val_gmeasure: 0.8028 - val_auc: 0.8890
Epoch 21/100
600/600 [==============================] - 0s 65us/step - loss: 0.2882 - binary_accuracy: 0.8633 - sensitivity: 0.8493 - specificity: 0.9011 - gmeasure: 0.8748 - auc: 0.9112 - val_loss: 0.3569 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8906
Epoch 22/100
600/600 [==============================] - 0s 72us/step - loss: 0.2882 - binary_accuracy: 0.8600 - sensitivity: 0.8513 - specificity: 0.8828 - gmeasure: 0.8666 - auc: 0.9096 - val_loss: 0.3561 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8906
Epoch 23/100
600/600 [==============================] - 0s 65us/step - loss: 0.2862 - binary_accuracy: 0.8617 - sensitivity: 0.8396 - specificity: 0.9199 - gmeasure: 0.8787 - auc: 0.9126 - val_loss: 0.3488 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8257 - val_specificity: 0.7805 - val_gmeasure: 0.8028 - val_auc: 0.8890
Epoch 24/100
600/600 [==============================] - 0s 64us/step - loss: 0.2859 - binary_accuracy: 0.8650 - sensitivity: 0.8435 - specificity: 0.9216 - gmeasure: 0.8815 - auc: 0.9115 - val_loss: 0.3646 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8940
Epoch 25/100
600/600 [==============================] - 0s 65us/step - loss: 0.2872 - binary_accuracy: 0.8533 - sensitivity: 0.8533 - specificity: 0.8551 - gmeasure: 0.8541 - auc: 0.9073 - val_loss: 0.3546 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8871
Epoch 26/100
600/600 [==============================] - 0s 69us/step - loss: 0.2860 - binary_accuracy: 0.8617 - sensitivity: 0.8332 - specificity: 0.9406 - gmeasure: 0.8852 - auc: 0.9130 - val_loss: 0.3426 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8257 - val_specificity: 0.8049 - val_gmeasure: 0.8152 - val_auc: 0.8857
Epoch 27/100
600/600 [==============================] - 0s 69us/step - loss: 0.2839 - binary_accuracy: 0.8667 - sensitivity: 0.8422 - specificity: 0.9319 - gmeasure: 0.8859 - auc: 0.9142 - val_loss: 0.3687 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8440 - val_specificity: 0.7805 - val_gmeasure: 0.8116 - val_auc: 0.8925
Epoch 28/100
600/600 [==============================] - 0s 62us/step - loss: 0.2921 - binary_accuracy: 0.8483 - sensitivity: 0.8519 - specificity: 0.8372 - gmeasure: 0.8445 - auc: 0.9054 - val_loss: 0.3606 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8908
Epoch 29/100
600/600 [==============================] - 0s 66us/step - loss: 0.2818 - binary_accuracy: 0.8667 - sensitivity: 0.8374 - specificity: 0.9439 - gmeasure: 0.8890 - auc: 0.9131 - val_loss: 0.3405 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8257 - val_specificity: 0.8049 - val_gmeasure: 0.8152 - val_auc: 0.8877
Epoch 30/100
600/600 [==============================] - 0s 63us/step - loss: 0.2857 - binary_accuracy: 0.8583 - sensitivity: 0.8350 - specificity: 0.9194 - gmeasure: 0.8753 - auc: 0.9138 - val_loss: 0.3647 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8440 - val_specificity: 0.7805 - val_gmeasure: 0.8116 - val_auc: 0.8929
Epoch 31/100
600/600 [==============================] - 0s 66us/step - loss: 0.2823 - binary_accuracy: 0.8567 - sensitivity: 0.8540 - specificity: 0.8672 - gmeasure: 0.8603 - auc: 0.9109 - val_loss: 0.3580 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8862
Epoch 32/100
600/600 [==============================] - 0s 66us/step - loss: 0.2798 - binary_accuracy: 0.8667 - sensitivity: 0.8444 - specificity: 0.9274 - gmeasure: 0.8848 - auc: 0.9130 - val_loss: 0.3451 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8257 - val_specificity: 0.8049 - val_gmeasure: 0.8152 - val_auc: 0.8880
Epoch 33/100
600/600 [==============================] - 0s 59us/step - loss: 0.2835 - binary_accuracy: 0.8700 - sensitivity: 0.8464 - specificity: 0.9378 - gmeasure: 0.8907 - auc: 0.9108 - val_loss: 0.3606 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8440 - val_specificity: 0.7805 - val_gmeasure: 0.8116 - val_auc: 0.8898
Epoch 34/100
600/600 [==============================] - 0s 65us/step - loss: 0.2775 - binary_accuracy: 0.8667 - sensitivity: 0.8491 - specificity: 0.9115 - gmeasure: 0.8796 - auc: 0.9139 - val_loss: 0.3514 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8257 - val_specificity: 0.7805 - val_gmeasure: 0.8028 - val_auc: 0.8880
Epoch 35/100
600/600 [==============================] - 0s 64us/step - loss: 0.2781 - binary_accuracy: 0.8700 - sensitivity: 0.8463 - specificity: 0.9283 - gmeasure: 0.8858 - auc: 0.9150 - val_loss: 0.3570 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8257 - val_specificity: 0.7805 - val_gmeasure: 0.8028 - val_auc: 0.8896
Epoch 36/100
600/600 [==============================] - 0s 67us/step - loss: 0.2768 - binary_accuracy: 0.8617 - sensitivity: 0.8496 - specificity: 0.8957 - gmeasure: 0.8723 - auc: 0.9108 - val_loss: 0.3619 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8440 - val_specificity: 0.7805 - val_gmeasure: 0.8116 - val_auc: 0.8917
Epoch 37/100
600/600 [==============================] - 0s 67us/step - loss: 0.2757 - binary_accuracy: 0.8650 - sensitivity: 0.8490 - specificity: 0.9077 - gmeasure: 0.8778 - auc: 0.9125 - val_loss: 0.3488 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8257 - val_specificity: 0.7805 - val_gmeasure: 0.8028 - val_auc: 0.8885
Epoch 38/100
600/600 [==============================] - 0s 65us/step - loss: 0.2767 - binary_accuracy: 0.8683 - sensitivity: 0.8400 - specificity: 0.9442 - gmeasure: 0.8904 - auc: 0.9154 - val_loss: 0.3556 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8257 - val_specificity: 0.7805 - val_gmeasure: 0.8028 - val_auc: 0.8919
Epoch 39/100
600/600 [==============================] - 0s 70us/step - loss: 0.2745 - binary_accuracy: 0.8683 - sensitivity: 0.8482 - specificity: 0.9210 - gmeasure: 0.8838 - auc: 0.9115 - val_loss: 0.3596 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8440 - val_specificity: 0.7805 - val_gmeasure: 0.8116 - val_auc: 0.8902
Epoch 40/100
600/600 [==============================] - 0s 70us/step - loss: 0.2743 - binary_accuracy: 0.8683 - sensitivity: 0.8467 - specificity: 0.9255 - gmeasure: 0.8851 - auc: 0.9120 - val_loss: 0.3531 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8257 - val_specificity: 0.7805 - val_gmeasure: 0.8028 - val_auc: 0.8885
Epoch 41/100
600/600 [==============================] - 0s 72us/step - loss: 0.2732 - binary_accuracy: 0.8700 - sensitivity: 0.8467 - specificity: 0.9335 - gmeasure: 0.8889 - auc: 0.9138 - val_loss: 0.3591 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8905
Epoch 42/100
600/600 [==============================] - 0s 65us/step - loss: 0.2728 - binary_accuracy: 0.8683 - sensitivity: 0.8478 - specificity: 0.9183 - gmeasure: 0.8815 - auc: 0.9108 - val_loss: 0.3570 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8902
Epoch 43/100
600/600 [==============================] - 0s 68us/step - loss: 0.2720 - binary_accuracy: 0.8700 - sensitivity: 0.8484 - specificity: 0.9256 - gmeasure: 0.8861 - auc: 0.9121 - val_loss: 0.3534 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8917
Epoch 44/100
600/600 [==============================] - 0s 71us/step - loss: 0.2735 - binary_accuracy: 0.8733 - sensitivity: 0.8436 - specificity: 0.9471 - gmeasure: 0.8934 - auc: 0.9152 - val_loss: 0.3551 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8917
Epoch 45/100
600/600 [==============================] - 0s 74us/step - loss: 0.2743 - binary_accuracy: 0.8683 - sensitivity: 0.8520 - specificity: 0.9157 - gmeasure: 0.8828 - auc: 0.9109 - val_loss: 0.3611 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8440 - val_specificity: 0.7805 - val_gmeasure: 0.8116 - val_auc: 0.8900
Epoch 46/100
600/600 [==============================] - 0s 70us/step - loss: 0.2764 - binary_accuracy: 0.8717 - sensitivity: 0.8424 - specificity: 0.9523 - gmeasure: 0.8952 - auc: 0.9163 - val_loss: 0.3459 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8165 - val_specificity: 0.8049 - val_gmeasure: 0.8107 - val_auc: 0.8885
Epoch 47/100
600/600 [==============================] - 0s 70us/step - loss: 0.2715 - binary_accuracy: 0.8733 - sensitivity: 0.8490 - specificity: 0.9382 - gmeasure: 0.8924 - auc: 0.9123 - val_loss: 0.3696 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8440 - val_specificity: 0.7561 - val_gmeasure: 0.7989 - val_auc: 0.8902
Epoch 48/100
600/600 [==============================] - 0s 74us/step - loss: 0.2719 - binary_accuracy: 0.8667 - sensitivity: 0.8498 - specificity: 0.9143 - gmeasure: 0.8813 - auc: 0.9128 - val_loss: 0.3496 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8165 - val_specificity: 0.8049 - val_gmeasure: 0.8107 - val_auc: 0.8885
Epoch 49/100
600/600 [==============================] - 0s 69us/step - loss: 0.2693 - binary_accuracy: 0.8750 - sensitivity: 0.8490 - specificity: 0.9445 - gmeasure: 0.8953 - auc: 0.9136 - val_loss: 0.3598 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8902
Epoch 50/100
600/600 [==============================] - 0s 66us/step - loss: 0.2688 - binary_accuracy: 0.8750 - sensitivity: 0.8529 - specificity: 0.9400 - gmeasure: 0.8947 - auc: 0.9174 - val_loss: 0.3552 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8902
Epoch 51/100
600/600 [==============================] - 0s 66us/step - loss: 0.2674 - binary_accuracy: 0.8717 - sensitivity: 0.8490 - specificity: 0.9311 - gmeasure: 0.8889 - auc: 0.9130 - val_loss: 0.3571 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8924
Epoch 52/100
600/600 [==============================] - 0s 65us/step - loss: 0.2682 - binary_accuracy: 0.8750 - sensitivity: 0.8491 - specificity: 0.9546 - gmeasure: 0.8995 - auc: 0.9198 - val_loss: 0.3485 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8257 - val_specificity: 0.8049 - val_gmeasure: 0.8152 - val_auc: 0.8873
Epoch 53/100
600/600 [==============================] - 0s 66us/step - loss: 0.2703 - binary_accuracy: 0.8700 - sensitivity: 0.8510 - specificity: 0.9215 - gmeasure: 0.8853 - auc: 0.9140 - val_loss: 0.3650 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8349 - val_specificity: 0.7561 - val_gmeasure: 0.7945 - val_auc: 0.8909
Epoch 54/100
600/600 [==============================] - 0s 70us/step - loss: 0.2682 - binary_accuracy: 0.8750 - sensitivity: 0.8493 - specificity: 0.9460 - gmeasure: 0.8962 - auc: 0.9145 - val_loss: 0.3419 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8165 - val_specificity: 0.8537 - val_gmeasure: 0.8349 - val_auc: 0.8863
Epoch 55/100
600/600 [==============================] - 0s 65us/step - loss: 0.2674 - binary_accuracy: 0.8750 - sensitivity: 0.8446 - specificity: 0.9587 - gmeasure: 0.8997 - auc: 0.9169 - val_loss: 0.3677 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8440 - val_specificity: 0.7561 - val_gmeasure: 0.7989 - val_auc: 0.8911
Epoch 56/100
600/600 [==============================] - 0s 65us/step - loss: 0.2670 - binary_accuracy: 0.8733 - sensitivity: 0.8492 - specificity: 0.9365 - gmeasure: 0.8916 - auc: 0.9147 - val_loss: 0.3541 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8257 - val_specificity: 0.8049 - val_gmeasure: 0.8152 - val_auc: 0.8928
Epoch 57/100
600/600 [==============================] - 0s 61us/step - loss: 0.2675 - binary_accuracy: 0.8717 - sensitivity: 0.8514 - specificity: 0.9276 - gmeasure: 0.8884 - auc: 0.9133 - val_loss: 0.3544 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8257 - val_specificity: 0.8049 - val_gmeasure: 0.8152 - val_auc: 0.8928
Epoch 58/100
600/600 [==============================] - 0s 59us/step - loss: 0.2684 - binary_accuracy: 0.8767 - sensitivity: 0.8444 - specificity: 0.9642 - gmeasure: 0.9023 - auc: 0.9195 - val_loss: 0.3508 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8257 - val_specificity: 0.8293 - val_gmeasure: 0.8275 - val_auc: 0.8930
Epoch 59/100
600/600 [==============================] - 0s 69us/step - loss: 0.2624 - binary_accuracy: 0.8733 - sensitivity: 0.8513 - specificity: 0.9308 - gmeasure: 0.8900 - auc: 0.9150 - val_loss: 0.3729 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8532 - val_specificity: 0.7561 - val_gmeasure: 0.8032 - val_auc: 0.8928
Epoch 60/100
600/600 [==============================] - 0s 68us/step - loss: 0.2637 - binary_accuracy: 0.8750 - sensitivity: 0.8535 - specificity: 0.9282 - gmeasure: 0.8897 - auc: 0.9136 - val_loss: 0.3442 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8257 - val_specificity: 0.8537 - val_gmeasure: 0.8396 - val_auc: 0.8844
Epoch 61/100
600/600 [==============================] - 0s 62us/step - loss: 0.2644 - binary_accuracy: 0.8767 - sensitivity: 0.8438 - specificity: 0.9640 - gmeasure: 0.9019 - auc: 0.9195 - val_loss: 0.3658 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8349 - val_specificity: 0.7561 - val_gmeasure: 0.7945 - val_auc: 0.8930
Epoch 62/100
600/600 [==============================] - 0s 64us/step - loss: 0.2655 - binary_accuracy: 0.8683 - sensitivity: 0.8512 - specificity: 0.9142 - gmeasure: 0.8818 - auc: 0.9126 - val_loss: 0.3587 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8257 - val_specificity: 0.8049 - val_gmeasure: 0.8152 - val_auc: 0.8928
Epoch 63/100
600/600 [==============================] - 0s 62us/step - loss: 0.2645 - binary_accuracy: 0.8733 - sensitivity: 0.8432 - specificity: 0.9554 - gmeasure: 0.8975 - auc: 0.9194 - val_loss: 0.3491 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8257 - val_specificity: 0.8537 - val_gmeasure: 0.8396 - val_auc: 0.8893
Epoch 64/100
600/600 [==============================] - 0s 66us/step - loss: 0.2632 - binary_accuracy: 0.8767 - sensitivity: 0.8520 - specificity: 0.9444 - gmeasure: 0.8968 - auc: 0.9166 - val_loss: 0.3785 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8532 - val_specificity: 0.7561 - val_gmeasure: 0.8032 - val_auc: 0.8963
Epoch 65/100
600/600 [==============================] - 0s 64us/step - loss: 0.2665 - binary_accuracy: 0.8717 - sensitivity: 0.8462 - specificity: 0.9415 - gmeasure: 0.8925 - auc: 0.9136 - val_loss: 0.3436 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8257 - val_specificity: 0.8537 - val_gmeasure: 0.8396 - val_auc: 0.8843
Epoch 66/100
600/600 [==============================] - 0s 65us/step - loss: 0.2616 - binary_accuracy: 0.8800 - sensitivity: 0.8494 - specificity: 0.9607 - gmeasure: 0.9033 - auc: 0.9174 - val_loss: 0.3678 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8257 - val_specificity: 0.7805 - val_gmeasure: 0.8028 - val_auc: 0.8963
Epoch 67/100
600/600 [==============================] - 0s 60us/step - loss: 0.2615 - binary_accuracy: 0.8800 - sensitivity: 0.8537 - specificity: 0.9526 - gmeasure: 0.9017 - auc: 0.9170 - val_loss: 0.3608 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8257 - val_specificity: 0.7805 - val_gmeasure: 0.8028 - val_auc: 0.8947
Epoch 68/100
600/600 [==============================] - 0s 63us/step - loss: 0.2590 - binary_accuracy: 0.8767 - sensitivity: 0.8514 - specificity: 0.9452 - gmeasure: 0.8970 - auc: 0.9156 - val_loss: 0.3579 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8257 - val_specificity: 0.7805 - val_gmeasure: 0.8028 - val_auc: 0.8963
Epoch 69/100
600/600 [==============================] - 0s 69us/step - loss: 0.2585 - binary_accuracy: 0.8800 - sensitivity: 0.8514 - specificity: 0.9554 - gmeasure: 0.9017 - auc: 0.9181 - val_loss: 0.3586 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8257 - val_specificity: 0.7805 - val_gmeasure: 0.8028 - val_auc: 0.8963
Epoch 70/100
600/600 [==============================] - 0s 66us/step - loss: 0.2582 - binary_accuracy: 0.8817 - sensitivity: 0.8516 - specificity: 0.9635 - gmeasure: 0.9056 - auc: 0.9185 - val_loss: 0.3570 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8257 - val_specificity: 0.8049 - val_gmeasure: 0.8152 - val_auc: 0.8963
Epoch 71/100
600/600 [==============================] - 0s 63us/step - loss: 0.2578 - binary_accuracy: 0.8817 - sensitivity: 0.8519 - specificity: 0.9614 - gmeasure: 0.9050 - auc: 0.9178 - val_loss: 0.3627 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8257 - val_specificity: 0.7805 - val_gmeasure: 0.8028 - val_auc: 0.8965
Epoch 72/100
600/600 [==============================] - 0s 62us/step - loss: 0.2575 - binary_accuracy: 0.8783 - sensitivity: 0.8539 - specificity: 0.9449 - gmeasure: 0.8981 - auc: 0.9170 - val_loss: 0.3619 - val_binary_accuracy: 0.8133 - val_sensitivity: 0.8257 - val_specificity: 0.7805 - val_gmeasure: 0.8028 - val_auc: 0.8947
Epoch 73/100
600/600 [==============================] - 0s 55us/step - loss: 0.2580 - binary_accuracy: 0.8767 - sensitivity: 0.8474 - specificity: 0.9582 - gmeasure: 0.9008 - auc: 0.9190 - val_loss: 0.3589 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8257 - val_specificity: 0.8049 - val_gmeasure: 0.8152 - val_auc: 0.8965
Epoch 74/100
600/600 [==============================] - 0s 54us/step - loss: 0.2565 - binary_accuracy: 0.8833 - sensitivity: 0.8534 - specificity: 0.9630 - gmeasure: 0.9064 - auc: 0.9180 - val_loss: 0.3642 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8967
Epoch 75/100
600/600 [==============================] - 0s 62us/step - loss: 0.2583 - binary_accuracy: 0.8783 - sensitivity: 0.8539 - specificity: 0.9471 - gmeasure: 0.8988 - auc: 0.9170 - val_loss: 0.3544 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8257 - val_specificity: 0.8537 - val_gmeasure: 0.8396 - val_auc: 0.8898
Epoch 76/100
600/600 [==============================] - 0s 59us/step - loss: 0.2561 - binary_accuracy: 0.8817 - sensitivity: 0.8491 - specificity: 0.9692 - gmeasure: 0.9070 - auc: 0.9195 - val_loss: 0.3649 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8349 - val_specificity: 0.7805 - val_gmeasure: 0.8072 - val_auc: 0.8972
Epoch 77/100
600/600 [==============================] - 0s 62us/step - loss: 0.2593 - binary_accuracy: 0.8750 - sensitivity: 0.8527 - specificity: 0.9352 - gmeasure: 0.8925 - auc: 0.9149 - val_loss: 0.3504 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8257 - val_specificity: 0.8537 - val_gmeasure: 0.8396 - val_auc: 0.8921
Epoch 78/100
600/600 [==============================] - 0s 67us/step - loss: 0.2640 - binary_accuracy: 0.8767 - sensitivity: 0.8401 - specificity: 0.9759 - gmeasure: 0.9053 - auc: 0.9227 - val_loss: 0.3589 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8349 - val_specificity: 0.8293 - val_gmeasure: 0.8321 - val_auc: 0.8972
Epoch 79/100
600/600 [==============================] - 0s 64us/step - loss: 0.2640 - binary_accuracy: 0.8683 - sensitivity: 0.8530 - specificity: 0.9064 - gmeasure: 0.8790 - auc: 0.9144 - val_loss: 0.3760 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8440 - val_specificity: 0.7805 - val_gmeasure: 0.8116 - val_auc: 0.8972
Epoch 80/100
600/600 [==============================] - 0s 58us/step - loss: 0.2583 - binary_accuracy: 0.8800 - sensitivity: 0.8519 - specificity: 0.9564 - gmeasure: 0.9024 - auc: 0.9202 - val_loss: 0.3399 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8165 - val_specificity: 0.8537 - val_gmeasure: 0.8349 - val_auc: 0.8926
Epoch 81/100
600/600 [==============================] - 0s 64us/step - loss: 0.2579 - binary_accuracy: 0.8817 - sensitivity: 0.8467 - specificity: 0.9758 - gmeasure: 0.9089 - auc: 0.9207 - val_loss: 0.3875 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8532 - val_specificity: 0.7561 - val_gmeasure: 0.8032 - val_auc: 0.8990
Epoch 82/100
600/600 [==============================] - 0s 66us/step - loss: 0.2636 - binary_accuracy: 0.8667 - sensitivity: 0.8533 - specificity: 0.9028 - gmeasure: 0.8777 - auc: 0.9115 - val_loss: 0.3533 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8349 - val_specificity: 0.8537 - val_gmeasure: 0.8442 - val_auc: 0.8921
Epoch 83/100
600/600 [==============================] - 0s 66us/step - loss: 0.2607 - binary_accuracy: 0.8783 - sensitivity: 0.8426 - specificity: 0.9747 - gmeasure: 0.9062 - auc: 0.9211 - val_loss: 0.3403 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8257 - val_specificity: 0.8537 - val_gmeasure: 0.8396 - val_auc: 0.8944
Epoch 84/100
600/600 [==============================] - 0s 63us/step - loss: 0.2610 - binary_accuracy: 0.8700 - sensitivity: 0.8440 - specificity: 0.9365 - gmeasure: 0.8888 - auc: 0.9179 - val_loss: 0.3951 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8624 - val_specificity: 0.7561 - val_gmeasure: 0.8075 - val_auc: 0.8974
Epoch 85/100
600/600 [==============================] - 0s 59us/step - loss: 0.2579 - binary_accuracy: 0.8683 - sensitivity: 0.8529 - specificity: 0.9002 - gmeasure: 0.8755 - auc: 0.9153 - val_loss: 0.3460 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8349 - val_specificity: 0.8537 - val_gmeasure: 0.8442 - val_auc: 0.8886
Epoch 86/100
600/600 [==============================] - 0s 59us/step - loss: 0.2662 - binary_accuracy: 0.8750 - sensitivity: 0.8378 - specificity: 0.9761 - gmeasure: 0.9042 - auc: 0.9206 - val_loss: 0.3515 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8349 - val_specificity: 0.8537 - val_gmeasure: 0.8442 - val_auc: 0.8924
Epoch 87/100
600/600 [==============================] - 0s 60us/step - loss: 0.2575 - binary_accuracy: 0.8750 - sensitivity: 0.8515 - specificity: 0.9390 - gmeasure: 0.8941 - auc: 0.9173 - val_loss: 0.3941 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8532 - val_specificity: 0.7561 - val_gmeasure: 0.8032 - val_auc: 0.8992
Epoch 88/100
600/600 [==============================] - 0s 56us/step - loss: 0.2586 - binary_accuracy: 0.8767 - sensitivity: 0.8517 - specificity: 0.9485 - gmeasure: 0.8984 - auc: 0.9186 - val_loss: 0.3451 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8349 - val_specificity: 0.8537 - val_gmeasure: 0.8442 - val_auc: 0.8906
Epoch 89/100
600/600 [==============================] - 0s 63us/step - loss: 0.2561 - binary_accuracy: 0.8817 - sensitivity: 0.8464 - specificity: 0.9759 - gmeasure: 0.9088 - auc: 0.9227 - val_loss: 0.3774 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8440 - val_specificity: 0.7805 - val_gmeasure: 0.8116 - val_auc: 0.8952
Epoch 90/100
600/600 [==============================] - 0s 65us/step - loss: 0.2627 - binary_accuracy: 0.8683 - sensitivity: 0.8538 - specificity: 0.9124 - gmeasure: 0.8821 - auc: 0.9152 - val_loss: 0.3695 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8349 - val_specificity: 0.8293 - val_gmeasure: 0.8321 - val_auc: 0.8974
Epoch 91/100
600/600 [==============================] - 0s 64us/step - loss: 0.2544 - binary_accuracy: 0.8833 - sensitivity: 0.8516 - specificity: 0.9693 - gmeasure: 0.9083 - auc: 0.9211 - val_loss: 0.3441 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8349 - val_specificity: 0.8537 - val_gmeasure: 0.8442 - val_auc: 0.8946
Epoch 92/100
600/600 [==============================] - 0s 64us/step - loss: 0.2545 - binary_accuracy: 0.8850 - sensitivity: 0.8509 - specificity: 0.9748 - gmeasure: 0.9106 - auc: 0.9207 - val_loss: 0.3852 - val_binary_accuracy: 0.8200 - val_sensitivity: 0.8440 - val_specificity: 0.7561 - val_gmeasure: 0.7989 - val_auc: 0.8956
Epoch 93/100
600/600 [==============================] - 0s 66us/step - loss: 0.2547 - binary_accuracy: 0.8783 - sensitivity: 0.8537 - specificity: 0.9443 - gmeasure: 0.8978 - auc: 0.9160 - val_loss: 0.3631 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8349 - val_specificity: 0.8293 - val_gmeasure: 0.8321 - val_auc: 0.8942
Epoch 94/100
600/600 [==============================] - 0s 67us/step - loss: 0.2558 - binary_accuracy: 0.8817 - sensitivity: 0.8494 - specificity: 0.9680 - gmeasure: 0.9067 - auc: 0.9211 - val_loss: 0.3526 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8349 - val_specificity: 0.8537 - val_gmeasure: 0.8442 - val_auc: 0.8911
Epoch 95/100
600/600 [==============================] - 0s 64us/step - loss: 0.2538 - binary_accuracy: 0.8817 - sensitivity: 0.8504 - specificity: 0.9667 - gmeasure: 0.9064 - auc: 0.9179 - val_loss: 0.3800 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8440 - val_specificity: 0.7805 - val_gmeasure: 0.8116 - val_auc: 0.8961
Epoch 96/100
600/600 [==============================] - 0s 61us/step - loss: 0.2514 - binary_accuracy: 0.8800 - sensitivity: 0.8525 - specificity: 0.9568 - gmeasure: 0.9028 - auc: 0.9187 - val_loss: 0.3548 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8349 - val_specificity: 0.8293 - val_gmeasure: 0.8321 - val_auc: 0.8896
Epoch 97/100
600/600 [==============================] - 0s 61us/step - loss: 0.2525 - binary_accuracy: 0.8833 - sensitivity: 0.8494 - specificity: 0.9759 - gmeasure: 0.9103 - auc: 0.9220 - val_loss: 0.3692 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8349 - val_specificity: 0.8293 - val_gmeasure: 0.8321 - val_auc: 0.8929
Epoch 98/100
600/600 [==============================] - 0s 62us/step - loss: 0.2566 - binary_accuracy: 0.8750 - sensitivity: 0.8535 - specificity: 0.9346 - gmeasure: 0.8924 - auc: 0.9175 - val_loss: 0.3695 - val_binary_accuracy: 0.8333 - val_sensitivity: 0.8349 - val_specificity: 0.8293 - val_gmeasure: 0.8321 - val_auc: 0.8948
Epoch 99/100
600/600 [==============================] - 0s 67us/step - loss: 0.2521 - binary_accuracy: 0.8850 - sensitivity: 0.8537 - specificity: 0.9695 - gmeasure: 0.9096 - auc: 0.9215 - val_loss: 0.3446 - val_binary_accuracy: 0.8400 - val_sensitivity: 0.8349 - val_specificity: 0.8537 - val_gmeasure: 0.8442 - val_auc: 0.8919
Epoch 100/100
600/600 [==============================] - 0s 70us/step - loss: 0.2579 - binary_accuracy: 0.8783 - sensitivity: 0.8516 - specificity: 0.9546 - gmeasure: 0.9013 - auc: 0.9209 - val_loss: 0.3848 - val_binary_accuracy: 0.8267 - val_sensitivity: 0.8440 - val_specificity: 0.7805 - val_gmeasure: 0.8116 - val_auc: 0.8987
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:151] Training end with time 6.21663236618042!
[root    |INFO|build_network.py:79] Saved trained model weight at ./example_result/weight_1.h5
[root    |DEBUG|deepbiome.py:166] Save weight at ./example_result/weight_1.h5
[root    |DEBUG|deepbiome.py:169] Save history at ./example_result/hist_1.json
[root    |INFO|build_network.py:177] Evaluation start!
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
750/750 [==============================] - 0s 7us/step
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:182] Evaluation end with time 0.011637210845947266!
[root    |INFO|build_network.py:183] Evaluation: [0.27894675731658936, 0.8679999709129333, 0.8516483306884766, 0.9117646813392639, 0.881193995475769, 0.9125816822052002]
[root    |INFO|build_network.py:177] Evaluation start!
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
250/250 [==============================] - 0s 17us/step
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:182] Evaluation end with time 0.011641740798950195!
[root    |INFO|build_network.py:183] Evaluation: [0.4448338449001312, 0.828000009059906, 0.8505747318267822, 0.7763158082962036, 0.8125974535942078, 0.8998034000396729]
[root    |INFO|deepbiome.py:179] Compute time : 8.03881287574768
[root    |INFO|deepbiome.py:180] 2 fold computing end!---------------------------------------------
[root    |INFO|deepbiome.py:137] -------3 simulation start!----------------------------------
[root    |INFO|readers.py:58] -----------------------------------------------------------------------
[root    |INFO|readers.py:59] Construct Dataset
[root    |INFO|readers.py:60] -----------------------------------------------------------------------
[root    |INFO|readers.py:61] Load data
[root    |INFO|deepbiome.py:147] -----------------------------------------------------------------
[root    |INFO|deepbiome.py:148] Build network for 3 simulation
[root    |INFO|build_network.py:513] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:514] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv
[root    |INFO|build_network.py:518] Phylogenetic tree level list: [&#39;Genus&#39;, &#39;Family&#39;, &#39;Order&#39;, &#39;Class&#39;, &#39;Phylum&#39;]
[root    |INFO|build_network.py:519] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:526]      Genus: 48
[root    |INFO|build_network.py:526]     Family: 40
[root    |INFO|build_network.py:526]      Order: 23
[root    |INFO|build_network.py:526]      Class: 17
[root    |INFO|build_network.py:526]     Phylum: 9
[root    |INFO|build_network.py:529] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:530] Phylogenetic_tree_dict info: [&#39;Genus&#39;, &#39;Number&#39;, &#39;Order&#39;, &#39;Class&#39;, &#39;Phylum&#39;, &#39;Family&#39;]
[root    |INFO|build_network.py:531] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:541] Build edge weights between [ Genus, Family]
[root    |INFO|build_network.py:541] Build edge weights between [Family,  Order]
[root    |INFO|build_network.py:541] Build edge weights between [ Order,  Class]
[root    |INFO|build_network.py:541] Build edge weights between [ Class, Phylum]
[root    |INFO|build_network.py:554] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:570] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:571] Build network based on phylogenetic tree information
[root    |INFO|build_network.py:572] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:648] ------------------------------------------------------------------------------------------
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Model: &#34;model_1&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input (InputLayer)           (None, 48)                0
_________________________________________________________________
l1_dense (Dense_with_tree)   (None, 40)                1960
_________________________________________________________________
l1_activation (Activation)   (None, 40)                0
_________________________________________________________________
l2_dense (Dense_with_tree)   (None, 23)                943
_________________________________________________________________
l2_activation (Activation)   (None, 23)                0
_________________________________________________________________
l3_dense (Dense_with_tree)   (None, 17)                408
_________________________________________________________________
l3_activation (Activation)   (None, 17)                0
_________________________________________________________________
l4_dense (Dense_with_tree)   (None, 9)                 162
_________________________________________________________________
l4_activation (Activation)   (None, 9)                 0
_________________________________________________________________
last_dense_h (Dense)         (None, 1)                 10
_________________________________________________________________
p_hat (Activation)           (None, 1)                 0
=================================================================
Total params: 3,483
Trainable params: 3,483
Non-trainable params: 0
_________________________________________________________________
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:57] Build Network
[root    |INFO|build_network.py:58] Optimizer = adam
[root    |INFO|build_network.py:59] Loss = binary_crossentropy
[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc
[root    |INFO|build_network.py:83] Load trained model weight at ./example_result/weight_2.h5
[root    |INFO|deepbiome.py:157] -----------------------------------------------------------------
[root    |INFO|deepbiome.py:158] 3 fold computing start!----------------------------------
[root    |INFO|build_network.py:141] Training start!
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Train on 600 samples, validate on 150 samples
Epoch 1/100
600/600 [==============================] - 1s 834us/step - loss: 0.1879 - binary_accuracy: 0.9250 - sensitivity: 0.9544 - specificity: 0.8668 - gmeasure: 0.9075 - auc: 0.9862 - val_loss: 0.2923 - val_binary_accuracy: 0.8933 - val_sensitivity: 0.9796 - val_specificity: 0.7308 - val_gmeasure: 0.8461 - val_auc: 0.9662
Epoch 2/100
600/600 [==============================] - ETA: 0s - loss: 0.1461 - binary_accuracy: 0.9550 - sensitivity: 0.9845 - specificity: 0.9014 - gmeasure: 0.9420 - auc: 0.99 - 0s 80us/step - loss: 0.1519 - binary_accuracy: 0.9483 - sensitivity: 0.9643 - specificity: 0.9154 - gmeasure: 0.9393 - auc: 0.9873 - val_loss: 0.2505 - val_binary_accuracy: 0.9067 - val_sensitivity: 0.9082 - val_specificity: 0.9038 - val_gmeasure: 0.9060 - val_auc: 0.9609
Epoch 3/100
600/600 [==============================] - 0s 68us/step - loss: 0.1656 - binary_accuracy: 0.9433 - sensitivity: 0.9377 - specificity: 0.9592 - gmeasure: 0.9481 - auc: 0.9877 - val_loss: 0.2538 - val_binary_accuracy: 0.9133 - val_sensitivity: 0.9694 - val_specificity: 0.8077 - val_gmeasure: 0.8849 - val_auc: 0.9672
Epoch 4/100
600/600 [==============================] - 0s 66us/step - loss: 0.1482 - binary_accuracy: 0.9450 - sensitivity: 0.9733 - specificity: 0.8812 - gmeasure: 0.9259 - auc: 0.9868 - val_loss: 0.2775 - val_binary_accuracy: 0.9000 - val_sensitivity: 0.9796 - val_specificity: 0.7500 - val_gmeasure: 0.8571 - val_auc: 0.9670
Epoch 5/100
600/600 [==============================] - 0s 64us/step - loss: 0.1450 - binary_accuracy: 0.9500 - sensitivity: 0.9733 - specificity: 0.8987 - gmeasure: 0.9353 - auc: 0.9878 - val_loss: 0.2310 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9666
Epoch 6/100
600/600 [==============================] - 0s 67us/step - loss: 0.1410 - binary_accuracy: 0.9583 - sensitivity: 0.9612 - specificity: 0.9511 - gmeasure: 0.9561 - auc: 0.9883 - val_loss: 0.2307 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9664
Epoch 7/100
600/600 [==============================] - 0s 69us/step - loss: 0.1382 - binary_accuracy: 0.9550 - sensitivity: 0.9712 - specificity: 0.9200 - gmeasure: 0.9451 - auc: 0.9884 - val_loss: 0.2489 - val_binary_accuracy: 0.9133 - val_sensitivity: 0.9694 - val_specificity: 0.8077 - val_gmeasure: 0.8849 - val_auc: 0.9676
Epoch 8/100
600/600 [==============================] - 0s 70us/step - loss: 0.1369 - binary_accuracy: 0.9500 - sensitivity: 0.9731 - specificity: 0.8966 - gmeasure: 0.9338 - auc: 0.9875 - val_loss: 0.2441 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9694 - val_specificity: 0.8269 - val_gmeasure: 0.8953 - val_auc: 0.9674
Epoch 9/100
600/600 [==============================] - 0s 66us/step - loss: 0.1343 - binary_accuracy: 0.9500 - sensitivity: 0.9703 - specificity: 0.9017 - gmeasure: 0.9352 - auc: 0.9876 - val_loss: 0.2330 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9674
Epoch 10/100
600/600 [==============================] - 0s 64us/step - loss: 0.1350 - binary_accuracy: 0.9617 - sensitivity: 0.9714 - specificity: 0.9408 - gmeasure: 0.9560 - auc: 0.9883 - val_loss: 0.2319 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9670
Epoch 11/100
600/600 [==============================] - 0s 64us/step - loss: 0.1332 - binary_accuracy: 0.9567 - sensitivity: 0.9703 - specificity: 0.9255 - gmeasure: 0.9474 - auc: 0.9880 - val_loss: 0.2404 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9694 - val_specificity: 0.8269 - val_gmeasure: 0.8953 - val_auc: 0.9680
Epoch 12/100
600/600 [==============================] - 0s 59us/step - loss: 0.1329 - binary_accuracy: 0.9517 - sensitivity: 0.9706 - specificity: 0.9076 - gmeasure: 0.9378 - auc: 0.9882 - val_loss: 0.2370 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9592 - val_specificity: 0.8462 - val_gmeasure: 0.9009 - val_auc: 0.9686
Epoch 13/100
600/600 [==============================] - 0s 64us/step - loss: 0.1322 - binary_accuracy: 0.9517 - sensitivity: 0.9715 - specificity: 0.9110 - gmeasure: 0.9407 - auc: 0.9887 - val_loss: 0.2333 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9688
Epoch 14/100
600/600 [==============================] - 0s 63us/step - loss: 0.1319 - binary_accuracy: 0.9567 - sensitivity: 0.9707 - specificity: 0.9268 - gmeasure: 0.9485 - auc: 0.9884 - val_loss: 0.2331 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9686
Epoch 15/100
600/600 [==============================] - 0s 70us/step - loss: 0.1304 - binary_accuracy: 0.9567 - sensitivity: 0.9711 - specificity: 0.9243 - gmeasure: 0.9473 - auc: 0.9890 - val_loss: 0.2385 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9694 - val_specificity: 0.8269 - val_gmeasure: 0.8953 - val_auc: 0.9692
Epoch 16/100
600/600 [==============================] - 0s 64us/step - loss: 0.1301 - binary_accuracy: 0.9517 - sensitivity: 0.9708 - specificity: 0.9109 - gmeasure: 0.9403 - auc: 0.9885 - val_loss: 0.2313 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9684
Epoch 17/100
600/600 [==============================] - 0s 62us/step - loss: 0.1294 - binary_accuracy: 0.9533 - sensitivity: 0.9713 - specificity: 0.9154 - gmeasure: 0.9429 - auc: 0.9892 - val_loss: 0.2346 - val_binary_accuracy: 0.9267 - val_sensitivity: 0.9592 - val_specificity: 0.8654 - val_gmeasure: 0.9111 - val_auc: 0.9694
Epoch 18/100
600/600 [==============================] - 0s 64us/step - loss: 0.1283 - binary_accuracy: 0.9533 - sensitivity: 0.9710 - specificity: 0.9164 - gmeasure: 0.9432 - auc: 0.9894 - val_loss: 0.2329 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9690
Epoch 19/100
600/600 [==============================] - 0s 64us/step - loss: 0.1276 - binary_accuracy: 0.9600 - sensitivity: 0.9710 - specificity: 0.9362 - gmeasure: 0.9534 - auc: 0.9894 - val_loss: 0.2283 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9692
Epoch 20/100
600/600 [==============================] - 0s 67us/step - loss: 0.1277 - binary_accuracy: 0.9633 - sensitivity: 0.9709 - specificity: 0.9460 - gmeasure: 0.9582 - auc: 0.9897 - val_loss: 0.2328 - val_binary_accuracy: 0.9267 - val_sensitivity: 0.9592 - val_specificity: 0.8654 - val_gmeasure: 0.9111 - val_auc: 0.9696
Epoch 21/100
600/600 [==============================] - 0s 56us/step - loss: 0.1263 - binary_accuracy: 0.9533 - sensitivity: 0.9710 - specificity: 0.9134 - gmeasure: 0.9417 - auc: 0.9895 - val_loss: 0.2377 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9694 - val_specificity: 0.8269 - val_gmeasure: 0.8953 - val_auc: 0.9706
Epoch 22/100
600/600 [==============================] - 0s 59us/step - loss: 0.1266 - binary_accuracy: 0.9533 - sensitivity: 0.9731 - specificity: 0.9060 - gmeasure: 0.9386 - auc: 0.9895 - val_loss: 0.2287 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9696
Epoch 23/100
600/600 [==============================] - 0s 54us/step - loss: 0.1248 - binary_accuracy: 0.9667 - sensitivity: 0.9710 - specificity: 0.9564 - gmeasure: 0.9635 - auc: 0.9893 - val_loss: 0.2244 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9698
Epoch 24/100
600/600 [==============================] - 0s 67us/step - loss: 0.1246 - binary_accuracy: 0.9583 - sensitivity: 0.9707 - specificity: 0.9308 - gmeasure: 0.9505 - auc: 0.9891 - val_loss: 0.2326 - val_binary_accuracy: 0.9333 - val_sensitivity: 0.9694 - val_specificity: 0.8654 - val_gmeasure: 0.9159 - val_auc: 0.9702
Epoch 25/100
600/600 [==============================] - 0s 68us/step - loss: 0.1233 - binary_accuracy: 0.9550 - sensitivity: 0.9710 - specificity: 0.9199 - gmeasure: 0.9450 - auc: 0.9899 - val_loss: 0.2269 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9704
Epoch 26/100
600/600 [==============================] - 0s 62us/step - loss: 0.1227 - binary_accuracy: 0.9633 - sensitivity: 0.9708 - specificity: 0.9471 - gmeasure: 0.9588 - auc: 0.9889 - val_loss: 0.2242 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9708
Epoch 27/100
600/600 [==============================] - 0s 68us/step - loss: 0.1253 - binary_accuracy: 0.9550 - sensitivity: 0.9708 - specificity: 0.9212 - gmeasure: 0.9451 - auc: 0.9896 - val_loss: 0.2300 - val_binary_accuracy: 0.9267 - val_sensitivity: 0.9592 - val_specificity: 0.8654 - val_gmeasure: 0.9111 - val_auc: 0.9714
Epoch 28/100
600/600 [==============================] - 0s 62us/step - loss: 0.1212 - binary_accuracy: 0.9650 - sensitivity: 0.9707 - specificity: 0.9510 - gmeasure: 0.9607 - auc: 0.9891 - val_loss: 0.2212 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9712
Epoch 29/100
600/600 [==============================] - 0s 62us/step - loss: 0.1217 - binary_accuracy: 0.9583 - sensitivity: 0.9714 - specificity: 0.9315 - gmeasure: 0.9509 - auc: 0.9897 - val_loss: 0.2315 - val_binary_accuracy: 0.9333 - val_sensitivity: 0.9694 - val_specificity: 0.8654 - val_gmeasure: 0.9159 - val_auc: 0.9715
Epoch 30/100
600/600 [==============================] - 0s 61us/step - loss: 0.1222 - binary_accuracy: 0.9583 - sensitivity: 0.9715 - specificity: 0.9335 - gmeasure: 0.9520 - auc: 0.9905 - val_loss: 0.2209 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9710
Epoch 31/100
600/600 [==============================] - 0s 64us/step - loss: 0.1185 - binary_accuracy: 0.9617 - sensitivity: 0.9705 - specificity: 0.9386 - gmeasure: 0.9542 - auc: 0.9892 - val_loss: 0.2304 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9796 - val_specificity: 0.8654 - val_gmeasure: 0.9207 - val_auc: 0.9715
Epoch 32/100
600/600 [==============================] - 0s 72us/step - loss: 0.1223 - binary_accuracy: 0.9533 - sensitivity: 0.9754 - specificity: 0.9088 - gmeasure: 0.9409 - auc: 0.9895 - val_loss: 0.2182 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9490 - val_specificity: 0.8654 - val_gmeasure: 0.9062 - val_auc: 0.9687
Epoch 33/100
600/600 [==============================] - 0s 62us/step - loss: 0.1177 - binary_accuracy: 0.9667 - sensitivity: 0.9709 - specificity: 0.9569 - gmeasure: 0.9638 - auc: 0.9906 - val_loss: 0.2128 - val_binary_accuracy: 0.9267 - val_sensitivity: 0.9490 - val_specificity: 0.8846 - val_gmeasure: 0.9162 - val_auc: 0.9697
Epoch 34/100
600/600 [==============================] - 0s 61us/step - loss: 0.1183 - binary_accuracy: 0.9650 - sensitivity: 0.9703 - specificity: 0.9497 - gmeasure: 0.9596 - auc: 0.9897 - val_loss: 0.2282 - val_binary_accuracy: 0.9333 - val_sensitivity: 0.9796 - val_specificity: 0.8462 - val_gmeasure: 0.9104 - val_auc: 0.9698
Epoch 35/100
600/600 [==============================] - 0s 65us/step - loss: 0.1176 - binary_accuracy: 0.9600 - sensitivity: 0.9709 - specificity: 0.9365 - gmeasure: 0.9532 - auc: 0.9908 - val_loss: 0.2154 - val_binary_accuracy: 0.9267 - val_sensitivity: 0.9592 - val_specificity: 0.8654 - val_gmeasure: 0.9111 - val_auc: 0.9698
Epoch 36/100
600/600 [==============================] - 0s 62us/step - loss: 0.1155 - binary_accuracy: 0.9617 - sensitivity: 0.9704 - specificity: 0.9378 - gmeasure: 0.9536 - auc: 0.9895 - val_loss: 0.2132 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9592 - val_specificity: 0.8462 - val_gmeasure: 0.9009 - val_auc: 0.9702
Epoch 37/100
600/600 [==============================] - 0s 58us/step - loss: 0.1187 - binary_accuracy: 0.9583 - sensitivity: 0.9733 - specificity: 0.9281 - gmeasure: 0.9502 - auc: 0.9896 - val_loss: 0.2161 - val_binary_accuracy: 0.9200 - val_sensitivity: 0.9592 - val_specificity: 0.8462 - val_gmeasure: 0.9009 - val_auc: 0.9704
Epoch 38/100
600/600 [==============================] - 0s 56us/step - loss: 0.1184 - binary_accuracy: 0.9600 - sensitivity: 0.9637 - specificity: 0.9518 - gmeasure: 0.9576 - auc: 0.9906 - val_loss: 0.2034 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9490 - val_specificity: 0.9231 - val_gmeasure: 0.9359 - val_auc: 0.9709
Epoch 39/100
600/600 [==============================] - 0s 60us/step - loss: 0.1147 - binary_accuracy: 0.9617 - sensitivity: 0.9668 - specificity: 0.9520 - gmeasure: 0.9592 - auc: 0.9914 - val_loss: 0.2295 - val_binary_accuracy: 0.9133 - val_sensitivity: 0.9796 - val_specificity: 0.7885 - val_gmeasure: 0.8788 - val_auc: 0.9714
Epoch 40/100
600/600 [==============================] - 0s 65us/step - loss: 0.1160 - binary_accuracy: 0.9567 - sensitivity: 0.9758 - specificity: 0.9147 - gmeasure: 0.9447 - auc: 0.9901 - val_loss: 0.2060 - val_binary_accuracy: 0.9333 - val_sensitivity: 0.9490 - val_specificity: 0.9038 - val_gmeasure: 0.9261 - val_auc: 0.9715
Epoch 41/100
600/600 [==============================] - 0s 66us/step - loss: 0.1118 - binary_accuracy: 0.9683 - sensitivity: 0.9708 - specificity: 0.9606 - gmeasure: 0.9655 - auc: 0.9904 - val_loss: 0.2039 - val_binary_accuracy: 0.9333 - val_sensitivity: 0.9490 - val_specificity: 0.9038 - val_gmeasure: 0.9261 - val_auc: 0.9716
Epoch 42/100
600/600 [==============================] - 0s 62us/step - loss: 0.1122 - binary_accuracy: 0.9667 - sensitivity: 0.9732 - specificity: 0.9499 - gmeasure: 0.9612 - auc: 0.9905 - val_loss: 0.2099 - val_binary_accuracy: 0.9333 - val_sensitivity: 0.9694 - val_specificity: 0.8654 - val_gmeasure: 0.9159 - val_auc: 0.9719
Epoch 43/100
600/600 [==============================] - 0s 57us/step - loss: 0.1133 - binary_accuracy: 0.9633 - sensitivity: 0.9685 - specificity: 0.9513 - gmeasure: 0.9597 - auc: 0.9909 - val_loss: 0.2065 - val_binary_accuracy: 0.9333 - val_sensitivity: 0.9592 - val_specificity: 0.8846 - val_gmeasure: 0.9211 - val_auc: 0.9721
Epoch 44/100
600/600 [==============================] - 0s 62us/step - loss: 0.1096 - binary_accuracy: 0.9617 - sensitivity: 0.9731 - specificity: 0.9349 - gmeasure: 0.9538 - auc: 0.9908 - val_loss: 0.2145 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9796 - val_specificity: 0.8654 - val_gmeasure: 0.9207 - val_auc: 0.9719
Epoch 45/100
600/600 [==============================] - 0s 57us/step - loss: 0.1101 - binary_accuracy: 0.9667 - sensitivity: 0.9758 - specificity: 0.9450 - gmeasure: 0.9602 - auc: 0.9903 - val_loss: 0.2012 - val_binary_accuracy: 0.9267 - val_sensitivity: 0.9490 - val_specificity: 0.8846 - val_gmeasure: 0.9162 - val_auc: 0.9721
Epoch 46/100
600/600 [==============================] - 0s 53us/step - loss: 0.1084 - binary_accuracy: 0.9683 - sensitivity: 0.9708 - specificity: 0.9620 - gmeasure: 0.9664 - auc: 0.9907 - val_loss: 0.2081 - val_binary_accuracy: 0.9333 - val_sensitivity: 0.9694 - val_specificity: 0.8654 - val_gmeasure: 0.9159 - val_auc: 0.9721
Epoch 47/100
600/600 [==============================] - 0s 61us/step - loss: 0.1084 - binary_accuracy: 0.9600 - sensitivity: 0.9784 - specificity: 0.9191 - gmeasure: 0.9482 - auc: 0.9903 - val_loss: 0.2067 - val_binary_accuracy: 0.9333 - val_sensitivity: 0.9694 - val_specificity: 0.8654 - val_gmeasure: 0.9159 - val_auc: 0.9723
Epoch 48/100
600/600 [==============================] - 0s 67us/step - loss: 0.1103 - binary_accuracy: 0.9683 - sensitivity: 0.9713 - specificity: 0.9622 - gmeasure: 0.9665 - auc: 0.9912 - val_loss: 0.1982 - val_binary_accuracy: 0.9267 - val_sensitivity: 0.9490 - val_specificity: 0.8846 - val_gmeasure: 0.9162 - val_auc: 0.9721
Epoch 49/100
600/600 [==============================] - 0s 62us/step - loss: 0.1055 - binary_accuracy: 0.9700 - sensitivity: 0.9758 - specificity: 0.9571 - gmeasure: 0.9663 - auc: 0.9909 - val_loss: 0.2228 - val_binary_accuracy: 0.9333 - val_sensitivity: 0.9796 - val_specificity: 0.8462 - val_gmeasure: 0.9104 - val_auc: 0.9723
Epoch 50/100
600/600 [==============================] - 0s 66us/step - loss: 0.1084 - binary_accuracy: 0.9600 - sensitivity: 0.9784 - specificity: 0.9193 - gmeasure: 0.9484 - auc: 0.9906 - val_loss: 0.2001 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9694 - val_specificity: 0.8846 - val_gmeasure: 0.9260 - val_auc: 0.9723
Epoch 51/100
600/600 [==============================] - 0s 65us/step - loss: 0.1085 - binary_accuracy: 0.9667 - sensitivity: 0.9693 - specificity: 0.9625 - gmeasure: 0.9657 - auc: 0.9919 - val_loss: 0.2050 - val_binary_accuracy: 0.9333 - val_sensitivity: 0.9694 - val_specificity: 0.8654 - val_gmeasure: 0.9159 - val_auc: 0.9725
Epoch 52/100
600/600 [==============================] - 0s 64us/step - loss: 0.1044 - binary_accuracy: 0.9683 - sensitivity: 0.9780 - specificity: 0.9455 - gmeasure: 0.9615 - auc: 0.9906 - val_loss: 0.2079 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9796 - val_specificity: 0.8654 - val_gmeasure: 0.9207 - val_auc: 0.9727
Epoch 53/100
600/600 [==============================] - 0s 61us/step - loss: 0.1042 - binary_accuracy: 0.9700 - sensitivity: 0.9757 - specificity: 0.9585 - gmeasure: 0.9670 - auc: 0.9909 - val_loss: 0.1987 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9694 - val_specificity: 0.8846 - val_gmeasure: 0.9260 - val_auc: 0.9725
Epoch 54/100
600/600 [==============================] - 0s 65us/step - loss: 0.1045 - binary_accuracy: 0.9683 - sensitivity: 0.9757 - specificity: 0.9519 - gmeasure: 0.9637 - auc: 0.9906 - val_loss: 0.2028 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9694 - val_specificity: 0.8846 - val_gmeasure: 0.9260 - val_auc: 0.9725
Epoch 55/100
600/600 [==============================] - 0s 69us/step - loss: 0.1041 - binary_accuracy: 0.9683 - sensitivity: 0.9755 - specificity: 0.9535 - gmeasure: 0.9642 - auc: 0.9914 - val_loss: 0.1998 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9694 - val_specificity: 0.8846 - val_gmeasure: 0.9260 - val_auc: 0.9725
Epoch 56/100
600/600 [==============================] - 0s 64us/step - loss: 0.1085 - binary_accuracy: 0.9633 - sensitivity: 0.9613 - specificity: 0.9681 - gmeasure: 0.9646 - auc: 0.9910 - val_loss: 0.1980 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9694 - val_specificity: 0.8846 - val_gmeasure: 0.9260 - val_auc: 0.9725
Epoch 57/100
600/600 [==============================] - 0s 61us/step - loss: 0.1055 - binary_accuracy: 0.9683 - sensitivity: 0.9778 - specificity: 0.9434 - gmeasure: 0.9601 - auc: 0.9911 - val_loss: 0.2254 - val_binary_accuracy: 0.9333 - val_sensitivity: 0.9796 - val_specificity: 0.8462 - val_gmeasure: 0.9104 - val_auc: 0.9731
Epoch 58/100
600/600 [==============================] - 0s 59us/step - loss: 0.1016 - binary_accuracy: 0.9667 - sensitivity: 0.9758 - specificity: 0.9463 - gmeasure: 0.9609 - auc: 0.9910 - val_loss: 0.1909 - val_binary_accuracy: 0.9267 - val_sensitivity: 0.9388 - val_specificity: 0.9038 - val_gmeasure: 0.9211 - val_auc: 0.9729
Epoch 59/100
600/600 [==============================] - 0s 60us/step - loss: 0.1036 - binary_accuracy: 0.9683 - sensitivity: 0.9685 - specificity: 0.9678 - gmeasure: 0.9682 - auc: 0.9917 - val_loss: 0.2026 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9694 - val_specificity: 0.8846 - val_gmeasure: 0.9260 - val_auc: 0.9739
Epoch 60/100
600/600 [==============================] - 0s 65us/step - loss: 0.0999 - binary_accuracy: 0.9683 - sensitivity: 0.9784 - specificity: 0.9476 - gmeasure: 0.9628 - auc: 0.9913 - val_loss: 0.2041 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9796 - val_specificity: 0.8654 - val_gmeasure: 0.9207 - val_auc: 0.9741
Epoch 61/100
600/600 [==============================] - 0s 65us/step - loss: 0.1007 - binary_accuracy: 0.9667 - sensitivity: 0.9732 - specificity: 0.9515 - gmeasure: 0.9621 - auc: 0.9915 - val_loss: 0.1937 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9694 - val_specificity: 0.8846 - val_gmeasure: 0.9260 - val_auc: 0.9739
Epoch 62/100
600/600 [==============================] - 0s 63us/step - loss: 0.0992 - binary_accuracy: 0.9683 - sensitivity: 0.9758 - specificity: 0.9518 - gmeasure: 0.9636 - auc: 0.9913 - val_loss: 0.2018 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9796 - val_specificity: 0.8654 - val_gmeasure: 0.9207 - val_auc: 0.9743
Epoch 63/100
600/600 [==============================] - 0s 67us/step - loss: 0.0980 - binary_accuracy: 0.9717 - sensitivity: 0.9788 - specificity: 0.9575 - gmeasure: 0.9680 - auc: 0.9908 - val_loss: 0.1907 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9694 - val_specificity: 0.9038 - val_gmeasure: 0.9360 - val_auc: 0.9741
Epoch 64/100
600/600 [==============================] - 0s 62us/step - loss: 0.0984 - binary_accuracy: 0.9733 - sensitivity: 0.9733 - specificity: 0.9752 - gmeasure: 0.9742 - auc: 0.9917 - val_loss: 0.1972 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9694 - val_specificity: 0.8846 - val_gmeasure: 0.9260 - val_auc: 0.9753
Epoch 65/100
600/600 [==============================] - 0s 62us/step - loss: 0.1009 - binary_accuracy: 0.9617 - sensitivity: 0.9783 - specificity: 0.9268 - gmeasure: 0.9521 - auc: 0.9908 - val_loss: 0.2003 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9796 - val_specificity: 0.8654 - val_gmeasure: 0.9207 - val_auc: 0.9757
Epoch 66/100
600/600 [==============================] - 0s 67us/step - loss: 0.0972 - binary_accuracy: 0.9733 - sensitivity: 0.9758 - specificity: 0.9678 - gmeasure: 0.9718 - auc: 0.9920 - val_loss: 0.1852 - val_binary_accuracy: 0.9267 - val_sensitivity: 0.9388 - val_specificity: 0.9038 - val_gmeasure: 0.9211 - val_auc: 0.9755
Epoch 67/100
600/600 [==============================] - 0s 65us/step - loss: 0.0992 - binary_accuracy: 0.9650 - sensitivity: 0.9655 - specificity: 0.9623 - gmeasure: 0.9636 - auc: 0.9923 - val_loss: 0.2118 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9796 - val_specificity: 0.8654 - val_gmeasure: 0.9207 - val_auc: 0.9757
Epoch 68/100
600/600 [==============================] - 0s 56us/step - loss: 0.0971 - binary_accuracy: 0.9683 - sensitivity: 0.9782 - specificity: 0.9474 - gmeasure: 0.9626 - auc: 0.9915 - val_loss: 0.1889 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9694 - val_specificity: 0.9038 - val_gmeasure: 0.9360 - val_auc: 0.9761
Epoch 69/100
600/600 [==============================] - 0s 63us/step - loss: 0.0959 - binary_accuracy: 0.9750 - sensitivity: 0.9761 - specificity: 0.9721 - gmeasure: 0.9741 - auc: 0.9921 - val_loss: 0.1938 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9694 - val_specificity: 0.8846 - val_gmeasure: 0.9260 - val_auc: 0.9763
Epoch 70/100
600/600 [==============================] - 0s 63us/step - loss: 0.0952 - binary_accuracy: 0.9700 - sensitivity: 0.9783 - specificity: 0.9524 - gmeasure: 0.9651 - auc: 0.9919 - val_loss: 0.1994 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9796 - val_specificity: 0.8654 - val_gmeasure: 0.9207 - val_auc: 0.9761
Epoch 71/100
600/600 [==============================] - 0s 62us/step - loss: 0.0945 - binary_accuracy: 0.9733 - sensitivity: 0.9783 - specificity: 0.9623 - gmeasure: 0.9701 - auc: 0.9918 - val_loss: 0.1876 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9694 - val_specificity: 0.9038 - val_gmeasure: 0.9360 - val_auc: 0.9759
Epoch 72/100
600/600 [==============================] - 0s 62us/step - loss: 0.0963 - binary_accuracy: 0.9717 - sensitivity: 0.9758 - specificity: 0.9626 - gmeasure: 0.9690 - auc: 0.9922 - val_loss: 0.2040 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9796 - val_specificity: 0.8654 - val_gmeasure: 0.9207 - val_auc: 0.9765
Epoch 73/100
600/600 [==============================] - 0s 54us/step - loss: 0.0941 - binary_accuracy: 0.9733 - sensitivity: 0.9782 - specificity: 0.9628 - gmeasure: 0.9704 - auc: 0.9920 - val_loss: 0.1842 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9592 - val_specificity: 0.9038 - val_gmeasure: 0.9311 - val_auc: 0.9767
Epoch 74/100
600/600 [==============================] - 0s 62us/step - loss: 0.0931 - binary_accuracy: 0.9767 - sensitivity: 0.9783 - specificity: 0.9722 - gmeasure: 0.9752 - auc: 0.9919 - val_loss: 0.1995 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9796 - val_specificity: 0.8654 - val_gmeasure: 0.9207 - val_auc: 0.9766
Epoch 75/100
600/600 [==============================] - 0s 57us/step - loss: 0.0926 - binary_accuracy: 0.9700 - sensitivity: 0.9783 - specificity: 0.9522 - gmeasure: 0.9651 - auc: 0.9920 - val_loss: 0.1873 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9694 - val_specificity: 0.9038 - val_gmeasure: 0.9360 - val_auc: 0.9767
Epoch 76/100
600/600 [==============================] - 0s 58us/step - loss: 0.0921 - binary_accuracy: 0.9750 - sensitivity: 0.9784 - specificity: 0.9679 - gmeasure: 0.9731 - auc: 0.9926 - val_loss: 0.1861 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9694 - val_specificity: 0.9038 - val_gmeasure: 0.9360 - val_auc: 0.9766
Epoch 77/100
600/600 [==============================] - 0s 62us/step - loss: 0.0913 - binary_accuracy: 0.9783 - sensitivity: 0.9780 - specificity: 0.9784 - gmeasure: 0.9782 - auc: 0.9919 - val_loss: 0.1935 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9796 - val_specificity: 0.8846 - val_gmeasure: 0.9309 - val_auc: 0.9768
Epoch 78/100
600/600 [==============================] - 0s 64us/step - loss: 0.0906 - binary_accuracy: 0.9750 - sensitivity: 0.9788 - specificity: 0.9670 - gmeasure: 0.9729 - auc: 0.9920 - val_loss: 0.1912 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9796 - val_specificity: 0.8846 - val_gmeasure: 0.9309 - val_auc: 0.9769
Epoch 79/100
600/600 [==============================] - 0s 66us/step - loss: 0.0908 - binary_accuracy: 0.9767 - sensitivity: 0.9784 - specificity: 0.9730 - gmeasure: 0.9757 - auc: 0.9915 - val_loss: 0.1884 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9694 - val_specificity: 0.8846 - val_gmeasure: 0.9260 - val_auc: 0.9769
Epoch 80/100
600/600 [==============================] - 0s 65us/step - loss: 0.0900 - binary_accuracy: 0.9750 - sensitivity: 0.9782 - specificity: 0.9685 - gmeasure: 0.9733 - auc: 0.9920 - val_loss: 0.1919 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9796 - val_specificity: 0.8846 - val_gmeasure: 0.9309 - val_auc: 0.9768
Epoch 81/100
600/600 [==============================] - 0s 61us/step - loss: 0.0912 - binary_accuracy: 0.9750 - sensitivity: 0.9764 - specificity: 0.9734 - gmeasure: 0.9749 - auc: 0.9919 - val_loss: 0.1861 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9694 - val_specificity: 0.9038 - val_gmeasure: 0.9360 - val_auc: 0.9768
Epoch 82/100
600/600 [==============================] - 0s 65us/step - loss: 0.0893 - binary_accuracy: 0.9717 - sensitivity: 0.9785 - specificity: 0.9573 - gmeasure: 0.9678 - auc: 0.9916 - val_loss: 0.1963 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9796 - val_specificity: 0.8654 - val_gmeasure: 0.9207 - val_auc: 0.9770
Epoch 83/100
600/600 [==============================] - 0s 63us/step - loss: 0.0908 - binary_accuracy: 0.9783 - sensitivity: 0.9785 - specificity: 0.9787 - gmeasure: 0.9785 - auc: 0.9929 - val_loss: 0.1826 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9694 - val_specificity: 0.9038 - val_gmeasure: 0.9360 - val_auc: 0.9769
Epoch 84/100
600/600 [==============================] - 0s 58us/step - loss: 0.0879 - binary_accuracy: 0.9767 - sensitivity: 0.9784 - specificity: 0.9735 - gmeasure: 0.9759 - auc: 0.9925 - val_loss: 0.1972 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9796 - val_specificity: 0.8654 - val_gmeasure: 0.9207 - val_auc: 0.9770
Epoch 85/100
600/600 [==============================] - 0s 63us/step - loss: 0.0879 - binary_accuracy: 0.9750 - sensitivity: 0.9782 - specificity: 0.9679 - gmeasure: 0.9731 - auc: 0.9919 - val_loss: 0.1840 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9694 - val_specificity: 0.9038 - val_gmeasure: 0.9360 - val_auc: 0.9769
Epoch 86/100
600/600 [==============================] - 0s 65us/step - loss: 0.0880 - binary_accuracy: 0.9783 - sensitivity: 0.9774 - specificity: 0.9804 - gmeasure: 0.9789 - auc: 0.9919 - val_loss: 0.1826 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9694 - val_specificity: 0.9038 - val_gmeasure: 0.9360 - val_auc: 0.9771
Epoch 87/100
600/600 [==============================] - 0s 66us/step - loss: 0.0867 - binary_accuracy: 0.9767 - sensitivity: 0.9783 - specificity: 0.9734 - gmeasure: 0.9758 - auc: 0.9929 - val_loss: 0.1958 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9796 - val_specificity: 0.8654 - val_gmeasure: 0.9207 - val_auc: 0.9772
Epoch 88/100
600/600 [==============================] - 0s 65us/step - loss: 0.0876 - binary_accuracy: 0.9733 - sensitivity: 0.9778 - specificity: 0.9601 - gmeasure: 0.9687 - auc: 0.9927 - val_loss: 0.1793 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9694 - val_specificity: 0.9038 - val_gmeasure: 0.9360 - val_auc: 0.9774
Epoch 89/100
600/600 [==============================] - 0s 58us/step - loss: 0.0866 - binary_accuracy: 0.9767 - sensitivity: 0.9758 - specificity: 0.9788 - gmeasure: 0.9772 - auc: 0.9927 - val_loss: 0.1900 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9796 - val_specificity: 0.8846 - val_gmeasure: 0.9309 - val_auc: 0.9776
Epoch 90/100
600/600 [==============================] - 0s 64us/step - loss: 0.0858 - binary_accuracy: 0.9767 - sensitivity: 0.9784 - specificity: 0.9738 - gmeasure: 0.9760 - auc: 0.9930 - val_loss: 0.1821 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9694 - val_specificity: 0.9038 - val_gmeasure: 0.9360 - val_auc: 0.9772
Epoch 91/100
600/600 [==============================] - 0s 66us/step - loss: 0.0850 - binary_accuracy: 0.9783 - sensitivity: 0.9782 - specificity: 0.9785 - gmeasure: 0.9783 - auc: 0.9928 - val_loss: 0.1860 - val_binary_accuracy: 0.9533 - val_sensitivity: 0.9796 - val_specificity: 0.9038 - val_gmeasure: 0.9410 - val_auc: 0.9771
Epoch 92/100
600/600 [==============================] - 0s 57us/step - loss: 0.0846 - binary_accuracy: 0.9767 - sensitivity: 0.9781 - specificity: 0.9741 - gmeasure: 0.9761 - auc: 0.9928 - val_loss: 0.1906 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9796 - val_specificity: 0.8846 - val_gmeasure: 0.9309 - val_auc: 0.9772
Epoch 93/100
600/600 [==============================] - 0s 61us/step - loss: 0.0841 - binary_accuracy: 0.9783 - sensitivity: 0.9781 - specificity: 0.9778 - gmeasure: 0.9779 - auc: 0.9930 - val_loss: 0.1834 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9694 - val_specificity: 0.9038 - val_gmeasure: 0.9360 - val_auc: 0.9774
Epoch 94/100
600/600 [==============================] - 0s 70us/step - loss: 0.0839 - binary_accuracy: 0.9783 - sensitivity: 0.9788 - specificity: 0.9790 - gmeasure: 0.9789 - auc: 0.9927 - val_loss: 0.1847 - val_binary_accuracy: 0.9533 - val_sensitivity: 0.9796 - val_specificity: 0.9038 - val_gmeasure: 0.9410 - val_auc: 0.9774
Epoch 95/100
600/600 [==============================] - 0s 68us/step - loss: 0.0842 - binary_accuracy: 0.9767 - sensitivity: 0.9784 - specificity: 0.9735 - gmeasure: 0.9759 - auc: 0.9928 - val_loss: 0.1863 - val_binary_accuracy: 0.9533 - val_sensitivity: 0.9796 - val_specificity: 0.9038 - val_gmeasure: 0.9410 - val_auc: 0.9775
Epoch 96/100
600/600 [==============================] - 0s 64us/step - loss: 0.0835 - binary_accuracy: 0.9767 - sensitivity: 0.9781 - specificity: 0.9722 - gmeasure: 0.9751 - auc: 0.9927 - val_loss: 0.1785 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9694 - val_specificity: 0.9038 - val_gmeasure: 0.9360 - val_auc: 0.9780
Epoch 97/100
600/600 [==============================] - 0s 65us/step - loss: 0.0830 - binary_accuracy: 0.9783 - sensitivity: 0.9789 - specificity: 0.9789 - gmeasure: 0.9789 - auc: 0.9922 - val_loss: 0.1916 - val_binary_accuracy: 0.9400 - val_sensitivity: 0.9796 - val_specificity: 0.8654 - val_gmeasure: 0.9207 - val_auc: 0.9782
Epoch 98/100
600/600 [==============================] - 0s 59us/step - loss: 0.0845 - binary_accuracy: 0.9750 - sensitivity: 0.9789 - specificity: 0.9705 - gmeasure: 0.9745 - auc: 0.9933 - val_loss: 0.1787 - val_binary_accuracy: 0.9533 - val_sensitivity: 0.9796 - val_specificity: 0.9038 - val_gmeasure: 0.9410 - val_auc: 0.9780
Epoch 99/100
600/600 [==============================] - 0s 62us/step - loss: 0.0830 - binary_accuracy: 0.9767 - sensitivity: 0.9782 - specificity: 0.9712 - gmeasure: 0.9746 - auc: 0.9927 - val_loss: 0.1866 - val_binary_accuracy: 0.9533 - val_sensitivity: 0.9796 - val_specificity: 0.9038 - val_gmeasure: 0.9410 - val_auc: 0.9780
Epoch 100/100
600/600 [==============================] - 0s 66us/step - loss: 0.0818 - binary_accuracy: 0.9783 - sensitivity: 0.9781 - specificity: 0.9780 - gmeasure: 0.9780 - auc: 0.9928 - val_loss: 0.1796 - val_binary_accuracy: 0.9467 - val_sensitivity: 0.9694 - val_specificity: 0.9038 - val_gmeasure: 0.9360 - val_auc: 0.9779
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:151] Training end with time 6.0595057010650635!
[root    |INFO|build_network.py:79] Saved trained model weight at ./example_result/weight_2.h5
[root    |DEBUG|deepbiome.py:166] Save weight at ./example_result/weight_2.h5
[root    |DEBUG|deepbiome.py:169] Save history at ./example_result/hist_2.json
[root    |INFO|build_network.py:177] Evaluation start!
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
750/750 [==============================] - 0s 7us/step
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:182] Evaluation end with time 0.013599395751953125!
[root    |INFO|build_network.py:183] Evaluation: [0.10130026936531067, 0.972000002861023, 0.976516604423523, 0.9623430967330933, 0.9694039225578308, 0.989936888217926]
[root    |INFO|build_network.py:177] Evaluation start!
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
250/250 [==============================] - 0s 19us/step
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:182] Evaluation end with time 0.011395692825317383!
[root    |INFO|build_network.py:183] Evaluation: [0.2500866949558258, 0.9120000004768372, 0.932584285736084, 0.8611111044883728, 0.8961353898048401, 0.958996593952179]
[root    |INFO|deepbiome.py:179] Compute time : 7.9529359340667725
[root    |INFO|deepbiome.py:180] 3 fold computing end!---------------------------------------------
[root    |INFO|deepbiome.py:183] -----------------------------------------------------------------
[root    |INFO|deepbiome.py:185] Train Evaluation : [&#39;loss&#39; &#39;binary_accuracy&#39; &#39;sensitivity&#39; &#39;specificity&#39; &#39;gmeasure&#39; &#39;auc&#39;]
[root    |INFO|deepbiome.py:188]       mean : [0.1876763  0.92355555 0.92663413 0.91924282 0.92254825 0.95947075]
[root    |INFO|deepbiome.py:189]        std : [0.07260641 0.04275455 0.05397934 0.0325704  0.03622105 0.0336476 ]
[root    |INFO|deepbiome.py:190] -----------------------------------------------------------------
[root    |INFO|deepbiome.py:192] Test Evaluation : [&#39;loss&#39; &#39;binary_accuracy&#39; &#39;sensitivity&#39; &#39;specificity&#39; &#39;gmeasure&#39; &#39;auc&#39;]
[root    |INFO|deepbiome.py:195]       mean : [0.32584215 0.87866668 0.90602342 0.81741391 0.86051089 0.93644808]
[root    |INFO|deepbiome.py:196]        std : [0.08518076 0.03641733 0.03921965 0.03466629 0.03519384 0.02614045]
[root    |INFO|deepbiome.py:197] -----------------------------------------------------------------
[root    |INFO|deepbiome.py:206] Total Computing Ended
[root    |INFO|deepbiome.py:207] -----------------------------------------------------------------
</pre></div></div>
</div>
<p>Let’s check the history plot again.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[29]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;./</span><span class="si">%s</span><span class="s1">/hist_0.json&#39;</span> <span class="o">%</span> <span class="n">path_info</span><span class="p">[</span><span class="s1">&#39;model_info&#39;</span><span class="p">][</span><span class="s1">&#39;model_dir&#39;</span><span class="p">],</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">history</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epochs&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="_images/example_with_the_list_of_inputs_51_0.png" src="_images/example_with_the_list_of_inputs_51_0.png" />
</div>
</div>
</div>
<div class="section" id="6.-Load-the-pretrained-network-for-testing">
<h2>6. Load the pretrained network for testing<a class="headerlink" href="#6.-Load-the-pretrained-network-for-testing" title="Permalink to this headline">¶</a></h2>
<p>If you want to test the trained model, you can use the <code class="docutils literal notranslate"><span class="pre">deepbiome_test</span></code> function. If you use the index file, this function provide the evaluation using test index (index set not included in the index file) for each fold. If not, this function provide the evaluation using the whole samples.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[30]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">evaluation</span> <span class="o">=</span> <span class="n">deepbiome</span><span class="o">.</span><span class="n">deepbiome_test</span><span class="p">(</span><span class="n">log</span><span class="p">,</span> <span class="n">network_info</span><span class="p">,</span> <span class="n">path_info</span><span class="p">,</span> <span class="n">number_of_fold</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|deepbiome.py:259] -----------------------------------------------------------------
[root    |INFO|deepbiome.py:289] Test Evaluation : [&#39;loss&#39; &#39;binary_accuracy&#39; &#39;sensitivity&#39; &#39;specificity&#39; &#39;gmeasure&#39; &#39;auc&#39;]
[root    |INFO|deepbiome.py:291] -------1 fold test start!----------------------------------
[root    |INFO|readers.py:58] -----------------------------------------------------------------------
[root    |INFO|readers.py:59] Construct Dataset
[root    |INFO|readers.py:60] -----------------------------------------------------------------------
[root    |INFO|readers.py:61] Load data
[root    |INFO|deepbiome.py:301] -----------------------------------------------------------------
[root    |INFO|deepbiome.py:302] Build network for 1 fold testing
[root    |INFO|build_network.py:513] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:514] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv
[root    |INFO|build_network.py:518] Phylogenetic tree level list: [&#39;Genus&#39;, &#39;Family&#39;, &#39;Order&#39;, &#39;Class&#39;, &#39;Phylum&#39;]
[root    |INFO|build_network.py:519] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:526]      Genus: 48
[root    |INFO|build_network.py:526]     Family: 40
[root    |INFO|build_network.py:526]      Order: 23
[root    |INFO|build_network.py:526]      Class: 17
[root    |INFO|build_network.py:526]     Phylum: 9
[root    |INFO|build_network.py:529] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:530] Phylogenetic_tree_dict info: [&#39;Genus&#39;, &#39;Number&#39;, &#39;Order&#39;, &#39;Class&#39;, &#39;Phylum&#39;, &#39;Family&#39;]
[root    |INFO|build_network.py:531] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:541] Build edge weights between [ Genus, Family]
[root    |INFO|build_network.py:541] Build edge weights between [Family,  Order]
[root    |INFO|build_network.py:541] Build edge weights between [ Order,  Class]
[root    |INFO|build_network.py:541] Build edge weights between [ Class, Phylum]
[root    |INFO|build_network.py:554] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:570] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:571] Build network based on phylogenetic tree information
[root    |INFO|build_network.py:572] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:648] ------------------------------------------------------------------------------------------
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Model: &#34;model_1&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input (InputLayer)           (None, 48)                0
_________________________________________________________________
l1_dense (Dense_with_tree)   (None, 40)                1960
_________________________________________________________________
l1_activation (Activation)   (None, 40)                0
_________________________________________________________________
l2_dense (Dense_with_tree)   (None, 23)                943
_________________________________________________________________
l2_activation (Activation)   (None, 23)                0
_________________________________________________________________
l3_dense (Dense_with_tree)   (None, 17)                408
_________________________________________________________________
l3_activation (Activation)   (None, 17)                0
_________________________________________________________________
l4_dense (Dense_with_tree)   (None, 9)                 162
_________________________________________________________________
l4_activation (Activation)   (None, 9)                 0
_________________________________________________________________
last_dense_h (Dense)         (None, 1)                 10
_________________________________________________________________
p_hat (Activation)           (None, 1)                 0
=================================================================
Total params: 3,483
Trainable params: 3,483
Non-trainable params: 0
_________________________________________________________________
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:57] Build Network
[root    |INFO|build_network.py:58] Optimizer = adam
[root    |INFO|build_network.py:59] Loss = binary_crossentropy
[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc
[root    |INFO|deepbiome.py:311] -----------------------------------------------------------------
[root    |INFO|deepbiome.py:312] 1 fold computing start!----------------------------------
[root    |INFO|build_network.py:177] Evaluation start!
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
250/250 [==============================] - 0s 425us/step
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:182] Evaluation end with time 0.2846240997314453!
[root    |INFO|build_network.py:183] Evaluation: [0.282605916261673, 0.8960000276565552, 0.9349112510681152, 0.8148148059844971, 0.872799813747406, 0.9505442380905151]
[root    |INFO|deepbiome.py:315]
[root    |INFO|deepbiome.py:317] Compute time : 1.8910877704620361
[root    |INFO|deepbiome.py:318] 1 fold computing end!---------------------------------------------
[root    |INFO|deepbiome.py:291] -------2 fold test start!----------------------------------
[root    |INFO|readers.py:58] -----------------------------------------------------------------------
[root    |INFO|readers.py:59] Construct Dataset
[root    |INFO|readers.py:60] -----------------------------------------------------------------------
[root    |INFO|readers.py:61] Load data
[root    |INFO|deepbiome.py:301] -----------------------------------------------------------------
[root    |INFO|deepbiome.py:302] Build network for 2 fold testing
[root    |INFO|build_network.py:513] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:514] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv
[root    |INFO|build_network.py:518] Phylogenetic tree level list: [&#39;Genus&#39;, &#39;Family&#39;, &#39;Order&#39;, &#39;Class&#39;, &#39;Phylum&#39;]
[root    |INFO|build_network.py:519] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:526]      Genus: 48
[root    |INFO|build_network.py:526]     Family: 40
[root    |INFO|build_network.py:526]      Order: 23
[root    |INFO|build_network.py:526]      Class: 17
[root    |INFO|build_network.py:526]     Phylum: 9
[root    |INFO|build_network.py:529] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:530] Phylogenetic_tree_dict info: [&#39;Genus&#39;, &#39;Number&#39;, &#39;Order&#39;, &#39;Class&#39;, &#39;Phylum&#39;, &#39;Family&#39;]
[root    |INFO|build_network.py:531] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:541] Build edge weights between [ Genus, Family]
[root    |INFO|build_network.py:541] Build edge weights between [Family,  Order]
[root    |INFO|build_network.py:541] Build edge weights between [ Order,  Class]
[root    |INFO|build_network.py:541] Build edge weights between [ Class, Phylum]
[root    |INFO|build_network.py:554] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:570] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:571] Build network based on phylogenetic tree information
[root    |INFO|build_network.py:572] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:648] ------------------------------------------------------------------------------------------
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Model: &#34;model_1&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input (InputLayer)           (None, 48)                0
_________________________________________________________________
l1_dense (Dense_with_tree)   (None, 40)                1960
_________________________________________________________________
l1_activation (Activation)   (None, 40)                0
_________________________________________________________________
l2_dense (Dense_with_tree)   (None, 23)                943
_________________________________________________________________
l2_activation (Activation)   (None, 23)                0
_________________________________________________________________
l3_dense (Dense_with_tree)   (None, 17)                408
_________________________________________________________________
l3_activation (Activation)   (None, 17)                0
_________________________________________________________________
l4_dense (Dense_with_tree)   (None, 9)                 162
_________________________________________________________________
l4_activation (Activation)   (None, 9)                 0
_________________________________________________________________
last_dense_h (Dense)         (None, 1)                 10
_________________________________________________________________
p_hat (Activation)           (None, 1)                 0
=================================================================
Total params: 3,483
Trainable params: 3,483
Non-trainable params: 0
_________________________________________________________________
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:57] Build Network
[root    |INFO|build_network.py:58] Optimizer = adam
[root    |INFO|build_network.py:59] Loss = binary_crossentropy
[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc
[root    |INFO|deepbiome.py:311] -----------------------------------------------------------------
[root    |INFO|deepbiome.py:312] 2 fold computing start!----------------------------------
[root    |INFO|build_network.py:177] Evaluation start!
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
250/250 [==============================] - 0s 456us/step
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:182] Evaluation end with time 0.3126504421234131!
[root    |INFO|build_network.py:183] Evaluation: [0.4448338449001312, 0.828000009059906, 0.8505747318267822, 0.7763158082962036, 0.8125974535942078, 0.8998034000396729]
[root    |INFO|deepbiome.py:315]
[root    |INFO|deepbiome.py:317] Compute time : 1.777827262878418
[root    |INFO|deepbiome.py:318] 2 fold computing end!---------------------------------------------
[root    |INFO|deepbiome.py:291] -------3 fold test start!----------------------------------
[root    |INFO|readers.py:58] -----------------------------------------------------------------------
[root    |INFO|readers.py:59] Construct Dataset
[root    |INFO|readers.py:60] -----------------------------------------------------------------------
[root    |INFO|readers.py:61] Load data
[root    |INFO|deepbiome.py:301] -----------------------------------------------------------------
[root    |INFO|deepbiome.py:302] Build network for 3 fold testing
[root    |INFO|build_network.py:513] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:514] Read phylogenetic tree information from /DATA/home/muha/github_repos/deepbiome/deepbiome/tests/data/genus48_dic.csv
[root    |INFO|build_network.py:518] Phylogenetic tree level list: [&#39;Genus&#39;, &#39;Family&#39;, &#39;Order&#39;, &#39;Class&#39;, &#39;Phylum&#39;]
[root    |INFO|build_network.py:519] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:526]      Genus: 48
[root    |INFO|build_network.py:526]     Family: 40
[root    |INFO|build_network.py:526]      Order: 23
[root    |INFO|build_network.py:526]      Class: 17
[root    |INFO|build_network.py:526]     Phylum: 9
[root    |INFO|build_network.py:529] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:530] Phylogenetic_tree_dict info: [&#39;Genus&#39;, &#39;Number&#39;, &#39;Order&#39;, &#39;Class&#39;, &#39;Phylum&#39;, &#39;Family&#39;]
[root    |INFO|build_network.py:531] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:541] Build edge weights between [ Genus, Family]
[root    |INFO|build_network.py:541] Build edge weights between [Family,  Order]
[root    |INFO|build_network.py:541] Build edge weights between [ Order,  Class]
[root    |INFO|build_network.py:541] Build edge weights between [ Class, Phylum]
[root    |INFO|build_network.py:554] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:570] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:571] Build network based on phylogenetic tree information
[root    |INFO|build_network.py:572] ------------------------------------------------------------------------------------------
[root    |INFO|build_network.py:648] ------------------------------------------------------------------------------------------
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Model: &#34;model_1&#34;
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input (InputLayer)           (None, 48)                0
_________________________________________________________________
l1_dense (Dense_with_tree)   (None, 40)                1960
_________________________________________________________________
l1_activation (Activation)   (None, 40)                0
_________________________________________________________________
l2_dense (Dense_with_tree)   (None, 23)                943
_________________________________________________________________
l2_activation (Activation)   (None, 23)                0
_________________________________________________________________
l3_dense (Dense_with_tree)   (None, 17)                408
_________________________________________________________________
l3_activation (Activation)   (None, 17)                0
_________________________________________________________________
l4_dense (Dense_with_tree)   (None, 9)                 162
_________________________________________________________________
l4_activation (Activation)   (None, 9)                 0
_________________________________________________________________
last_dense_h (Dense)         (None, 1)                 10
_________________________________________________________________
p_hat (Activation)           (None, 1)                 0
=================================================================
Total params: 3,483
Trainable params: 3,483
Non-trainable params: 0
_________________________________________________________________
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:57] Build Network
[root    |INFO|build_network.py:58] Optimizer = adam
[root    |INFO|build_network.py:59] Loss = binary_crossentropy
[root    |INFO|build_network.py:60] Metrics = binary_accuracy, sensitivity, specificity, gmeasure, auc
[root    |INFO|deepbiome.py:311] -----------------------------------------------------------------
[root    |INFO|deepbiome.py:312] 3 fold computing start!----------------------------------
[root    |INFO|build_network.py:177] Evaluation start!
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
250/250 [==============================] - 0s 434us/step
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
[root    |INFO|build_network.py:182] Evaluation end with time 0.2911825180053711!
[root    |INFO|build_network.py:183] Evaluation: [0.2500866949558258, 0.9120000004768372, 0.932584285736084, 0.8611111044883728, 0.8961353898048401, 0.958996593952179]
[root    |INFO|deepbiome.py:315]
[root    |INFO|deepbiome.py:317] Compute time : 1.838158369064331
[root    |INFO|deepbiome.py:318] 3 fold computing end!---------------------------------------------
[root    |INFO|deepbiome.py:321] -----------------------------------------------------------------
[root    |INFO|deepbiome.py:323] Test Evaluation : [&#39;loss&#39; &#39;binary_accuracy&#39; &#39;sensitivity&#39; &#39;specificity&#39; &#39;gmeasure&#39; &#39;auc&#39;]
[root    |INFO|deepbiome.py:326]       mean : [0.32584215 0.87866668 0.90602342 0.81741391 0.86051089 0.93644808]
[root    |INFO|deepbiome.py:327]        std : [0.08518076 0.03641733 0.03921965 0.03466629 0.03519384 0.02614045]
[root    |INFO|deepbiome.py:328] -----------------------------------------------------------------
[root    |INFO|deepbiome.py:331] Total Computing Ended
[root    |INFO|deepbiome.py:332] -----------------------------------------------------------------
</pre></div></div>
</div>
<p>This function provide the evaluation result as a numpy array of the shape (number of fold, number of evaluation measures).</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[31]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">print</span><span class="p">(</span><span class="s1">&#39;      </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="s1">&#39;</span><span class="si">%16s</span><span class="s1">&#39;</span><span class="o">%</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span><span class="o">+</span> <span class="p">[</span><span class="s1">&#39;</span><span class="si">%16s</span><span class="s1">&#39;</span><span class="o">%</span><span class="k">s</span>.strip() for s in network_info[&#39;model_info&#39;][&#39;metrics&#39;].split(&#39;,&#39;)]))
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Mean: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="s1">&#39;</span><span class="si">%16.4f</span><span class="s1">&#39;</span><span class="o">%</span><span class="k">v</span> for v in np.mean(evaluation, axis=0)]))
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Std : </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="s1">&#39;</span><span class="si">%16.4f</span><span class="s1">&#39;</span><span class="o">%</span><span class="k">v</span> for v in np.std(evaluation, axis=0)]))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
                  loss binary_accuracy     sensitivity     specificity        gmeasure             auc
Mean:           0.3258          0.8787          0.9060          0.8174          0.8605          0.9364
Std :           0.0852          0.0364          0.0392          0.0347          0.0352          0.0261
</pre></div></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="release-history.html" class="btn btn-neutral float-right" title="Release History" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="usage.html" class="btn btn-neutral float-left" title="Usage" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Youngwon Choi

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>