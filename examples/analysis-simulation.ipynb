{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep MicroBiome\n",
    "\n",
    "Aug. 14. 2019\n",
    "@ Youngwon (youngwon08@gmail.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DATA/home/muha/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import keras.backend as k\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deepbiome.deepbiome import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "k.set_session(tf.Session(config=config))\n",
    "\n",
    "logging.basicConfig(format = '[%(name)-8s|%(levelname)s|%(filename)s:%(lineno)s] %(message)s',\n",
    "                    level=logging.DEBUG)\n",
    "log = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save = False\n",
    "# kfold=1000\n",
    "kfold=10\n",
    "network_model_keys = ['optimizer','lr','decay']\n",
    "architecture_keys = ['weight_decay', 'weight_l1_penalty', #'weight_l2_penalty',\n",
    "                     'tree_thrd', 'weight_initial',\n",
    "                     'batch_normalization','drop_out']\n",
    "network_training_keys = ['batch_size','epochs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# filenames = 'simulation_s0.Rmd'\n",
    "# models = [\n",
    "#           'simulation_s0/simulation_s0_deep',\n",
    "#           'simulation_s0/simulation_s0_deep_l1',\n",
    "#           'simulation_s0/simulation_s0_deepbiome',\n",
    "#          ]\n",
    "\n",
    "# models_aka = [\n",
    "#           'DNN',\n",
    "#           'DNN+l1',\n",
    "#           'DeepBiome',\n",
    "#          ]\n",
    "# num_classes = 0\n",
    "########################################################################\n",
    "filenames = 'simulation_s1.Rmd'\n",
    "models = [\n",
    "          'simulation_s1/simulation_s1_deep',\n",
    "          'simulation_s1/simulation_s1_deep_l1',\n",
    "          'simulation_s1/simulation_s1_deepbiome',\n",
    "         ]\n",
    "\n",
    "models_aka = [\n",
    "          'DNN',\n",
    "          'DNN+l1',\n",
    "          'DeepBiome',\n",
    "         ]\n",
    "num_classes = 0\n",
    "# ########################################################################\n",
    "# filenames = 'simulation_s2_v1.Rmd'\n",
    "# models = [\n",
    "#           'simulation_s2/simulation_s2_deep',\n",
    "#           'simulation_s2/simulation_s2_deep_l1',\n",
    "#           'simulation_s2/simulation_s2_deepbiome',\n",
    "#          ]\n",
    "\n",
    "# models_aka = [\n",
    "#           'DNN',\n",
    "#           'DNN+l1',\n",
    "#           'DeepBiome',\n",
    "#          ]\n",
    "# num_classes = 1\n",
    "# #######################################################################\n",
    "# filenames = 'simulation_s3.Rmd'\n",
    "# models = [\n",
    "#           'simulation_s3/simulation_s3_deep',\n",
    "#           'simulation_s3/simulation_s3_deep_l1',\n",
    "#           'simulation_s3/simulation_s3_deepbiome',\n",
    "#          ]\n",
    "\n",
    "# models_aka = [\n",
    "#           'DNN',\n",
    "#           'DNN+l1',\n",
    "#           'DeepBiome',\n",
    "#          ]\n",
    "# num_classes = 3\n",
    "# # ########################################################################\n",
    "# filenames = 'simulation_s4.Rmd'\n",
    "# models = [\n",
    "#           'simulation_s4/simulation_s4_deep',\n",
    "#           'simulation_s4/simulation_s4_deep_l1',\n",
    "#           'simulation_s4/simulation_s4_deepbiome',\n",
    "#          ]\n",
    "\n",
    "# models_aka = [\n",
    "#           'DNN',\n",
    "#           'DNN+l1',\n",
    "#           'DeepBiome',\n",
    "#          ]\n",
    "# num_classes = 0\n",
    "########################################################################\n",
    "# filenames = 'simulation_s5.Rmd'\n",
    "# models = [\n",
    "#           'simulation_s5/simulation_s4_deep',\n",
    "#           'simulation_s5/simulation_s4_deep_l1',\n",
    "#           'simulation_s5/simulation_s4_deepbiome',\n",
    "#          ]\n",
    "\n",
    "# models_aka = [\n",
    "#           'DNN',\n",
    "#           'DNN+l1',\n",
    "#           'DeepBiome',\n",
    "#          ]\n",
    "# num_classes = 0\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_network_info = {}\n",
    "model_path_info = {}\n",
    "for model_path in models:\n",
    "    config_data = configuration.Configurator('%s/config/path_info.cfg' % model_path, log, verbose=False)\n",
    "    config_data.set_config_map(config_data.get_section_map())\n",
    "    config_network = configuration.Configurator('%s/config/network_info.cfg' % model_path, log, verbose=False)\n",
    "    config_network.set_config_map(config_network.get_section_map())\n",
    "\n",
    "    model_path_info[model_path] = config_data.get_config_map()\n",
    "    model_network_info[model_path] = config_network.get_config_map()\n",
    "    \n",
    "if num_classes == 0: y_names = ['loss','correlation_coefficient']\n",
    "elif num_classes==1: y_names = ['loss','binary_accuracy','sensitivity','specificity','gmeasure', 'auc']\n",
    "else: y_names=['loss','categorical_accuracy','precision','recall','f1', 'auc']\n",
    "\n",
    "if num_classes == 0: measure_index = np.array([0,1])\n",
    "elif num_classes==1: measure_index = np.array([2,3,4,1,5])\n",
    "else: measure_index = np.array([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               model & loss & correlation_coefficient  \\\\\\ \\hline\n",
      "DNN &      0.007 &      0.011&     0.997 &      0.004 &      0.002 &      0.006&     0.999 &      0.002 \\\\\n",
      "DNN+l1 &      0.058 &      0.115&     0.977 &      0.048 &      0.010 &      0.019&     0.996 &      0.007 \\\\\n",
      "DeepBiome &      0.009 &      0.008&     0.996 &      0.003 &      0.003 &      0.005&     0.999 &      0.002 \\\\\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "# log.info('%20s & %s' % ('model', '& '.join(['%s ' % name for name in np.array(y_names)[[measure_index]]])))\n",
    "print('%20s & %s \\\\\\\\\\ \\hline' % ('model', '& '.join(['%s ' % name for name in np.array(y_names)[[measure_index]]])))\n",
    "# for model, aka in zip(models, models_aka):\n",
    "#     evaluation = np.load('%s/eval.npy' % model)\n",
    "#     log.info('%20s: %s' % (aka, ''.join(['%10.4f (%10.4f)'%(mean, std) for mean, std in zip(np.mean(evaluation, axis=0),np.std(evaluation, axis=0))])))\n",
    "#     results.append(np.vstack([np.mean(evaluation, axis=0),np.std(evaluation, axis=0)]).transpose())\n",
    "for model, aka in zip(models, models_aka):\n",
    "    train_evaluation = np.load('%s/train_eval.npy' % model)[:,measure_index]\n",
    "    train_res = '&'.join(['%10.3f & %10.3f'%(mean, std) for mean, std in zip(np.mean(train_evaluation, axis=0),np.std(train_evaluation, axis=0))])\n",
    "    test_evaluation = np.load('%s/test_eval.npy' % model)[:,measure_index]\n",
    "    test_res = '&'.join(['%10.3f & %10.3f'%(mean, std) for mean, std in zip(np.mean(test_evaluation, axis=0),np.std(test_evaluation, axis=0))])\n",
    "#     log.info('%s & %s & %s \\\\\\\\' % (aka, train_res, test_res))\n",
    "    print('%s & %s & %s \\\\\\\\' % (aka, test_res, train_res))\n",
    "#     results.append(np.vstack([np.mean(evaluation, axis=0),np.std(evaluation, axis=0)]).transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|<ipython-input-9-ce0e784f4c7b>:23]                  model : simulation_s1/simulation_s1_deepbiome\n",
      "[root    |INFO|<ipython-input-9-ce0e784f4c7b>:24]              model_aka : DeepBiome\n",
      "[root    |INFO|<ipython-input-9-ce0e784f4c7b>:26]           weight_decay : phylogenetic_tree\n",
      "[root    |INFO|<ipython-input-9-ce0e784f4c7b>:26]      weight_l1_penalty : None\n",
      "[root    |INFO|<ipython-input-9-ce0e784f4c7b>:26]              tree_thrd : None\n",
      "[root    |INFO|<ipython-input-9-ce0e784f4c7b>:26]         weight_initial : glorot_uniform\n",
      "[root    |INFO|<ipython-input-9-ce0e784f4c7b>:26]    batch_normalization : False\n",
      "[root    |INFO|<ipython-input-9-ce0e784f4c7b>:26]               drop_out : 0\n",
      "[root    |INFO|<ipython-input-9-ce0e784f4c7b>:28]              optimizer : adam\n",
      "[root    |INFO|<ipython-input-9-ce0e784f4c7b>:28]                     lr : 0.01\n",
      "[root    |INFO|<ipython-input-9-ce0e784f4c7b>:28]                  decay : 0.\n",
      "[root    |INFO|<ipython-input-9-ce0e784f4c7b>:30]             batch_size : 30\n",
      "[root    |INFO|<ipython-input-9-ce0e784f4c7b>:30]                 epochs : 5000\n"
     ]
    }
   ],
   "source": [
    "num=2\n",
    "model_path = models[num]\n",
    "model_aka = models_aka[num]\n",
    "\n",
    "config_data = configuration.Configurator('%s/config/path_info.cfg' % model_path, log, verbose=False)\n",
    "config_data.set_config_map(config_data.get_section_map())\n",
    "config_network = configuration.Configurator('%s/config/network_info.cfg' % model_path, log, verbose=False)\n",
    "config_network.set_config_map(config_network.get_section_map())\n",
    "\n",
    "path_info = config_data.get_config_map()\n",
    "network_info = config_network.get_config_map()\n",
    "\n",
    "path_info['data_info']['data_path'] = '/'.join(path_info['data_info']['data_path'].split('/')[2:])\n",
    "path_info['data_info']['tree_info_path'] = '/'.join(path_info['data_info']['tree_info_path'].split('/')[2:])\n",
    "try: path_info['data_info']['count_list_path'] = '/'.join(path_info['data_info']['count_list_path'].split('/')[2:])\n",
    "except: pass\n",
    "try: path_info['data_info']['count_path'] = '/'.join(path_info['data_info']['count_path'].split('/')[2:])\n",
    "except: pass\n",
    "path_info['data_info']['idx_path'] = '/'.join(path_info['data_info']['idx_path'].split('/')[2:])\n",
    "try: path_info['data_info']['disease_weight_path'] = '/'.join(path_info['data_info']['disease_weight_path'].split('/')[2:])\n",
    "except: pass\n",
    "\n",
    "log.info('%22s : %s' % ('model', model_path))\n",
    "log.info('%22s : %s' % ('model_aka', model_aka))\n",
    "for k in architecture_keys:\n",
    "    log.info('%22s : %s' % (k, network_info['architecture_info'].get(k, None)))\n",
    "for k in network_model_keys:\n",
    "    log.info('%22s : %s' % (k, network_info['model_info'].get(k, None)))\n",
    "for k in network_training_keys:\n",
    "    log.info('%22s : %s' % (k, network_info['training_info'].get(k, None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|<ipython-input-10-65b4b58a34e1>:2] \t loss correlation_coefficient\n",
      "[root    |INFO|<ipython-input-10-65b4b58a34e1>:4] 0 fold : [9.40851634e-04 9.99662399e-01]\n",
      "[root    |INFO|<ipython-input-10-65b4b58a34e1>:4] 1 fold : [0.00644644 0.99756938]\n",
      "[root    |INFO|<ipython-input-10-65b4b58a34e1>:4] 2 fold : [2.38116947e-04 9.99898791e-01]\n",
      "[root    |INFO|<ipython-input-10-65b4b58a34e1>:4] 3 fold : [0.01101121 0.99508071]\n",
      "[root    |INFO|<ipython-input-10-65b4b58a34e1>:4] 4 fold : [0.00183378 0.99934578]\n",
      "[root    |INFO|<ipython-input-10-65b4b58a34e1>:4] 5 fold : [0.02952539 0.98883462]\n",
      "[root    |INFO|<ipython-input-10-65b4b58a34e1>:4] 6 fold : [0.0109659  0.99568689]\n",
      "[root    |INFO|<ipython-input-10-65b4b58a34e1>:4] 7 fold : [0.00389818 0.99866223]\n",
      "[root    |INFO|<ipython-input-10-65b4b58a34e1>:4] 8 fold : [0.01359978 0.99511695]\n",
      "[root    |INFO|<ipython-input-10-65b4b58a34e1>:4] 9 fold : [0.01584209 0.99380291]\n",
      "[root    |INFO|<ipython-input-10-65b4b58a34e1>:5] Mean   : [0.00943017 0.99636607]\n",
      "[root    |INFO|<ipython-input-10-65b4b58a34e1>:6] Std   : [0.00849086 0.0032544 ]\n"
     ]
    }
   ],
   "source": [
    "evaluation = np.load('%s/test_eval.npy' % model_path)\n",
    "log.info('\\t %s'%' '.join(['%s' % name for name in y_names]))\n",
    "\n",
    "_ = [log.info('%d fold : %s' % (i,line)) for i, line in enumerate(evaluation)]\n",
    "log.info('Mean   : %s' % np.mean(evaluation, axis=0))\n",
    "log.info('Std   : %s' % np.std(evaluation, axis=0))\n",
    "\n",
    "# _ = [print('%d fold & %s \\\\tabularnewline' % (i, ' & '.join(['%.3f'% v for v in line]))) for i, line in enumerate(evaluation)]\n",
    "# print('Mean & %s \\\\tabularnewline' % (' & '.join(['%.3f'% v for v in np.mean(evaluation, axis=0)])))\n",
    "# print('Sd & %s \\\\tabularnewline' % (' & '.join(['%.3f'% v for v in np.std(evaluation, axis=0)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight estimation of DeepBiom\n",
    "\n",
    "We identify the largest weight estimatio of neurons in two hidden layers; by doing this, we can identify the strongest phylogenetic connections. We compute the True Positive Rate (``TPR``, sensitivity), True Negative Rate (``TNR``, specificity), and their geometric mean (i.e., ``g-Measure``). The false discovery rate (FDR) would be ``FDR = 1-TPR`` in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def texa_selection_accuracy(tree_weight_list, true_tree_weight_list):\n",
    "    accuracy_list = []\n",
    "    for i in range(len(true_tree_weight_list)):\n",
    "        tree_tw = true_tree_weight_list[i].astype(np.int32)\n",
    "#         tree_tw = (true_tree_weight_list[i]>0).astype(np.int32)\n",
    "        tree_w = np.zeros_like(tree_tw, dtype=np.int32)\n",
    "        tree_w_abs = np.abs(tree_weight_list[i])\n",
    "        for row, maxcol in enumerate(np.argmax(tree_w_abs, axis=1)):\n",
    "            tree_w[row,maxcol] = tree_w_abs[row,maxcol]\n",
    "#         tree_w = (tree_w > 1e-2).astype(np.int32)\n",
    "        tree_w = (tree_w > 0).astype(np.int32)\n",
    "        num_selected_texa = np.sum(np.sum(tree_w, axis=1)>0)\n",
    "        sensitivity, specificity, gmeasure, accuracy = metric_texa_test(tree_tw.flatten(), tree_w.flatten())\n",
    "        accuracy_list.append([num_selected_texa, sensitivity, specificity, gmeasure, accuracy])\n",
    "    return accuracy_list\n",
    "\n",
    "def texa_selection_accuracy_2(tree_weight_list, true_tree_weight_list):\n",
    "    accuracy_list = []\n",
    "    for i in range(len(true_tree_weight_list)):\n",
    "        tree_tw = true_tree_weight_list[i].astype(np.int32)\n",
    "#         tree_tw = (true_tree_weight_list[i]>0).astype(np.int32)\n",
    "        tree_w = np.zeros_like(tree_tw, dtype=np.int32)\n",
    "        tree_w_abs = np.abs(tree_weight_list[i])\n",
    "        for row in range(tree_w_abs.shape[0]):\n",
    "#             tree_w[row,:] = (tree_w_abs[row,:]> 0).astype(np.int32)\n",
    "            tree_w[row,:] = (tree_w_abs[row,:]> 1e-2).astype(np.int32)\n",
    "        num_selected_texa = np.sum(np.sum(tree_w, axis=1)>0)\n",
    "        sensitivity, specificity, gmeasure, accuracy = metric_texa_test(tree_tw.flatten(), tree_w.flatten())\n",
    "        accuracy_list.append([num_selected_texa, sensitivity, specificity, gmeasure, accuracy])\n",
    "    return accuracy_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[root    |INFO|build_network.py:508] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:509] Read phylogenetic tree information from data/genus48/genus48_dic.csv\n",
      "[root    |INFO|build_network.py:513] Phylogenetic tree level list: ['Genus', 'Family', 'Order', 'Class', 'Phylum']\n",
      "[root    |INFO|build_network.py:514] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:519]      Genus: 48\n",
      "[root    |INFO|build_network.py:519]     Family: 40\n",
      "[root    |INFO|build_network.py:519]      Order: 23\n",
      "[root    |INFO|build_network.py:519]      Class: 17\n",
      "[root    |INFO|build_network.py:519]     Phylum: 9\n",
      "[root    |INFO|build_network.py:522] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:523] Phylogenetic_tree_dict info: ['Class', 'Number', 'Order', 'Genus', 'Phylum', 'Family']\n",
      "[root    |INFO|build_network.py:524] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:534] Build edge weights between [ Genus, Family]\n",
      "[root    |INFO|build_network.py:534] Build edge weights between [Family,  Order]\n",
      "[root    |INFO|build_network.py:534] Build edge weights between [ Order,  Class]\n",
      "[root    |INFO|build_network.py:534] Build edge weights between [ Class, Phylum]\n",
      "[root    |INFO|build_network.py:547] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:563] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:564] Build network based on phylogenetic tree information\n",
      "[root    |INFO|build_network.py:565] ------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:432: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[tensorflow|WARNING|deprecation.py:328] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:432: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "[root    |INFO|build_network.py:636] ------------------------------------------------------------------------------------------\n",
      "[root    |INFO|build_network.py:57] Build Network\n",
      "[root    |INFO|build_network.py:58] Optimizer = adam\n",
      "[root    |INFO|build_network.py:59] Loss = mean_squared_error\n",
      "[root    |INFO|build_network.py:60] Metrics = correlation_coefficient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "l1_dense (Dense_with_tree)   (None, 40)                1960      \n",
      "_________________________________________________________________\n",
      "l1_activation (Activation)   (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "l2_dense (Dense_with_tree)   (None, 23)                943       \n",
      "_________________________________________________________________\n",
      "l2_activation (Activation)   (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "l3_dense (Dense_with_tree)   (None, 17)                408       \n",
      "_________________________________________________________________\n",
      "l3_activation (Activation)   (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "l4_dense (Dense_with_tree)   (None, 9)                 162       \n",
      "_________________________________________________________________\n",
      "l4_activation (Activation)   (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "last_dense_h (Dense)         (None, 1)                 10        \n",
      "_________________________________________________________________\n",
      "p_hat (Activation)           (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,483\n",
      "Trainable params: 3,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_weight_path = './%s/%s' % (model_path, path_info['model_info']['weight'])\n",
    "\n",
    "network_class = getattr(build_network, network_info['model_info']['network_class'].strip()) \n",
    "# network = network_class(network_info, path_info['data_info'], log, fold=0, num_classes=max(1,num_classes))\n",
    "network = network_class(network_info, path_info['data_info'], log, fold=0, num_classes=num_classes)\n",
    "network.model_compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/simulation/s1//tw_1.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-79f964607dd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weight_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrue_tree_weight_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_true_tree_weight_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_info'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtree_weight_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trained_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0maccuracy_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexa_selection_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree_weight_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_tree_weight_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github_repos/deepbiome/deepbiome/build_network.py\u001b[0m in \u001b[0;36mload_true_tree_weight_list\u001b[0;34m(self, data_path)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_true_tree_weight_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0mtrue_tree_weight_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s/tw_%d.npy'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_level_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrue_tree_weight_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github_repos/deepbiome/deepbiome/build_network.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_true_tree_weight_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0mtrue_tree_weight_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s/tw_%d.npy'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_level_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrue_tree_weight_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/simulation/s1//tw_1.npy'"
     ]
    }
   ],
   "source": [
    "accuracy_list = []\n",
    "for fold in range(kfold):\n",
    "    network.fold = fold\n",
    "    network.load_weights(file_path_fold(model_weight_path, fold), verbose=False)\n",
    "    true_tree_weight_list = network.load_true_tree_weight_list(path_info['data_info']['data_path'])\n",
    "    tree_weight_list = network.get_trained_weight()\n",
    "    accuracy_list.append(np.array(texa_selection_accuracy(tree_weight_list, true_tree_weight_list)))\n",
    "accuracy_list = np.array(accuracy_list)[:,:,1:]\n",
    "\n",
    "# print('%7s, %12s, %12s, %12s, %12s, %12s, %12s' % ('Model','True (Total)','Selected','Sensitivity','Specificity','gMeasure','Accuracy'))\n",
    "# print('---------------------------------------------------------------------------------------------------------------')\n",
    "# values = []\n",
    "# for i, (mean, std) in enumerate(zip(np.mean(accuracy_list, axis=0), np.std(accuracy_list, axis=0))):\n",
    "#     tree_tw = true_tree_weight_list[i].astype(np.int32)\n",
    "#     args = [network.tree_level_list[i], np.sum(np.sum(tree_tw, axis=1)>0), tree_tw.shape[0]]+ np.stack([mean, std]).T.flatten().tolist()\n",
    "#     value = '%7s, %7d (%2d), %7d (%2d), %5.3f (%5.3f), %5.3f (%5.3f), %5.3f (%5.3f), %5.3f (%5.3f)' % tuple(args)\n",
    "#     values.append(value.split(','))\n",
    "    \n",
    "print('%7s, %12s, %12s, %12s, %12s, %12s, %12s' % ('Model','True (Total)','Selected','Sensitivity','Specificity','gMeasure','Accuracy'))\n",
    "print('---------------------------------------------------------------------------------------------------------------')\n",
    "values = []\n",
    "for i, (mean, std) in enumerate(zip(np.mean(accuracy_list, axis=0), np.std(accuracy_list, axis=0))):\n",
    "    tree_tw = true_tree_weight_list[i].astype(np.int32)\n",
    "    args = [network.tree_level_list[i], np.sum(np.sum(tree_tw, axis=1)>0), tree_tw.shape[0]]\n",
    "    value = '%7s & %7d (%2d)' % tuple(args)\n",
    "    value = '%s & %s \\\\\\\\' % (value, '&'.join(['%6.3f & %6.3f'%(m,s) for m, s in zip(mean, std)]))\n",
    "    if i == 0: print('%10s & %s' % (model_aka, value))\n",
    "    else: print('%10s & %s' % ('', value))\n",
    "    values.append(value.split(','))\n",
    "    \n",
    "# if save: \n",
    "#     # filenametexa = '.'.join([\"%s_select_texa_1\" % filename.split('.')[0], filename.split('.')[1]])\n",
    "#     colname = ['Tree','True (Total)','Selected','Sensitivity','Specificity','gMeasure','Accuracy']\n",
    "#     with open('%s/%s' % (analysis_dir, filename), mode='a') as f:\n",
    "#     #     f.write('---\\ntitle: \"%s texa selection ver.1\"\\noutput: html_document\\n---\\n\\n' % filename.split('.')[0])\n",
    "#         f.write('\\n## Texa Selection Preformance (ver 1): %s\\n\\n' % model_aka)\n",
    "#         f.write('| %s |\\n' % ('|'.join([v for v in colname])))\n",
    "#         f.write('|'+'---|'*len(colname)+'\\n')\n",
    "#         for value in values:\n",
    "#             f.write('| %s |\\n' % ('|'.join(value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "for fold in range(kfold):\n",
    "    network.fold = fold\n",
    "    network.load_weights(file_path_fold(model_weight_path, fold), verbose=False)\n",
    "    true_tree_weight_list = network.load_true_tree_weight_list(path_info['data_info']['data_path'])\n",
    "    tree_weight_list = network.get_trained_weight()\n",
    "    accuracy_list.append(np.array(texa_selection_accuracy(tree_weight_list, true_tree_weight_list)))\n",
    "accuracy_list = np.array(accuracy_list)[:,:,1:]\n",
    "\n",
    "# print('%7s, %12s, %12s, %12s, %12s, %12s, %12s' % ('Model','True (Total)','Selected','Sensitivity','Specificity','gMeasure','Accuracy'))\n",
    "# print('---------------------------------------------------------------------------------------------------------------')\n",
    "# values = []\n",
    "# for i, (mean, std) in enumerate(zip(np.mean(accuracy_list, axis=0), np.std(accuracy_list, axis=0))):\n",
    "#     tree_tw = true_tree_weight_list[i].astype(np.int32)\n",
    "#     args = [network.tree_level_list[i], np.sum(np.sum(tree_tw, axis=1)>0), tree_tw.shape[0]]+ np.stack([mean, std]).T.flatten().tolist()\n",
    "#     value = '%7s, %7d (%2d), %7d (%2d), %5.3f (%5.3f), %5.3f (%5.3f), %5.3f (%5.3f), %5.3f (%5.3f)' % tuple(args)\n",
    "#     values.append(value.split(','))\n",
    "    \n",
    "print('%7s, %12s, %12s, %12s, %12s, %12s, %12s' % ('Model','True (Total)','Selected','Sensitivity','Specificity','gMeasure','Accuracy'))\n",
    "print('---------------------------------------------------------------------------------------------------------------')\n",
    "values = []\n",
    "for i, (mean, std) in enumerate(zip(np.mean(accuracy_list, axis=0), np.std(accuracy_list, axis=0))):\n",
    "    tree_tw = true_tree_weight_list[i].astype(np.int32)\n",
    "    args = [network.tree_level_list[i], np.sum(np.sum(tree_tw, axis=1)>0), tree_tw.shape[0]]\n",
    "    value = '%7s & %7d (%2d)' % tuple(args)\n",
    "    value = '%s & %s \\\\\\\\' % (value, '&'.join(['%6.3f & %6.3f'%(m,s) for m, s in zip(mean, std)]))\n",
    "    if i == 0: print('%10s & %s' % (model_aka, value))\n",
    "    else: print('%10s & %s' % ('', value))\n",
    "    values.append(value.split(','))\n",
    "    \n",
    "# if save: \n",
    "#     # filenametexa = '.'.join([\"%s_select_texa_1\" % filename.split('.')[0], filename.split('.')[1]])\n",
    "#     colname = ['Tree','True (Total)','Selected','Sensitivity','Specificity','gMeasure','Accuracy']\n",
    "#     with open('%s/%s' % (analysis_dir, filename), mode='a') as f:\n",
    "#     #     f.write('---\\ntitle: \"%s texa selection ver.1\"\\noutput: html_document\\n---\\n\\n' % filename.split('.')[0])\n",
    "#         f.write('\\n## Texa Selection Preformance (ver 1): %s\\n\\n' % model_aka)\n",
    "#         f.write('| %s |\\n' % ('|'.join([v for v in colname])))\n",
    "#         f.write('|'+'---|'*len(colname)+'\\n')\n",
    "#         for value in values:\n",
    "#             f.write('| %s |\\n' % ('|'.join(value)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
